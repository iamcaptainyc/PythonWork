{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8863915,"sourceType":"datasetVersion","datasetId":5335372},{"sourceId":10205319,"sourceType":"datasetVersion","datasetId":6306398},{"sourceId":10213912,"sourceType":"datasetVersion","datasetId":6293586},{"sourceId":96790,"sourceType":"modelInstanceVersion","modelInstanceId":81184,"modelId":82202},{"sourceId":143833,"sourceType":"modelInstanceVersion","modelInstanceId":121879,"modelId":144993},{"sourceId":206487,"sourceType":"modelInstanceVersion","modelInstanceId":176031,"modelId":192903}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"cp /kaggle/input/logger/logger.py /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:52:55.791368Z","iopub.execute_input":"2024-12-23T02:52:55.791658Z","iopub.status.idle":"2024-12-23T02:52:56.902620Z","shell.execute_reply.started":"2024-12-23T02:52:55.791615Z","shell.execute_reply":"2024-12-23T02:52:56.901442Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!wandb login 8d3272b3799965d019ae97e7cc2411cc23e4d87d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:52:56.904035Z","iopub.execute_input":"2024-12-23T02:52:56.904427Z","iopub.status.idle":"2024-12-23T02:53:00.185905Z","shell.execute_reply.started":"2024-12-23T02:52:56.904380Z","shell.execute_reply":"2024-12-23T02:53:00.184783Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:00.188822Z","iopub.execute_input":"2024-12-23T02:53:00.189265Z","iopub.status.idle":"2024-12-23T02:53:00.196978Z","shell.execute_reply.started":"2024-12-23T02:53:00.189224Z","shell.execute_reply":"2024-12-23T02:53:00.196006Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"ACCOUNT='yuyututa'\nNOTE='drseg'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:00.198205Z","iopub.execute_input":"2024-12-23T02:53:00.198514Z","iopub.status.idle":"2024-12-23T02:53:00.208617Z","shell.execute_reply.started":"2024-12-23T02:53:00.198478Z","shell.execute_reply":"2024-12-23T02:53:00.207790Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install torcheval\n!pip install torch-dct\n!pip install einops\n!pip install segmentation-models-pytorch\n# !pip install ml_collections\n# !pip install wget\n# !pip install medpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:00.209653Z","iopub.execute_input":"2024-12-23T02:53:00.210005Z","iopub.status.idle":"2024-12-23T02:53:40.270295Z","shell.execute_reply.started":"2024-12-23T02:53:00.209970Z","shell.execute_reply":"2024-12-23T02:53:40.269157Z"}},"outputs":[{"name":"stdout","text":"Collecting torcheval\n  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.12.2)\nDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torcheval\nSuccessfully installed torcheval-0.0.7\nCollecting torch-dct\n  Downloading torch_dct-0.1.6-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from torch-dct) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch-dct) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch-dct) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch-dct) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch-dct) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch-dct) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch-dct) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.1->torch-dct) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.1->torch-dct) (1.3.0)\nDownloading torch_dct-0.1.6-py3-none-any.whl (5.1 kB)\nInstalling collected packages: torch-dct\nSuccessfully installed torch-dct-0.1.6\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\nCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.24.6 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.26.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.3.0)\nCollecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (1.16.0)\nCollecting timm==0.9.7 (from segmentation-models-pytorch)\n  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.19.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.4.0)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\nDownloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=8bcafb062fbb16330f8ec2867cf5a7b6bc26c37704d8b72524bbb36a397e53e4\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=51c62bc5c339e8504084644bb4227cf34a392677f3e4cbf4fc039201a8aad684\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.11\n    Uninstalling timm-1.0.11:\n      Successfully uninstalled timm-1.0.11\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import sys, os\nimport argparse\nimport csv\nimport random\nimport numpy as np\nimport torch\nimport itertools\nimport json\nfrom torch.utils.data.sampler import Sampler\n\n# from const import *\n\n\ndef seed_torch(seed):\n    seed = int(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = False\n\ndef stable(dataloader, seed):\n    seed_torch(seed)\n    return dataloader\n\n\ndef add_dict_to_argparser(parser, default_dict):\n    for k, v in default_dict.items():\n        v_type = type(v)\n        if v is None:\n            v_type = str\n        elif isinstance(v, bool):\n            v_type = str2bool\n        parser.add_argument(f\"--{k}\", default=v, type=v_type)\n        \nclass Aobj():\n    def __init__(self,default_dict):\n        for k, v in default_dict.items():\n            setattr(self,k,v)\n    def get_dict(self):\n        return self.__dict__\n    \n    def update(self, default_dict):\n        for k, v in default_dict.items():\n            setattr(self,k,v)\n\n\ndef mkdir(path):\n    isExists = os.path.exists(path)\n    if not isExists:\n        os.makedirs(path)\n        print(path + ' ----- folder created')\n        return True\n    else:\n        return False\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n        self.warning=0\n        self.min_avg=sys.float_info.max\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n    \n    def is_overfitting(self):\n        if self.min_avg < self.avg:\n            print(f'warning val_loss:{self.avg} > minimum val_loss:{self.min_avg}')\n            self.warning+=1\n            print(f'warning count: {self.warning}')\n        elif self.min_avg > self.avg:\n            self.warning=0\n            self.min_avg=self.avg\n        return self.warning>=3\n    \n    def shall_save(self):\n        return self.warning==1\n\nclass Timer:\n    def __init__(self):\n        self.start_time=0\n\n    def start_timer(self,start_time):\n        self.start_time=start_time\n\n    def compute(self,end_time, name=''):\n        self.print_duration(end_time-self.start_time,name)\n        self.start_time=0\n\n    def print_duration(self, total_time, name):\n        minutes, seconds = divmod(total_time, 60)\n        print(f'{name} Time: {int(minutes)}m {int(seconds)}s, total:{total_time}')\n    \ndef count_class(label, num_classes):\n    if not isinstance(num_classes, list):\n        num_classes=[num_classes]\n        label=np.expand_dims(label, axis=0)\n    if label.shape[0] != len(num_classes):\n        label=label.transpose()\n    results={}\n    for i,n in enumerate(num_classes):\n        l=label[i].tolist()\n        result={}\n        for j in range(n):\n            result[f'第{j}类']=l.count(j)\n        results.update({f'标签{i}':result})\n    # return json.dumps(results, indent=4, ensure_ascii=False)\n    return results\n\nprint('utils.py')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:40.272006Z","iopub.execute_input":"2024-12-23T02:53:40.272399Z","iopub.status.idle":"2024-12-23T02:53:43.284802Z","shell.execute_reply.started":"2024-12-23T02:53:40.272360Z","shell.execute_reply":"2024-12-23T02:53:43.283886Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"utils.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#!coding:utf-8\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.nn.modules.loss import BCEWithLogitsLoss\n\n# from const import *\n\ndef get_criterion(args, **kwargs):\n    if args.loss_method == 'ce':\n        criterion=torch.nn.CrossEntropyLoss()\n    elif args.loss_method == 'wce':\n        criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor(args.ce_weight).to(kwargs['device']))\n    elif args.loss_method == 'dice':\n        criterion=DiceLoss()\n    elif args.loss_method == 'ce_dice':\n        criterion=CE_DiceLoss(weight=torch.tensor(args.ce_weight).to(kwargs['device']))\n    elif args.loss_method == 'focal':\n        criterion=FocalLoss(gamma=args.focal_gamma, alpha=torch.tensor(args.ce_weight).to(kwargs['device']))\n    elif args.loss_method == 'bce':\n        weight=torch.tensor(args.bce_weight).to(kwargs['device'])\n        criterion=nn.BCEWithLogitsLoss(pos_weight=weight)\n    elif args.loss_method == 'bce_dice':\n        weight=torch.tensor(args.bce_weight).to(kwargs['device'])\n        criterion=BCE_DiceLoss(weight=weight)\n    elif args.loss_method == 'wbce':\n        weight=torch.tensor(args.bce_weight).to(kwargs['device'])\n        criterion=WeightedBCE(weight=weight)\n    elif args.loss_method == 'cwbce':\n        criterion=CustomWeightedLoss()\n    return criterion\n\n#学习率调整\ndef get_scheduler(args, optimizer, last_epoch=-1):\n    if args.scheduler == 'cos':\n        scheduler= lr_scheduler.CosineAnnealingLR(optimizer,T_max=args.num_epochs,eta_min=args.min_lr,last_epoch=last_epoch)\n    elif args.scheduler == 'lam':\n        def ad_lr(epoch):\n            t=8\n            if epoch <=t:\n                return 1\n            else:\n                return math.pow(0.1,((epoch-t)//4)+1)\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=ad_lr,last_epoch=last_epoch)\n    elif args.scheduler == 'multi':\n        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=args.milestones, gamma=args.lr_gamma,last_epoch=last_epoch)\n    elif args.scheduler == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.lr_gamma,last_epoch=last_epoch)\n    else:\n        scheduler=None\n    return scheduler\n\ndef make_one_hot(labels, classes):\n    one_hot = torch.FloatTensor(labels.size()[0], classes, labels.size()[2], labels.size()[3]).zero_().to(labels.device)\n    target = one_hot.scatter_(1, labels.data, 1)\n    return target\n\nclass CustomWeightedLoss(nn.Module):\n    def __init__(self):\n        super(CustomWeightedLoss, self).__init__()\n        self.beta = 0.8\n\n    def forward(self, preds, targets):\n        \"\"\"\n        Args:\n            preds (Tensor): 模型预测的概率，形状为 (batch_size, c, H, W)。\n            targets (Tensor): 目标标签，形状为 (batch_size, c, H, W)。\n        \"\"\"\n        preds = torch.clamp(preds,min=1e-8,max=1-1e-8)\n        preds = F.sigmoid(preds)\n        \n        self.beta=(targets==0).sum(dim=(0,2,3))/(targets.shape[0]*targets.shape[2]*targets.shape[3])\n        self.beta=self.beta.view(1,targets.shape[1],1,1)\n        # 计算每个类别的损失\n        bce = - self.beta * targets * torch.log(preds) - (1 - targets) * (1-self.beta) * torch.log(1 - preds)\n        \n        loss = torch.mean(bce)\n        # print(self.beta)\n        # print(f'(targets==0).sum(dim=(0,2,3)):{(targets==0).sum(dim=(0,2,3))}')\n        # print(bce.shape)\n        # print(loss)\n        # print(loss.item())\n        return loss\n\nclass WeightedBCE(nn.Module):\n    def __init__(self, weight):\n        super(WeightedBCE, self).__init__()\n        self.weight=weight\n\n    def forward(self, output, target):\n        batch_weight = self.weight[target.data.view(-1).long()].view_as(target)\n        bce = nn.BCELoss(weight=batch_weight)\n        return bce(output, target)\n        \n        \nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1., ignore_index=255, binary=False):\n        super(DiceLoss, self).__init__()\n        self.ignore_index = ignore_index\n        self.smooth = smooth\n        self.binary=binary\n\n    def forward(self, output, target):\n        if self.binary:\n            output = F.sigmoid(output)\n            output = output.contiguous().view(-1)\n            target = target.contiguous().view(-1)\n            \n            pos_inter=output*target\n            neg_inter=(1-output)*(1-target)\n            intersection = pos_inter.sum()+neg_inter.sum()\n            loss = 1-((2 * intersection + self.smooth) / (2*target.numel()+ self.smooth))\n        else:\n            if self.ignore_index not in range(target.min(), target.max()):\n                if (target == self.ignore_index).sum() > 0:\n                    target[target == self.ignore_index] = target.min()\n            target = make_one_hot(target.unsqueeze(dim=1), classes=output.size()[1])\n            output = F.softmax(output, dim=1)\n            output_flat = output.contiguous().view(-1)\n            target_flat = target.contiguous().view(-1)\n            intersection = (output_flat * target_flat).sum()\n            loss = 1 - ((2. * intersection + self.smooth) /\n                        (output_flat.sum() + target_flat.sum() + self.smooth))\n        return loss\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2, alpha=None, ignore_index=255, size_average=True):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.size_average = size_average\n        self.CE_loss = nn.CrossEntropyLoss(reduce=False, ignore_index=ignore_index, weight=alpha)\n\n    def forward(self, output, target):\n        logpt = self.CE_loss(output, target)\n        pt = torch.exp(-logpt)\n        loss = ((1-pt)**self.gamma) * logpt\n        if self.size_average:\n            return loss.mean()\n        return loss.sum()\n\nclass CE_DiceLoss(nn.Module):\n    def __init__(self, reduction='mean', ignore_index=255, weight=None):\n        super(CE_DiceLoss, self).__init__()\n        self.dice = DiceLoss()\n        self.cross_entropy = nn.CrossEntropyLoss(weight=weight, reduction=reduction, ignore_index=ignore_index)\n    \n    def forward(self, output, target):\n        CE_loss = self.cross_entropy(output, target)\n        dice_loss = self.dice(output, target)\n        return CE_loss + dice_loss\n\nclass BCE_DiceLoss(nn.Module):\n    def __init__(self, weight, smooth=1):\n        super(BCE_DiceLoss, self).__init__()\n        self.dice = DiceLoss(smooth = smooth, binary=True)\n        self.bce = BCEWithLogitsLoss(pos_weight=weight)\n        \n    def forward(self, output, target):\n        bce_loss = self.bce(output, target)\n        dice_loss = self.dice(output, target)\n        return bce_loss+dice_loss\n        \n\nprint('loss.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:43.286353Z","iopub.execute_input":"2024-12-23T02:53:43.286828Z","iopub.status.idle":"2024-12-23T02:53:43.312033Z","shell.execute_reply.started":"2024-12-23T02:53:43.286790Z","shell.execute_reply":"2024-12-23T02:53:43.311119Z"}},"outputs":[{"name":"stdout","text":"loss.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torcheval.metrics as tm\nimport numpy as np\nfrom sklearn.metrics import *\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\n# from utils import utils\n# from const import *\n\n\nclass Estimator():\n    def __init__(self, metrics, num_classes, binary=False, input_logits=True):\n        \n        self.binary=binary\n        self.input_logits=input_logits\n        self.threshold=0.5\n        if self.binary:\n            self.metrics_names = {\n                'acc': tm.BinaryAccuracy,\n                'f1': tm.BinaryF1Score,\n                'auc': tm.BinaryAUROC,\n                'auprc': tm.BinaryAUPRC,\n                'precision': tm.BinaryPrecision,\n                'recall': tm.BinaryRecall,\n                'cm':tm.BinaryConfusionMatrix\n            }\n        else:\n            self.metrics_names = {\n                'acc': tm.MulticlassAccuracy,\n                'f1': tm.MulticlassF1Score,\n                'auc': tm.MulticlassAUROC,\n                'auprc': tm.MulticlassAUPRC,\n                'precision': tm.MulticlassPrecision,\n                'recall': tm.MulticlassRecall,\n                'kappa': QuadraticWeightedKappa,\n                'SegMetrics': SegMetrics\n            }\n        \n        self.return_dict_metrics = ['SegMetrics']\n        self.logits_required_metrics = ['auc', 'auprc']\n        self.need_kwargs_change=['f1','precision','recall']\n        self.need_named_metrics=['auc', 'auprc', 'PA']\n        \n        \n        self.num_classes = num_classes\n        self.metrics = metrics\n        self.metrics_fn={}\n        \n        if self.binary:\n            for m in metrics:\n                self.metrics_fn[m]=self.metrics_names[m]()\n        else:\n            for m in metrics:\n                if m in self.need_kwargs_change:\n                    self.metrics_fn[m]=self.metrics_names[m](num_classes=num_classes, average='macro')\n                if m in self.logits_required_metrics:\n                    self.metrics_fn[m]=self.metrics_names[m](num_classes=num_classes, average=None)\n                else:\n                    self.metrics_fn[m]=self.metrics_names[m](num_classes=num_classes)\n                \n        # self.y = np.empty(0)\n        # self.y_pred = np.empty(0)\n        # if self.binary:\n        #     self.logits = np.empty(0)\n        # else:\n        #     self.logits = np.empty((0,self.num_classes))\n            \n\n    def update(self, predictions, targets):\n        targets = targets.detach().cpu().long()\n        logits = predictions.detach().cpu()\n        predictions = self.to_prediction(logits)\n        \n        if len(logits.shape) == 4:\n            logits = logits.permute(0,2,3,1).reshape(-1, self.num_classes)\n            logits = F.softmax(logits, dim=1)\n        elif 'kappa' in self.metrics:\n            logits = F.softmax(logits, dim=1)\n        else:\n            if self.input_logits:\n                logits = F.sigmoid(logits)\n            logits = logits.contiguous().view(-1)\n        \n            \n        predictions = predictions.contiguous().view(-1)\n        targets = targets.contiguous().view(-1)\n        \n        # self.y_pred = np.concatenate((self.y_pred, predictions.numpy()), axis=0)\n        # self.y = np.concatenate((self.y, targets.numpy()), axis=0)\n        # self.logits = np.concatenate((self.logits, logits.numpy()), axis=0)\n\n        # update metrics\n        for m in self.metrics_fn.keys():\n            if m in self.logits_required_metrics:\n                self.metrics_fn[m].update(logits, targets)\n            else:\n                self.metrics_fn[m].update(predictions, targets)\n\n    def get_scores(self, digits=-1):\n        \n#         for i in range(self.num_classes):\n#             self.plot_auprc(self.y, self.logits, i)\n        \n        scores={}\n        for m in self.metrics:\n            scores.update(self._compute(m, digits))\n        return scores\n\n    # def count(self, name):\n    #     if name == 'gt' :\n    #         return count_class(self.y, self.num_classes)\n    #     elif name == 'pred': \n    #         return count_class(self.y_pred, self.num_classes)\n\n    def _compute(self, metric, digits=-1):\n        if not self.binary:\n            if metric in self.return_dict_metrics:\n                return self.metrics_fn[metric].compute(digits)\n            if metric in self.logits_required_metrics:\n                l=self.metrics_fn[metric].compute().tolist()\n                l.append(np.mean(l[1:]))\n                l=np.round(l,digits)\n                return {metric:l}\n            \n        score = self.metrics_fn[metric].compute().item()\n        score = score if digits == -1 else round(score, digits)\n        return {metric:score}\n\n    def reset(self):\n        for m in self.metrics_fn.keys():\n            self.metrics_fn[m].reset()\n        # self.y = np.empty(0)\n        # self.y_pred = np.empty(0)\n        # if self.binary:\n        #     self.logits = np.empty(0)\n        # else:\n        #     self.logits = np.empty((0,self.num_classes))\n    \n    def to_prediction(self, predictions):\n        if self.binary:\n            predictions = (predictions>self.threshold).to(torch.int16)\n        else:\n            predictions = torch.argmax(predictions, dim=1).long()\n        return predictions\n    \n    def name_val(self, label, metric, val):\n        l=label\n        label={'BK':None}\n        label.update(l)\n        named_val={}\n        for i,(k,v) in enumerate(label.items()):\n            named_val.update({metric+'_'+k:val[i]})\n        named_val.update({metric:val[-1]})\n        return named_val\n    \n    def plot_auprc(self, targets, logits, c):\n        targets=(targets==c).astype(np.int32)\n        logits=logits[:,c]\n        PrecisionRecallDisplay.from_predictions(targets ,logits)\n        precision, recall, th = precision_recall_curve(targets, logits)\n        print('precision:{}\\n'.format(precision))\n        print('recall:{}\\n'.format(recall))\n        print('threshhold:{}\\n'.format(th))\n        auprc=auc(recall, precision)\n        \n        plt.legend([f'AUPRC = {auprc:.2f}'], loc=\"lower left\")\n        plt.title(f'{c}th PR Curve')\n        plt.show()\n\nclass BinaryAUPRC():\n    def __init__(self):\n        self.logits = np.empty(0)\n        self.targets = np.empty(0)\n        \n    def update(self, logits, targets):\n        self.targets = np.concatenate((self.targets, targets.numpy()), axis=0)\n        self.logits = np.concatenate((self.logits, logits.numpy()), axis=0)\n        \n    def compute(self, digits=4):\n        precision, recall, thresholds = precision_recall_curve(self.targets, self.logits)\n        auprc = auc(recall, precision)\n        return np.round(auprc, digits)\n        \n    def reset(self):\n        self.logits = np.empty(0)\n        self.targets = np.empty(0)\n            \nclass SegMetrics():\n    def __init__(self, num_classes):\n        self.num_classes=num_classes\n        self.confusion_matrix = np.zeros((num_classes,num_classes))\n        self.cm=tm.MulticlassConfusionMatrix(num_classes=num_classes)\n        \n    def PA(self):\n        return np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n    \n    def PAC(self):\n        acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n        acc_class = np.nanmean(acc)\n        return acc_class\n    \n    def MIoU(self):\n        IoU = np.diag(self.confusion_matrix) / (\n            np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n            np.diag(self.confusion_matrix))\n        MIoU = np.nanmean(IoU)\n        MIoU_noback = np.nanmean(IoU[1:])\n        return MIoU, MIoU_noback\n    \n    def update(self, predictions, targets):\n        self.cm.update(predictions, targets)\n        \n    def reset(self):\n        self.cm = tm.MulticlassConfusionMatrix(num_classes=self.num_classes)\n\n    def compute(self, digits):\n        self.confusion_matrix=self.cm.compute().numpy().astype(np.int32)\n        cm=self.cm.normalized('true').numpy()*100\n        np.set_printoptions(suppress=True)\n        print(np.round(cm,2))\n        MIoU, MIoU_noback = self.MIoU()\n        return {\n            \"PA\": np.round(self.PA(), digits),\n            \"PAC\": np.round(self.PAC(), digits),\n            \"MIoU\": np.round(MIoU, digits),\n            \"MIoU_NoBack\": np.round(MIoU_noback, digits)\n        }\n\nclass QuadraticWeightedKappa():\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.conf_mat = torch.zeros((self.num_classes, self.num_classes), dtype=int)\n\n    def update(self, predictions, targets):\n        for i, p in enumerate(predictions):\n            self.conf_mat[int(targets[i])][int(p.item())] += 1\n\n    def compute(self):\n        return self.quadratic_weighted_kappa(self.conf_mat)\n\n    def reset(self):\n        self.conf_mat = torch.zeros((self.num_classes, self.num_classes), dtype=int)\n\n    def quadratic_weighted_kappa(self, conf_mat):\n        assert conf_mat.shape[0] == conf_mat.shape[1]\n        cate_num = conf_mat.shape[0]\n\n        # Quadratic weighted matrix\n        weighted_matrix = torch.zeros((cate_num, cate_num))\n        for i in range(cate_num):\n            for j in range(cate_num):\n                weighted_matrix[i][j] = 1 - float(((i - j)**2) / ((cate_num - 1)**2))\n\n        # Expected matrix\n        ground_truth_count = torch.sum(conf_mat, axis=1)\n        pred_count = torch.sum(conf_mat, axis=0)\n        expected_matrix = torch.outer(ground_truth_count, pred_count)\n\n        # Normalization\n        conf_mat = conf_mat / conf_mat.sum()\n        expected_matrix = expected_matrix / expected_matrix.sum()\n\n        observed = (conf_mat * weighted_matrix).sum()\n        expected = (expected_matrix * weighted_matrix).sum()\n        return (observed - expected) / (1 - expected)\n\nprint('metircs.py')\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:43.313245Z","iopub.execute_input":"2024-12-23T02:53:43.313539Z","iopub.status.idle":"2024-12-23T02:53:45.725873Z","shell.execute_reply.started":"2024-12-23T02:53:43.313510Z","shell.execute_reply":"2024-12-23T02:53:45.724977Z"}},"outputs":[{"name":"stdout","text":"metircs.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from torch import nn\nimport torch\nimport torch.nn.functional as F\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, dilation=1, groups=1):\n        #padding = (kernel_size - 1) // 2\n        if kernel_size==1:\n            padding=0\n        else:\n            padding=dilation\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU6(inplace=True)\n        )\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, dilation, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, dilation=dilation, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        ])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, downsample_factor=8, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n        \"\"\"\n        MobileNet V2 main class\n\n        Args:\n            num_classes (int): Number of classes\n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n            inverted_residual_setting: Network structure\n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n            Set to 1 to turn off rounding\n        \"\"\"\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(\"inverted_residual_setting should be non-empty \"\n                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        cur_downsample_factor = 2\n        dilation=1\n        previous_dilation = 1\n\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            previous_dilation = dilation\n            if cur_downsample_factor == downsample_factor:\n                stride = 1\n                dilation *= s\n            else:\n                stride = s\n                cur_downsample_factor *= s\n            output_channel = int(c * width_mult)\n\n            for i in range(n):\n                if i==0:\n                    features.append(block(input_channel, output_channel, stride, previous_dilation, expand_ratio=t))\n                else:\n                    features.append(block(input_channel, output_channel, 1, dilation, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        low_level_feature = self.features[:4](x)\n        x = self.features[4:-1](low_level_feature)\n        return low_level_feature, x\n\n\ndef _mobilenetv2(**kwargs):\n    print(kwargs)\n    url = kwargs.pop('url', -1)\n    \n    model = MobileNetV2(**kwargs)\n    \n    if url != -1:\n        weight_dict=torch.hub.load_state_dict_from_url(url, progress=True)\n        del_key = []\n        for key, _ in weight_dict.items():  # 遍历预训练权重的有序字典\n            if \"fc\" in key:  # 如果key中包含'fc'这个字段\n                del_key.append(key)\n\n        for key in del_key:  # 遍历要删除字段的list\n            del weight_dict[key]\n        # 抽出现有模型中的K,V\n        model_dict=model.state_dict()\n        # 新建权重字典，并更新\n        state_dict={k:v for k,v in weight_dict.items() if k in model_dict.keys()}\n        # 更新现有模型的权重字典\n        model_dict.update(state_dict)\n        # 载入更新后的权重字典\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef mobilenetv2(**kwargs):\n    pretrained = kwargs.pop('pretrained', -1)\n    if pretrained != -1 and pretrained:\n        kwargs['url'] = 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth'\n    \n    model = _mobilenetv2(**kwargs)\n    return model\n\nprint('mobilenetv2.py')","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:45.729710Z","iopub.execute_input":"2024-12-23T02:53:45.730094Z","iopub.status.idle":"2024-12-23T02:53:45.751020Z","shell.execute_reply.started":"2024-12-23T02:53:45.730065Z","shell.execute_reply":"2024-12-23T02:53:45.750101Z"}},"outputs":[{"name":"stdout","text":"mobilenetv2.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\n\"\"\"\nXception is adapted from https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/xception.py\n\nPorted to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n@author: tstandley\nAdapted by cadene\nCreates an Xception Model as defined in:\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\nREMEMBER to set your image size to 3x299x299 for both test and validation\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True, dilation=1):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides!=1:\n            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip=None\n\n        rep=[]\n\n        filters=in_filters\n        if grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=dilation, dilation=dilation, bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps-1):\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=dilation,dilation=dilation,bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=dilation,dilation=dilation,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3,strides,1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self,inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n        x+=skip\n        return x\n\n\nclass Xception(nn.Module):\n    \"\"\"\n    Xception optimized for the ImageNet dataset, as specified in\n    https://arxiv.org/pdf/1610.02357.pdf\n    \"\"\"\n    def __init__(self, num_classes=1000, downsample_factor=8):\n        \"\"\" Constructor\n        Args:\n            num_classes: number of classes\n        \"\"\"\n        super(Xception, self).__init__()\n\n        self.num_classes = num_classes\n        self.dilation = 1\n        if downsample_factor==8:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, True, True]\n        else:\n            replace_stride_with_dilation = [False, False, False, True]\n\n        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False) # 1 / 2\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        #do relu here\n\n        self.block1=self._make_block(64,128,2,2,start_with_relu=False,grow_first=True, dilate=replace_stride_with_dilation[0]) # 1 / 4\n        self.block2=self._make_block(128,256,2,2,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[1]) # 1 / 8\n        self.block3=self._make_block(256,728,2,2,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2]) # 1 / 16\n\n        self.block4=self._make_block(728,728,3,1,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2])\n        self.block5=self._make_block(728,728,3,1,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2])\n        self.block6=self._make_block(728,728,3,1,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2])\n        self.block7=self._make_block(728,728,3,1,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2])\n\n        self.block8=self._make_block(728,728,3,1,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2])\n        self.block9=self._make_block(728,728,3,1,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2])\n        self.block10=self._make_block(728,728,3,1,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2])\n        self.block11=self._make_block(728,728,3,1,start_with_relu=True,grow_first=True, dilate=replace_stride_with_dilation[2])\n\n        self.block12=self._make_block(728,1024,2,2,start_with_relu=True,grow_first=False, dilate=replace_stride_with_dilation[3]) # 1 / 32\n\n        self.conv3 = SeparableConv2d(1024,1536,3,1,1, dilation=self.dilation)\n        self.bn3 = nn.BatchNorm2d(1536)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        #do relu here\n        self.conv4 = SeparableConv2d(1536,2048,3,1,1, dilation=self.dilation)\n        self.bn4 = nn.BatchNorm2d(2048)\n\n        self.fc = nn.Linear(2048, num_classes)\n\n        # #------- init weights --------\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n        # #-----------------------------\n\n    def _make_block(self, in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True, dilate=False):\n        if dilate:\n            self.dilation *= strides\n            strides = 1\n        return Block(in_filters,out_filters,reps,strides,start_with_relu=start_with_relu,grow_first=grow_first, dilation=self.dilation)\n\n    def features(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n\n        low_level_feature = self.block1(x)\n        x = low_level_feature\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        return low_level_feature, x\n\n    def logits(self, features):\n        x = nn.ReLU(inplace=True)(features)\n\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        low_level_feature, x = self.features(input)\n        # x = self.logits(x)\n        return low_level_feature, x\n\n\ndef _xception(**kwargs):\n    print(kwargs)\n    url = kwargs.pop('url', -1)\n    \n    model = Xception(**kwargs)\n    \n    if url != -1:\n        try:\n            weight_dict=torch.hub.load_state_dict_from_url(url, progress=True)\n        except:\n            weight_dict=torch.load('/kaggle/input/pretrained_model/pytorch/xception/1/xception-43020ad28.pth')\n            # for k,v in weight_dict.items():\n            #     print(f'{k} -- {v.shape}')\n        del_key = []\n        for key, _ in weight_dict.items():  # 遍历预训练权重的有序字典\n            if \"fc\" in key:  # 如果key中包含'fc'这个字段\n                del_key.append(key)\n\n        for key in del_key:  # 遍历要删除字段的list\n            del weight_dict[key]\n        # 抽出现有模型中的K,V\n        model_dict=model.state_dict()\n        # 新建权重字典，并更新\n        state_dict={k:v for k,v in weight_dict.items() if k in model_dict.keys()}\n        # 更新现有模型的权重字典\n        model_dict.update(state_dict)\n        # 载入更新后的权重字典\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef xception(**kwargs):\n    pretrained = kwargs.pop('pretrained', -1)\n    if pretrained != -1 and pretrained:\n        kwargs['url'] = 'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth'\n    \n    model = _xception(**kwargs)\n    return model\n\nprint('xception.py')","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:45.752324Z","iopub.execute_input":"2024-12-23T02:53:45.752597Z","iopub.status.idle":"2024-12-23T02:53:45.780680Z","shell.execute_reply.started":"2024-12-23T02:53:45.752573Z","shell.execute_reply":"2024-12-23T02:53:45.779763Z"}},"outputs":[{"name":"stdout","text":"xception.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from collections import OrderedDict\nfrom functools import partial\nfrom typing import Any, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nNOKEY=-1\n\n# from model.modules import *\n\n\nclass _DenseAtrousLayer(nn.Module):\n    def __init__(\n        self, num_input_features: int, growth_rate: int, bn_size: int, drop_rate: float, memory_efficient: bool = False, dilation: int = 1 \n    ) -> None:\n        super().__init__()\n        self.norm1 = nn.BatchNorm2d(num_input_features)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n\n        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)\n\n        self.drop_rate = float(drop_rate)\n        self.memory_efficient = memory_efficient\n\n    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n        return bottleneck_output\n\n    # torchscript does not yet support *args, so we overload method\n    # allowing it to take either a List[Tensor] or single Tensor\n    def forward(self, input: Tensor) -> Tensor:  # noqa: F811\n        if isinstance(input, Tensor):\n            prev_features = [input]\n        else:\n            prev_features = input\n\n        if self.memory_efficient and self.any_requires_grad(prev_features):\n            if torch.jit.is_scripting():\n                raise Exception(\"Memory Efficient not supported in JIT\")\n\n            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n        else:\n            bottleneck_output = self.bn_function(prev_features)\n\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return new_features\n\n\nclass _DenseAtrousBlock(nn.ModuleDict):\n    _version = 2\n\n    def __init__(\n        self,\n        num_layers: int,\n        num_input_features: int,\n        bn_size: int,\n        growth_rate: int,\n        drop_rate: float,\n        memory_efficient: bool = False,\n        dilation: int = 1,\n    ) -> None:\n        super().__init__()\n        for i in range(num_layers):\n            layer = _DenseAtrousLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient,\n                dilation=dilation\n            )\n            self.add_module(\"denselayer%d\" % (i + 1), layer)\n\n    def forward(self, init_features: Tensor) -> Tensor:\n        features = [init_features]\n        for name, layer in self.items():\n            new_features = layer(features)\n            features.append(new_features)\n        return torch.cat(features, 1)\n\n\nclass _AtrousTransition(nn.Sequential):\n    def __init__(self, num_input_features: int, num_output_features: int, dilation: int = 1) -> None:\n        super().__init__()\n        self.norm = nn.BatchNorm2d(num_input_features)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n        if dilation == 1:\n            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)#减半尺寸\n\n\nclass DenseNetAtrous(nn.Module):\n    r\"\"\"Densenet-BC model class, based on\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\n\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n    \"\"\"\n\n    def __init__(\n        self,\n        growth_rate: int = 32,\n        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n        num_init_features: int = 64,\n        bn_size: int = 4,\n        drop_rate: float = 0,\n        num_classes: int = 1000,\n        memory_efficient: bool = False,\n        module_list = None,\n        downsample_factor=8,\n    ) -> None:\n\n        super().__init__()\n        \n        # self.module_list = module_list\n        # self.attention = module_list['attention']\n        \n        #x = [3,512,512]\n        # First convolution\n        self.low_features = nn.Sequential(\n            OrderedDict(\n                [\n                    (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),#x/2\n                    (\"norm0\", nn.BatchNorm2d(num_init_features)),\n                    (\"relu0\", nn.ReLU(inplace=True)),\n                    (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n                ]\n            )\n        )\n        self.features = nn.Sequential()\n            \n        self.cur_downsample_factor=2\n        self.dilation=1\n        # Each denseblock\n        num_features = num_init_features #初始通道数\n        for i, num_layers in enumerate(block_config):\n            if self.cur_downsample_factor == downsample_factor:\n                self.dilation*=2\n            else:\n                self.dilation=1\n                self.cur_downsample_factor*=2\n            block = _DenseAtrousBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient,\n                dilation=self.dilation\n            )\n            if i==0:\n                self.low_features.add_module(\"denseblock%d\" % (i + 1), block)\n            else:\n                self.features.add_module(\"denseblock%d\" % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _AtrousTransition(num_input_features=num_features, num_output_features=num_features // 2, dilation=self.dilation)#减半\n                if i==0:\n                    self.low_features.add_module(\"transition%d\" % (i + 1), trans)\n                    self.low_channels=num_features // 2\n                else:\n                    self.features.add_module(\"transition%d\" % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.output_channels=num_features\n        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n        \n        print(f'num_features:{num_features}')\n        # self.mymodules=get_modules(self.module_list, num_features, num_classes=num_classes)\n\n        # Linear layer\n        if isinstance(num_classes, list):\n            self.classifier=nn.ModuleList([nn.Linear(num_features, n) for n in num_classes])\n        else:\n            self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: Tensor) -> Tensor:\n        low_level_feature=self.low_features(x)\n        features = self.features(low_level_feature)\n        \n        # for i in self.mymodules:\n        #     features = i(features)\n        \n        # out = F.relu(features, inplace=True)\n        # out = F.adaptive_avg_pool2d(out, (1, 1))\n        # out = torch.flatten(out, 1)\n        # if isinstance(self.classifier, nn.ModuleList):\n        #     x = [classifier(out) for classifier in self.classifier]\n        # else:\n        #     x = self.classifier(out)\n        return low_level_feature, features\n\n\n\n\ndef _densenet_atrous(\n    growth_rate: int,\n    block_config: Tuple[int, int, int, int],\n    num_init_features: int,\n    **kwargs: Any,\n) -> DenseNetAtrous:\n    print(kwargs)\n    url = kwargs.pop('url', NOKEY)\n\n    model = DenseNetAtrous(growth_rate, block_config, num_init_features, **kwargs)\n    \n    if url != NOKEY:\n        weight_dict=torch.hub.load_state_dict_from_url(url, progress=True)\n        del_key = []\n        for key, _ in weight_dict.items():  # 遍历预训练权重的有序字典\n            if \"classifier\" in key:  # 如果key中包含'classifier'这个字段\n                del_key.append(key)\n\n        for key in del_key:  # 遍历要删除字段的list\n            del weight_dict[key]\n        # 抽出现有模型中的K,V\n        model_dict=model.state_dict()\n        # 新建权重字典，并更新\n        state_dict={k:v for k,v in weight_dict.items() if k in model_dict.keys()}\n        # 更新现有模型的权重字典\n        model_dict.update(state_dict)\n        # 载入更新后的权重字典\n        model.load_state_dict(model_dict)\n\n    return model\n\nDenseNetAtrousWeight={\n        'densenet121':'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n        'densenet161':'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n        'densenet169':'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n        'densenet201':'https://download.pytorch.org/models/densenet201-c1103571.pth'\n    }\n\n\ndef densenet_atrous121(**kwargs: Any) -> DenseNetAtrous:\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = DenseNetAtrousWeight['densenet121']\n\n    return _densenet_atrous(32, (6, 12, 24, 16), 64, **kwargs)\n\n\ndef densenet_atrous161(**kwargs: Any) -> DenseNetAtrous:\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = DenseNetAtrousWeight['densenet161']\n\n    return _densenet_atrous(48, (6, 12, 36, 24), 96, **kwargs)\n\ndef densenet_atrous169(**kwargs: Any) -> DenseNetAtrous:\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = DenseNetAtrousWeight['densenet169']\n\n    return _densenet_atrous(32, (6, 12, 32, 32), 64, **kwargs)\n\ndef densenet_atrous201(**kwargs: Any) -> DenseNetAtrous:\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = DenseNetAtrousWeight['densenet201']\n\n    return _densenet_atrous(32, (6, 12, 48, 32), 64, **kwargs)\n\nprint('densenet_atrous.py')","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:45.781909Z","iopub.execute_input":"2024-12-23T02:53:45.782265Z","iopub.status.idle":"2024-12-23T02:53:45.812597Z","shell.execute_reply.started":"2024-12-23T02:53:45.782229Z","shell.execute_reply":"2024-12-23T02:53:45.811815Z"}},"outputs":[{"name":"stdout","text":"densenet_atrous.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from model.densenet import densenet161\n# import xception\n# import mobilenetv2\n\n#-----------------------------------------#\n#   ASPP特征提取模块\n#   利用不同膨胀率的膨胀卷积进行特征提取\n#-----------------------------------------#\nclass ASPP(nn.Module):\n    def __init__(self, dim_in, dim_out, rate=1, bn_mom=0.1):\n        super(ASPP, self).__init__()\n        self.branch1 = nn.Sequential(\n                nn.Conv2d(dim_in, dim_out, 1, 1, padding=0, bias=True),\n                nn.BatchNorm2d(dim_out, momentum=bn_mom),\n                nn.ReLU(inplace=True),\n        )\n        self.branch2 = nn.Sequential(\n                nn.Conv2d(dim_in, dim_out, 3, 1, padding=6*rate, dilation=6*rate, bias=True),\n                nn.BatchNorm2d(dim_out, momentum=bn_mom),\n                nn.ReLU(inplace=True),    \n        )\n        self.branch3 = nn.Sequential(\n                nn.Conv2d(dim_in, dim_out, 3, 1, padding=12*rate, dilation=12*rate, bias=True),\n                nn.BatchNorm2d(dim_out, momentum=bn_mom),\n                nn.ReLU(inplace=True),    \n        )\n        self.branch4 = nn.Sequential(\n                nn.Conv2d(dim_in, dim_out, 3, 1, padding=18*rate, dilation=18*rate, bias=True),\n                nn.BatchNorm2d(dim_out, momentum=bn_mom),\n                nn.ReLU(inplace=True),    \n        )\n        self.branch5_conv = nn.Conv2d(dim_in, dim_out, 1, 1, 0,bias=True)\n        self.branch5_bn = nn.BatchNorm2d(dim_out, momentum=bn_mom)\n        self.branch5_relu = nn.ReLU(inplace=True)\n        \n        self.conv_cat = nn.Sequential(\n                nn.Conv2d(dim_out*5, dim_out, 1, 1, padding=0,bias=True),\n                nn.BatchNorm2d(dim_out, momentum=bn_mom),\n                nn.ReLU(inplace=True),        \n        )\n\n    def forward(self, x):\n        [b, c, row, col] = x.size()\n        # print('x.shape:{}'.format(x.size()))\n        #-----------------------------------------#\n        #   一共五个分支\n        #-----------------------------------------#\n        conv1x1 = self.branch1(x)\n        conv3x3_1 = self.branch2(x)\n        conv3x3_2 = self.branch3(x)\n        conv3x3_3 = self.branch4(x)\n        #-----------------------------------------#\n        #   第五个分支，全局平均池化(image pooling)+卷积\n        #-----------------------------------------#\n        global_feature = torch.mean(x,2,True)\n        global_feature = torch.mean(global_feature,3,True)#shape=(1,dim_out,1,1)\n        global_feature = self.branch5_conv(global_feature)\n        global_feature = self.branch5_bn(global_feature)\n        global_feature = self.branch5_relu(global_feature)\n        global_feature = F.interpolate(global_feature, (row, col), None, 'bilinear', True)#恢复原图尺寸\n        \n        #-----------------------------------------#\n        #   将五个分支的内容堆叠起来\n        #   然后1x1卷积整合特征。\n        #-----------------------------------------#\n        feature_cat = torch.cat([conv1x1, conv3x3_1, conv3x3_2, conv3x3_3, global_feature], dim=1)#在channel维度上拼接所有特征图\n        result = self.conv_cat(feature_cat)\n        return result\n\nclass DeepLab(nn.Module):\n    def __init__(self, num_classes, backbone=\"mobilenet\", pretrained=True, downsample_factor=16, **kwargs):\n        super(DeepLab, self).__init__()\n        if backbone==\"xception\":\n            #----------------------------------#\n            #   获得两个特征层\n            #   浅层特征    [128,128,256]\n            #   主干部分    [30,30,2048]\n            #----------------------------------#\n            self.backbone = xception(downsample_factor=downsample_factor, pretrained=pretrained)\n            in_channels = 2048\n            low_level_channels = 128\n        elif backbone==\"mobilenet\":\n            #----------------------------------#\n            #   获得两个特征层\n            #   浅层特征    [128,128,24]\n            #   主干部分    [30,30,320]\n            #----------------------------------#\n            self.backbone = mobilenetv2(downsample_factor=downsample_factor, pretrained=pretrained)\n            in_channels = 320\n            low_level_channels = 24\n        elif backbone=='densenet':\n            self.backbone = densenet_atrous161(downsample_factor=downsample_factor, pretrained=pretrained)\n            in_channels = self.backbone.output_channels\n            low_level_channels = self.backbone.low_channels\n        else:\n            raise ValueError('Unsupported backbone - `{}`, Use mobilenet, xception.'.format(backbone))\n\n        #-----------------------------------------#\n        #   ASPP特征提取模块\n        #   利用不同膨胀率的膨胀卷积进行特征提取\n        #   这里的rate可理解为如果downsample_factor=8的话，图片尺寸相比downsample_ractor=16要大一倍，所以可以用更大的空洞卷积核去卷积\n        #-----------------------------------------#\n        self.aspp = ASPP(dim_in=in_channels, dim_out=256, rate=16//downsample_factor)\n        \n        #----------------------------------#\n        #   浅层特征边\n        #----------------------------------#\n        self.shortcut_conv = nn.Sequential(\n            nn.Conv2d(low_level_channels, 48, 1),\n            nn.BatchNorm2d(48),\n            nn.ReLU(inplace=True)\n        )        \n\n        self.cat_conv = nn.Sequential(\n            nn.Conv2d(48+256, 256, 3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n\n            nn.Conv2d(256, 256, 3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n\n            nn.Dropout(0.1),\n        )\n        self.cls_conv = nn.Conv2d(256, num_classes, 1, stride=1)\n\n    def forward(self, x):\n        H, W = x.size(2), x.size(3)\n        #-----------------------------------------#\n        #   获得两个特征层\n        #   low_level_features: 浅层特征-进行卷积处理\n        #   x : 主干部分-利用ASPP结构进行加强特征提取\n        #-----------------------------------------#\n        low_level_features, x = self.backbone(x)\n        x = self.aspp(x)\n        low_level_features = self.shortcut_conv(low_level_features)\n        \n        #-----------------------------------------#\n        #   将加强特征边上采样\n        #   与浅层特征堆叠后利用卷积进行特征提取\n        #-----------------------------------------#\n        x = F.interpolate(x, size=(low_level_features.size(2), low_level_features.size(3)), mode='bilinear', align_corners=True)\n        x = self.cat_conv(torch.cat((x, low_level_features), dim=1))\n        x = self.cls_conv(x)\n        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True)\n        return x\n\nprint('deeplabv3+.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:45.813700Z","iopub.execute_input":"2024-12-23T02:53:45.813980Z","iopub.status.idle":"2024-12-23T02:53:45.837816Z","shell.execute_reply.started":"2024-12-23T02:53:45.813928Z","shell.execute_reply":"2024-12-23T02:53:45.836927Z"}},"outputs":[{"name":"stdout","text":"deeplabv3+.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import re\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Any, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom torch import Tensor\nNOKEY=-1\n\n# from model.modules import *\n\n\nclass _DenseLayer(nn.Module):\n    def __init__(\n        self, num_input_features: int, growth_rate: int, bn_size: int, drop_rate: float, memory_efficient: bool = False\n    ) -> None:\n        super().__init__()\n        self.norm1 = nn.BatchNorm2d(num_input_features)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n\n        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.drop_rate = float(drop_rate)\n        self.memory_efficient = memory_efficient\n\n    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n        return bottleneck_output\n\n    # torchscript does not yet support *args, so we overload method\n    # allowing it to take either a List[Tensor] or single Tensor\n    def forward(self, input: Tensor) -> Tensor:  # noqa: F811\n        if isinstance(input, Tensor):\n            prev_features = [input]\n        else:\n            prev_features = input\n\n        if self.memory_efficient and self.any_requires_grad(prev_features):\n            if torch.jit.is_scripting():\n                raise Exception(\"Memory Efficient not supported in JIT\")\n\n            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n        else:\n            bottleneck_output = self.bn_function(prev_features)\n\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return new_features\n\n\nclass _DenseBlock(nn.ModuleDict):\n    _version = 2\n\n    def __init__(\n        self,\n        num_layers: int,\n        num_input_features: int,\n        bn_size: int,\n        growth_rate: int,\n        drop_rate: float,\n        memory_efficient: bool = False,\n    ) -> None:\n        super().__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient,\n            )\n            self.add_module(\"denselayer%d\" % (i + 1), layer)\n\n    def forward(self, init_features: Tensor) -> Tensor:\n        features = [init_features]\n        for name, layer in self.items():\n            new_features = layer(features)\n            features.append(new_features)\n        return torch.cat(features, 1)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features: int, num_output_features: int) -> None:\n        super().__init__()\n        self.norm = nn.BatchNorm2d(num_input_features)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n\n\nclass DenseNet(nn.Module):\n    r\"\"\"Densenet-BC model class, based on\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\n\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n    \"\"\"\n\n    def __init__(\n        self,\n        growth_rate: int = 32,\n        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n        num_init_features: int = 64,\n        bn_size: int = 4,\n        drop_rate: float = 0,\n        num_classes: int = 1000,\n        memory_efficient: bool = False,\n        module_list = None,\n    ) -> None:\n\n        super().__init__()\n        \n        # self.module_list = module_list\n        # self.attention = module_list['attention']\n        \n        # First convolution\n        self.features = nn.Sequential(\n            OrderedDict(\n                [\n                    (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n                    (\"norm0\", nn.BatchNorm2d(num_init_features)),\n                    (\"relu0\", nn.ReLU(inplace=True)),\n                    (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n                ]\n            )\n        )\n\n        # Each denseblock\n        num_features = num_init_features #初始通道数\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient,\n            )\n            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\"transition%d\" % (i + 1), trans)\n                num_features = num_features // 2\n        self.last_num_features=num_features\n\n        # Final batch norm\n        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n        \n        # self.mymodules=get_modules(self.module_list, num_features, num_classes=num_classes)\n\n        # Linear layer\n        if isinstance(num_classes, list):\n            self.classifier=nn.ModuleList([nn.Linear(num_features, n) for n in num_classes])\n        else:\n            self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: Tensor) -> Tensor:\n        features = self.features(x)\n        \n        # for i in self.mymodules:\n        #     features = i(features)\n        \n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool2d(out, (1, 1))\n        out = torch.flatten(out, 1)\n        if isinstance(self.classifier, nn.ModuleList):\n            x = [classifier(out) for classifier in self.classifier]\n        else:\n            x = self.classifier(out)\n        return x\n\n\n\n\ndef _densenet(\n    growth_rate: int,\n    block_config: Tuple[int, int, int, int],\n    num_init_features: int,\n    **kwargs: Any,\n) -> DenseNet:\n    print(kwargs)\n    url = kwargs.pop('url', NOKEY)\n\n    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n    \n    if url != NOKEY:\n        weight_dict=torch.hub.load_state_dict_from_url(url, progress=True)\n        del_key = []\n        for key, _ in weight_dict.items():  # 遍历预训练权重的有序字典\n            if \"classifier\" in key:  # 如果key中包含'classifier'这个字段\n                del_key.append(key)\n\n        for key in del_key:  # 遍历要删除字段的list\n            del weight_dict[key]\n        # 抽出现有模型中的K,V\n        model_dict=model.state_dict()\n        # 新建权重字典，并更新\n        state_dict={k:v for k,v in weight_dict.items() if k in model_dict.keys()}\n        # 更新现有模型的权重字典\n        model_dict.update(state_dict)\n        # 载入更新后的权重字典\n        model.load_state_dict(model_dict)\n\n    return model\n\nDenseNetWeight={\n        'densenet121':'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n        'densenet161':'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n        'densenet169':'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n        'densenet201':'https://download.pytorch.org/models/densenet201-c1103571.pth'\n    }\n\n\ndef densenet121(**kwargs: Any) -> DenseNet:\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = DenseNetWeight['densenet121']\n\n    return _densenet(32, (6, 12, 24, 16), 64, **kwargs)\n\n\ndef densenet161(**kwargs: Any) -> DenseNet:\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = DenseNetWeight['densenet161']\n\n    return _densenet(48, (6, 12, 36, 24), 96, **kwargs)\n\ndef densenet169(**kwargs: Any) -> DenseNet:\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = DenseNetWeight['densenet169']\n\n    return _densenet(32, (6, 12, 32, 32), 64, **kwargs)\n\ndef densenet201(**kwargs: Any) -> DenseNet:\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = DenseNetWeight['densenet201']\n\n    return _densenet(32, (6, 12, 48, 32), 64, **kwargs)\n\nprint('densenet.py')","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:45.838965Z","iopub.execute_input":"2024-12-23T02:53:45.839219Z","iopub.status.idle":"2024-12-23T02:53:45.868270Z","shell.execute_reply.started":"2024-12-23T02:53:45.839196Z","shell.execute_reply":"2024-12-23T02:53:45.867509Z"}},"outputs":[{"name":"stdout","text":"densenet.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from functools import partial\nfrom typing import Any, Callable, List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn.parameter import Parameter\nNOKEY=-1\n\n# from model.modules import *\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    __constants__ = ['downsample']\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        \n        branch_features = planes * self.expansion\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None, module_list = None):\n        super(ResNet, self).__init__()\n        self.module_list = module_list\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n            self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        if isinstance(num_classes, list):\n            self.fc=nn.ModuleList([nn.Linear(512 * block.expansion, n) for n in num_classes])\n        else:\n            self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        if self.module_list:\n            self.mymodules=get_modules(self.module_list, 2048, num_classes=num_classes)\n        \n        self.attmap=self.module_list.get('use_attmap', False)\n        if self.attmap:\n            self.attmap_low = nn.ModuleList([nn.Sequential(\n                                                            nn.Conv2d(1, self.conv1.out_channels//2, 3, 2, 1),\n                                                            nn.Conv2d(self.conv1.out_channels//2, self.conv1.out_channels, 3, padding=1),\n                                                            nn.BatchNorm2d(self.conv1.out_channels),\n                                                            nn.ReLU(inplace=True),\n                                                            )\n                                            for _ in range(self.attmap)])\n            \n            self.attmap_high = nn.ModuleList([nn.Sequential(\n                                                            nn.Conv2d(self.conv1.out_channels, 1, 1),\n                                                            nn.BatchNorm2d(1),\n                                                            nn.ReLU(inplace=True)\n                                                            )\n                                            for _ in range(self.attmap)])\n            self.upconv = nn.ModuleList([\n                nn.Sequential(\n                            nn.Upsample(scale_factor=2),\n                            nn.Conv2d(1,1,1),\n                            nn.BatchNorm2d(1),\n                            nn.ReLU(inplace=True)\n                        )\n                for _ in range(self.attmap)\n            ])\n            self.cat_attmap = nn.Linear(512 * block.expansion * self.attmap, 512 * block.expansion)\n            self.high_conv1x1 = nn.Conv2d(512 * block.expansion, self.conv1.out_channels, 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x, attmaps=None):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        if attmaps != None:\n            low_feature=x\n            for i in range(len(attmaps)):\n                attmap = attmaps[i].to(low_feature.device)\n                attmap = attmap.unsqueeze(1)\n                attmap = self.attmap_low[i](attmap)\n                \n                attmaps[i] = attmap + low_feature\n        \n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        if self.module_list:\n            for i in self.mymodules:\n                x = i(x)\n\n        x = self.avgpool(x)\n        \n        if attmaps != None:\n            refined_attmaps=[]\n            high_feature = self.high_conv1x1(x)\n            for i, attmap in enumerate(attmaps):\n                refined_attmap=self.attmap_high[i](attmap*high_feature)\n                # print(f'refined_attmap:{refined_attmap.shape}')\n                refined_attmaps.append(self.upconv[i](refined_attmap))\n                attmap = refined_attmap+low_feature\n                attmap = self.layer1(attmap)\n                attmap = self.layer2(attmap)\n                attmap = self.layer3(attmap)\n                attmap = self.layer4(attmap)\n                \n                if self.module_list:\n                    for m in self.mymodules:\n                        attmap = m(attmap)\n                attmaps[i] = self.avgpool(attmap)\n                \n            x = torch.cat(attmaps, dim=1)\n            x = torch.flatten(x, 1)\n            x = self.cat_attmap(x)\n        \n        x = torch.flatten(x, 1)\n        \n        if isinstance(self.fc, nn.ModuleList):\n            out = [fc(x) for fc in self.fc]\n        else:\n            out = self.fc(x)\n        \n        if attmaps != None:\n            return out, torch.cat(refined_attmaps, dim=1)\n        return out\n\n\ndef _resnet(arch, block, layers, **kwargs):\n    print(kwargs)\n    url = kwargs.pop('url', NOKEY)\n    weights = kwargs.pop('weights', NOKEY)\n    \n    model = ResNet(block, layers, **kwargs)\n    \n    if url != NOKEY or weights != NOKEY:\n        if url != NOKEY:\n            print('backbone loading Imagenet weights')\n            weight_dict=torch.hub.load_state_dict_from_url(url, progress=True)\n        elif weights != NOKEY and weights != None:\n            print('backbone loading custome weights')\n            weight_dict=torch.load(weights, map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n            weight_dict=weight_dict['model_state_dict']\n        else:\n            return model\n        del_key = []\n        for key, _ in weight_dict.items():  # 遍历预训练权重的有序字典\n            if \"fc\" in key:  # 如果key中包含'fc'这个字段\n                del_key.append(key)\n\n        for key in del_key:  # 遍历要删除字段的list\n            del weight_dict[key]\n        # 抽出现有模型中的K,V\n        model_dict=model.state_dict()\n        # 新建权重字典，并更新\n        state_dict={k:v for k,v in weight_dict.items() if k in model_dict.keys()}\n        # print(model_dict.keys())\n        # print(weight_dict.keys())\n        # print(state_dict.keys())\n        # 更新现有模型的权重字典\n        model_dict.update(state_dict)\n        # 载入更新后的权重字典\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet50(**kwargs):\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = 'https://download.pytorch.org/models/resnet50-0676ba61.pth'\n    \n    model = _resnet('ResNet-50', Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet101(**kwargs):\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = 'https://download.pytorch.org/models/resnet101-cd907fc2.pth'\n    \n    model = _resnet('ResNet-101', Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef resnet152(**kwargs):\n    pretrained = kwargs.pop('pretrained',NOKEY)\n    if pretrained != NOKEY and pretrained:\n        kwargs['url'] = 'https://download.pytorch.org/models/resnet152-f82ba261.pth'\n    \n    model = _resnet('ResNet-152', Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\ndef get_resnet_stages(downsample_factor, backbone, module_list, args, **kwargs):\n    rsd=[downsample_factor<(2**i) for i in range(3,6)]\n    if backbone == 'rs50':\n        encoder = resnet50(pretrained=args.pretrained, module_list=module_list, weights=args.backbone_weights, replace_stride_with_dilation=rsd)\n    elif backbone == 'rs101':\n        encoder = resnet101(pretrained=args.pretrained, module_list=module_list, replace_stride_with_dilation=rsd)\n    elif backbone == 'rs152':\n        encoder = resnet152(pretrained=args.pretrained, module_list=module_list, replace_stride_with_dilation=rsd)\n    else:\n        assert f'{backbone} does not exists!'\n    \n    conv1 = nn.Sequential(encoder.conv1,\n                               encoder.bn1,\n                               encoder.relu)#x/2\n    \n    conv2=nn.Sequential(encoder.maxpool,\n                             encoder.layer1)#x/4\n    \n    conv3=nn.Sequential(encoder.layer2)#x/8\n    \n    conv4=nn.Sequential(encoder.layer3)#x/16\n    \n    conv5=nn.Sequential(encoder.layer4)#x/32\n\n    features = [conv1, conv2, conv3, conv4, conv5]\n    channels = [64, 256, 512, 1024, 2048]\n\n    return features, channels\n\nprint('resnet.py')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:45.869344Z","iopub.execute_input":"2024-12-23T02:53:45.869605Z","iopub.status.idle":"2024-12-23T02:53:45.909686Z","shell.execute_reply.started":"2024-12-23T02:53:45.869580Z","shell.execute_reply":"2024-12-23T02:53:45.908788Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"resnet.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport einops\n\nclass conv_block(nn.Module):\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 kernel_size=(3, 3),\n                 stride=(1, 1),\n                 padding=(1, 1),\n                 dilation=(1, 1),\n                 norm_type='bn',\n                 activation=True,\n                 use_bias=True, \n                 ):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels=in_features,\n                              out_channels=out_features,\n                              kernel_size=kernel_size,\n                              stride=stride,\n                              padding=padding,\n                              dilation=dilation,\n                              bias=use_bias)\n\n        self.norm_type = norm_type\n        self.act = activation\n\n        if self.norm_type == 'gn':\n            self.norm = nn.GroupNorm(32 if out_features >= 32 else out_features, out_features)\n        if self.norm_type == 'bn':\n            self.norm = nn.BatchNorm2d(out_features)\n        if self.act:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.norm_type is not None:\n            x = self.norm(x)\n        if self.act:\n            x = self.relu(x)\n        return x\n\nclass depthwise_conv_block(nn.Module):\n    def __init__(self, \n                in_features, \n                out_features,\n                kernel_size=(3, 3),\n                stride=(1, 1), \n                padding=(1, 1), \n                dilation=(1, 1),\n                groups=None, \n                norm_type='bn',\n                activation=True, \n                use_bias=True,\n                pointwise=False, \n                ):\n        super().__init__()\n        self.pointwise = pointwise\n        self.norm = norm_type\n        self.act = activation\n        self.depthwise = nn.Conv2d(\n            in_channels=in_features,\n            out_channels=in_features if pointwise else out_features,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            dilation=dilation, \n            bias=use_bias)\n        if pointwise:\n            self.pointwise = nn.Conv2d(in_features, \n                                        out_features, \n                                        kernel_size=(1, 1), \n                                        stride=(1, 1), \n                                        padding=(0, 0),\n                                        dilation=(1, 1), \n                                        bias=use_bias)\n\n        self.norm_type = norm_type\n        self.act = activation\n\n        if self.norm_type == 'gn':\n            self.norm = nn.GroupNorm(32 if out_features >= 32 else out_features, out_features)\n        if self.norm_type == 'bn':\n            self.norm = nn.BatchNorm2d(out_features)\n        if self.act:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        if self.pointwise:\n            x = self.pointwise(x)\n        if self.norm_type is not None:\n            x = self.norm(x)\n        if self.act:\n            x = self.relu(x)\n        return x\n\nclass depthwise_projection(nn.Module):\n    def __init__(self, \n                in_features, \n                out_features, \n                groups,\n                kernel_size=(1, 1), \n                padding=(0, 0), \n                norm_type=None, \n                activation=False, \n                pointwise=False) -> None:\n        super().__init__()\n\n        self.proj = depthwise_conv_block(in_features=in_features, \n                                        out_features=out_features, \n                                        kernel_size=kernel_size,\n                                        padding=padding,\n                                        groups=groups,\n                                        pointwise=pointwise, \n                                        norm_type=norm_type,\n                                        activation=activation)\n                            \n    def forward(self, x):\n        P = int(x.shape[1] ** 0.5)\n        x = einops.rearrange(x, 'B (H W) C-> B C H W', H=P) \n        x = self.proj(x)\n        x = einops.rearrange(x, 'B C H W -> B (H W) C')\n        return x\n    \nclass ScaleDotProduct(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.softmax = nn.Softmax(dim=-1)\n                                                    \n    def forward(self, x1, x2, x3, scale):\n        x2 = x2.transpose(-2, -1) #b n p*p ck || b n ck p*p\n        x12 = torch.einsum('bhcw, bhwk -> bhck', x1, x2) * scale #可以用torch.matmul代替\n        att = self.softmax(x12) #b n cq ck | x3: b n cv p*p || #b n p*p p*p | x3:b n p*p cv\n        x123 = torch.einsum('bhcw, bhwk -> bhck', att, x3) \n        return x123 #b n cq p*p || b n p*p cv\n\nclass PoolEmbedding(nn.Module):\n    def __init__(self,\n                pooling,\n                patch,\n                ) -> None:\n        super().__init__()\n        self.projection = pooling(output_size=(patch, patch))\n\n    def forward(self, x):\n        x = self.projection(x)\n        x = einops.rearrange(x, 'B C H W -> B (H W) C')        \n        return x\n\nclass ChannelCrossAttention(nn.Module):\n    def __init__(self, in_features, out_features, n_heads=1) -> None:\n        super().__init__()\n        self.n_heads = n_heads\n        self.q_map = depthwise_projection(in_features=out_features, \n                                            out_features=out_features, \n                                            groups=out_features)\n        self.k_map = depthwise_projection(in_features=in_features, \n                                            out_features=in_features, \n                                            groups=in_features)\n        self.v_map = depthwise_projection(in_features=in_features, \n                                            out_features=in_features, \n                                            groups=in_features) \n\n        self.projection = depthwise_projection(in_features=out_features, \n                                    out_features=out_features, \n                                    groups=out_features)\n        self.sdp = ScaleDotProduct()        \n        \n\n    def forward(self, x):\n        q, k, v = x[0], x[1], x[2]\n        q = self.q_map(q)\n        k = self.k_map(k)\n        v = self.v_map(v)\n        b, hw, c_q = q.shape\n        c = k.shape[2]\n        scale = c ** -0.5                     \n        q = q.reshape(b, hw, self.n_heads, c_q // self.n_heads).permute(0, 2, 1, 3).transpose(2, 3)# b n c_q p*p\n        k = k.reshape(b, hw, self.n_heads, c // self.n_heads).permute(0, 2, 1, 3).transpose(2, 3)\n        v = v.reshape(b, hw, self.n_heads, c // self.n_heads).permute(0, 2, 1, 3).transpose(2, 3)\n        att = self.sdp(q, k ,v, scale).permute(0, 3, 1, 2).flatten(2) #b p*p n c_q | flatten(2) -> b p*p n*c_q\n        att = self.projection(att)\n        return att# b p*p c_q\n\nclass SpatialCrossAttention(nn.Module):\n    def __init__(self, in_features, out_features, n_heads=4) -> None:\n        super().__init__()\n        self.n_heads = n_heads\n\n        self.q_map = depthwise_projection(in_features=in_features, \n                                            out_features=in_features, \n                                            groups=in_features)\n        self.k_map = depthwise_projection(in_features=in_features, \n                                            out_features=in_features, \n                                            groups=in_features)\n        self.v_map = depthwise_projection(in_features=out_features, \n                                            out_features=out_features, \n                                            groups=out_features)       \n\n        self.projection = depthwise_projection(in_features=out_features, \n                                    out_features=out_features, \n                                    groups=out_features)                                             \n        self.sdp = ScaleDotProduct()        \n\n    def forward(self, x):\n        q, k, v = x[0], x[1], x[2]\n        q = self.q_map(q)\n        k = self.k_map(k)\n        v = self.v_map(v)  \n        b, hw, c = q.shape\n        c_v = v.shape[2]\n        scale = (c // self.n_heads) ** -0.5        \n        q = q.reshape(b, hw, self.n_heads, c // self.n_heads).permute(0, 2, 1, 3) #b n p*p c/n\n        k = k.reshape(b, hw, self.n_heads, c // self.n_heads).permute(0, 2, 1, 3)\n        v = v.reshape(b, hw, self.n_heads, c_v // self.n_heads).permute(0, 2, 1, 3) #b n p*p c_v/n\n        att = self.sdp(q, k ,v, scale).transpose(1, 2).flatten(2)#b p*p n c_v/n | flatten(2) -> b p*p n*c_v/n\n        x = self.projection(att) # b p*p c_v\n        return x\n\nclass CCSABlock(nn.Module):\n    def __init__(self, \n                features, \n                channel_head, \n                spatial_head, \n                spatial_att=True, \n                channel_att=True) -> None:\n        super().__init__()\n        self.channel_att = channel_att\n        self.spatial_att = spatial_att\n        if self.channel_att:\n            self.channel_norm = nn.ModuleList([nn.LayerNorm(in_features,\n                                                    eps=1e-6) \n                                                    for in_features in features])#对每张图片进行标准化   \n\n            self.c_attention = nn.ModuleList([ChannelCrossAttention(\n                                                in_features=sum(features),\n                                                out_features=feature,\n                                                n_heads=head, \n                                        ) for feature, head in zip(features, channel_head)])\n        if self.spatial_att:\n            self.spatial_norm = nn.ModuleList([nn.LayerNorm(in_features,\n                                                    eps=1e-6) \n                                                    for in_features in features])          \n          \n            self.s_attention = nn.ModuleList([SpatialCrossAttention(\n                                                    in_features=sum(features),\n                                                    out_features=feature,\n                                                    n_heads=head, \n                                                    ) \n                                                    for feature, head in zip(features, spatial_head)])\n\n    def forward(self, x):\n        if self.channel_att:\n            x_ca = self.channel_attention(x)\n            x = self.m_sum(x, x_ca)   \n        if self.spatial_att:\n            x_sa = self.spatial_attention(x)\n            x = self.m_sum(x, x_sa)   \n        return x\n\n    def channel_attention(self, x):\n        x_c = self.m_apply(x, self.channel_norm) #b p*p c\n        x_cin = self.cat(*x_c) #b p*p c*4\n        x_in = [[q, x_cin, x_cin] for q in x_c]\n        x_att = self.m_apply(x_in, self.c_attention)\n        return x_att    \n\n    def spatial_attention(self, x):\n        x_c = self.m_apply(x, self.spatial_norm)\n        x_cin = self.cat(*x_c)\n        x_in = [[x_cin, x_cin, v] for v in x_c]        \n        x_att = self.m_apply(x_in, self.s_attention)\n        return x_att \n        \n\n    def m_apply(self, x, module):\n        return [module[i](j) for i, j in enumerate(x)]\n\n    def m_sum(self, x, y):\n        return [xi + xj for xi, xj in zip(x, y)]    \n\n    def cat(self, *args):\n        return torch.cat((args), dim=2)\n\n\n\nclass DCA(nn.Module):\n    def __init__(self,\n                features,\n                patch=28,\n                n=1,\n                ):\n        super().__init__()\n        self.n = n\n        self.features = features\n        self.spatial_head = [4, 4, 4, 4]\n        self.channel_head = [1, 1, 1, 1]\n        self.channel_att = True\n        self.spatial_att = True\n        self.patch = patch\n        self.patch_avg = nn.ModuleList([PoolEmbedding(\n                                                    pooling = nn.AdaptiveAvgPool2d,            \n                                                    patch=patch,\n                                                    ) #b h w c->b p*p c\n                                                    for _ in features])                \n        self.avg_map = nn.ModuleList([depthwise_projection(in_features=feature,\n                                                            out_features=feature, \n                                                            kernel_size=(1, 1),\n                                                            padding=(0, 0),\n                                                            groups=feature\n                                                            ) #1*1 DW\n                                                    for feature in features])         \n                                \n        self.attention = nn.ModuleList([\n                                        CCSABlock(features=features, \n                                                  channel_head=self.channel_head, \n                                                  spatial_head=self.spatial_head, \n                                                  channel_att=self.channel_att, \n                                                  spatial_att=self.spatial_att) \n                                                  for _ in range(n)])                                                   \n        self.bn_relu = nn.ModuleList([nn.Sequential(\n                                                    nn.BatchNorm2d(feature), \n                                                    nn.ReLU()\n                                                    ) \n                                                    for feature in features])\n    \n    def forward(self, raw):\n        x = self.m_apply(raw, self.patch_avg)\n        x = self.m_apply(x, self.avg_map)\n        for block in self.attention:\n            x = block(x)\n        x = [self.reshape(i) for i in x]\n#         for i in range(len(x)):\n#             print('after block-----{}----------'.format(i))\n#             print(x[i].shape)\n        x = self.upsample(x, raw)\n        x_out = self.m_sum(x, raw)\n        x_out = self.m_apply(x_out, self.bn_relu)\n        return (*x_out, )      \n\n    def m_apply(self, x, module):\n        return [module[i](j) for i, j in enumerate(x)]\n\n    def m_sum(self, x, y):\n        # for i in range(len(x)):\n        #     print('-----{}----------'.format(i))\n        #     print(x[i].shape)\n        #     print(y[i].shape)\n        return [xi + xj for xi, xj in zip(x, y)]  \n        \n    def reshape(self, x):\n        return einops.rearrange(x, 'B (H W) C-> B C H W', H=self.patch) \n\n    def upsample(self, x, raw):\n        for i in range(len(x)):\n            x[i] = F.interpolate(x[i], scale_factor=raw[i].shape[2]//x[i].shape[2], mode=\"nearest\")\n        return x\n\nprint('dca')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:45.910973Z","iopub.execute_input":"2024-12-23T02:53:45.911279Z","iopub.status.idle":"2024-12-23T02:53:45.953520Z","shell.execute_reply.started":"2024-12-23T02:53:45.911246Z","shell.execute_reply":"2024-12-23T02:53:45.952699Z"}},"outputs":[{"name":"stdout","text":"dca\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn.parameter import Parameter\nfrom torch import Tensor\nimport math\nimport cv2\nimport torch_dct as dct\nimport einops\n# from model.dca import *\n# from sklearn.cross_decomposition._pls import CCA\nNOKEY=-1\n\nc2wh = dict([(64,56), (128,28), (256,14) ,(512,7)])\nmy_c2wh = dict([(512,16),(256,32)])\nfreq_method='top16'\nend_modules=['mccsa']\n\ndef channel2wh(channels):\n    channels=channels//4\n    wh=my_c2wh.get(channels, -1)\n    if wh == -1:\n        if channels < 256:\n            wh=64\n        else:\n            wh=16\n    return wh\n\ndef get_modules(module_dict, out_c, **kwargs):\n    default_values={'fm':'top16'}\n    mymodules=nn.ModuleList()\n    module_list=list(module_dict.values())\n    for i in module_list:\n        m=None\n        if i == 'aspp':\n            m=ASPPModule(out_c)\n        elif i == 'biaspp':\n            m=ASPPModule(out_c,bi=True)\n        elif i == 'sa':\n            m=sa_layer(out_c)\n        elif i == 'cbam':\n            m=nn.Sequential(\n                ChannelAttention(out_c),\n                SpatialAttention(),\n            )\n        elif i == 'myffs':\n            m=nn.Sequential(\n                DFCAM(out_c, channel2wh(out_c), channel2wh(out_c), freq_size=8, freq_sel_method = module_dict.get('freq_method', 'dct16'), fc='conv'),\n                FSpatialAttention(transform='dct'),\n            )\n        elif i == 'fcbam':\n            m=nn.Sequential(\n                MultiSpectralAttentionLayer(out_c, channel2wh(out_c), channel2wh(out_c), freq_sel_method = module_dict.get('freq_method', default_values['fm'])),\n                SpatialAttention(),\n            )\n        elif i == 'fsa':\n            m=nn.Sequential(\n                DFCAM(out_c, channel2wh(out_c), channel2wh(out_c), freq_sel_method = module_dict.get('freq_method', default_values['fm']), fc='conv'),\n                SpatialAttention(),\n            )\n        elif i == 'cafs':\n            m=nn.Sequential(\n                ChannelAttention(out_c),\n                FSpatialAttention(transform='dct'),\n            )\n        \n            \n        if m!=None: mymodules.append(m)\n    return mymodules\n\ndef transformTo(x, transform='fft'):\n    if transform == 'dct':\n        x_np=x.detach().cpu().numpy()\n#         for b in range(x_np.shape[0]):\n#             for c in range(x_np.shape[1]):\n#                 x_np[b,c]=cv2.dct(x_np[b,c])\n        x=dct.dct_2d(x)\n        x=torch.from_numpy(x_np).to(x.device)\n        return x\n    elif transform == 'fft':\n        return torch.real(torch.fft.fft2(x, dim=(-2, -1),norm='forward'))\n\n'''\n#ASPP\n'''\n   \nclass ASPPConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        modules = [\n            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ]\n        super(ASPPConv, self).__init__(*modules)\n\nclass ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels, out_channels):\n        super(ASPPPooling, self).__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        x = super(ASPPPooling, self).forward(x)\n        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n\nclass ASPPModule(nn.Module):\n    def __init__(self, in_channels, atrous_rates=[6, 12, 18], bi=False):\n        super(ASPPModule, self).__init__()\n        self.bi=bi\n        out_channels = 256\n        modules = []\n        modules.append(nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)))\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        modules.append(ASPPConv(in_channels, out_channels, rate1))\n        modules.append(ASPPConv(in_channels, out_channels, rate2))\n        modules.append(ASPPConv(in_channels, out_channels, rate3))\n        modules.append(ASPPPooling(in_channels, out_channels))\n\n        self.convs = nn.ModuleList(modules)\n\n        self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, in_channels, 1, bias=False),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),)\n\n    def forward(self, x):\n#         print('aspp')\n        res = []\n        for conv in self.convs:\n            res.append(conv(x))\n            \n        if self.bi:\n            mid=res[1]+res[2]\n            res[3]=mid+res[3]\n            res[2]=mid+res[3]\n            res[1]=res[1]+res[2]\n        \n        res = torch.cat(res, dim=1)\n        return self.project(res)\n\n\n'''\nShuffle Attention\n'''\n\nclass sa_layer(nn.Module):\n    \"\"\"Constructs a Channel Spatial Group module.\n\n    Args:\n        k_size: Adaptive selection of kernel size\n    \"\"\"\n\n    def __init__(self, channel, groups=64):\n        super(sa_layer, self).__init__()\n        self.groups = groups\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.cweight = Parameter(torch.zeros(1, channel // (2 * groups), 1, 1))\n        self.cbias = Parameter(torch.ones(1, channel // (2 * groups), 1, 1))\n        self.sweight = Parameter(torch.zeros(1, channel // (2 * groups), 1, 1))\n        self.sbias = Parameter(torch.ones(1, channel // (2 * groups), 1, 1))\n\n        self.sigmoid = nn.Sigmoid()\n        self.gn = nn.GroupNorm(channel // (2 * groups), channel // (2 * groups))\n\n    @staticmethod\n    def channel_shuffle(x, groups):\n        b, c, h, w = x.shape\n\n        x = x.reshape(b, groups, -1, h, w)\n        x = x.permute(0, 2, 1, 3, 4)\n\n        # flatten\n        x = x.reshape(b, -1, h, w)\n\n        return x\n\n    def forward(self, x):\n#         print('sa')\n        b, c, h, w = x.shape\n\n        x = x.reshape(b * self.groups, -1, h, w)\n        x_0, x_1 = x.chunk(2, dim=1)\n\n        # channel attention\n        xn = self.avg_pool(x_0)\n        xn = self.cweight * xn + self.cbias\n        xn = x_0 * self.sigmoid(xn)\n\n        # spatial attention\n        xs = self.gn(x_1)\n        xs = self.sweight * xs + self.sbias\n        xs = x_1 * self.sigmoid(xs)\n\n        # concatenate along channel axis\n        out = torch.cat([xn, xs], dim=1)\n        out = out.reshape(b, -1, h, w)\n\n        out = self.channel_shuffle(out, 2)\n        return out\n    \n\n    \n'''\nCBAM\n'''\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n           \n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n#         print('cabm')\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        out = self.sigmoid(out)*x\n        return out\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv1(out)\n        return self.sigmoid(out)*x\n    \nclass FChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(FChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n           \n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        f_x=transformTo(x)\n        f_avg_out = self.fc(self.avg_pool(f_x))\n        f_max_out = self.fc(self.max_pool(f_x))\n        f_out=f_avg_out + f_max_out\n        f_out=self.sigmoid(f_out)*f_x\n        return f_out\n    \nclass FSpatialAttention(nn.Module):\n    def __init__(self,transform='fft', kernel_size=7):\n        super(FSpatialAttention, self).__init__()\n\n        self.transform=transform\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        f_x=transformTo(x, self.transform)\n        f_avg_out = torch.mean(f_x, dim=1, keepdim=True)\n        f_max_out, _ = torch.max(f_x, dim=1, keepdim=True)\n        f_out = torch.cat([f_avg_out, f_max_out], dim=1)\n        \n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        \n        out = out*f_out\n        out = self.conv1(out)\n        \n        return self.sigmoid(out)*x\n\n'''\nFcaNet\n'''\n   \ndef get_freq_indices(method):\n    assert method in ['top1','top2','top4','top8','top16','top32',\n                      'bot1','bot2','bot4','bot8','bot16','bot32',\n                      'low1','low2','low4','low8','low16','low32',\n                      'dct2','dct4','dct8','dct16','dct32',\n                      'idr2','idr4','idr8','idr16','idr32',]\n    num_freq = int(method[3:])\n    if 'top' in method:\n        all_top_indices_x = [0,0,6,0,0,1,1,4,5,1,3,0,0,0,3,2,4,6,3,5,5,2,6,5,5,3,3,4,2,2,6,1]\n        all_top_indices_y = [0,1,0,5,2,0,2,0,0,6,0,4,6,3,5,2,6,3,3,3,5,1,1,2,4,2,1,1,3,0,5,3]\n        mapper_x = all_top_indices_x[:num_freq]\n        mapper_y = all_top_indices_y[:num_freq]\n    elif 'low' in method:\n        all_low_indices_x = [0,0,1,1,0,2,2,1,2,0,3,4,0,1,3,0,1,2,3,4,5,0,1,2,3,4,5,6,1,2,3,4]\n        all_low_indices_y = [0,1,0,1,2,0,1,2,2,3,0,0,4,3,1,5,4,3,2,1,0,6,5,4,3,2,1,0,6,5,4,3]\n        mapper_x = all_low_indices_x[:num_freq]\n        mapper_y = all_low_indices_y[:num_freq]\n    elif 'bot' in method:\n        all_bot_indices_x = [6,1,3,3,2,4,1,2,4,4,5,1,4,6,2,5,6,1,6,2,2,4,3,3,5,5,6,2,5,5,3,6]\n        all_bot_indices_y = [6,4,4,6,6,3,1,4,4,5,6,5,2,2,5,1,4,3,5,0,3,1,1,2,4,2,1,1,5,3,3,3]\n        mapper_x = all_bot_indices_x[:num_freq]\n        mapper_y = all_bot_indices_y[:num_freq]\n    elif 'dct' in method:\n        all_bot_indices_x = [0, 4, 1, 3, 2, 0, 6, 0, 5, 2, 4, 3, 7, 6, 2, 2, 6, 2, 6, 6, 1, 5, 5, 6, 7, 5, 0, 4, 3, 4, 0, 5]\n        all_bot_indices_y = [0, 2, 4, 7, 4, 6, 0, 5, 1, 6, 4, 0, 6, 5, 3, 7, 4, 1, 3, 6, 0, 3, 4, 1, 3, 7, 7, 5, 6, 1, 1, 6]\n        mapper_x = all_bot_indices_x[:num_freq]\n        mapper_y = all_bot_indices_y[:num_freq]\n    elif 'idr' in method:\n        all_bot_indices_x = [0, 0, 1, 0, 2, 3, 4, 4, 6, 6, 0, 0, 2, 3, 7, 0, 1, 6, 6, 6, 7, 7, 0, 1, 1, 2, 2, 4, 5, 5, 5, 6]\n        all_bot_indices_y = [0, 1, 6, 3, 0, 0, 1, 6, 0, 1, 4, 7, 7, 6, 2, 2, 3, 3, 4, 6, 4, 6, 5, 0, 2, 3, 6, 5, 0, 2, 7, 5]\n        mapper_x = all_bot_indices_x[:num_freq]\n        mapper_y = all_bot_indices_y[:num_freq]\n    else:\n        raise NotImplementedError\n    return mapper_x, mapper_y\n\nclass MultiSpectralAttentionLayer(torch.nn.Module):\n    def __init__(self, channel, dct_h, dct_w, reduction = 16, freq_size=7, freq_sel_method = 'top16'):\n        super(MultiSpectralAttentionLayer, self).__init__()\n        self.reduction = reduction\n        self.dct_h = dct_h\n        self.dct_w = dct_w\n\n        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n        self.num_split = len(mapper_x)\n        mapper_x = [temp_x * (dct_h // freq_size) for temp_x in mapper_x]\n        mapper_y = [temp_y * (dct_w // freq_size) for temp_y in mapper_y]\n        # make the frequencies in different sizes are identical to a 7x7 frequency space\n        # eg, (2,2) in 14x14 is identical to (1,1) in 7x7\n\n        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, channel)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        n,c,h,w = x.shape\n        x_pooled = x\n        if h != self.dct_h or w != self.dct_w:\n            x_pooled = torch.nn.functional.adaptive_avg_pool2d(x, (self.dct_h, self.dct_w))\n            # If you have concerns about one-line-change, don't worry.   :)\n            # In the ImageNet models, this line will never be triggered. \n            # This is for compatibility in instance segmentation and object detection.\n        y = self.dct_layer(x_pooled)\n\n        y = self.fc(y).view(n, c, 1, 1)\n        return x * y.expand_as(x)\n    \nclass DCTSpectralLayer(torch.nn.Module):\n    def __init__(self, channel, dct_h, dct_w, reduction = 16, freq_size=8, mapper=[0,0]):\n        super(DCTSpectralLayer, self).__init__()\n        self.reduction = reduction\n        self.dct_h = dct_h\n        self.dct_w = dct_w\n\n        mapper_x, mapper_y = [mapper[0]],[mapper[1]]\n        self.num_split = len(mapper_x)\n        mapper_x = [temp_x * (dct_h // freq_size) for temp_x in mapper_x]\n        mapper_y = [temp_y * (dct_w // freq_size) for temp_y in mapper_y]\n        # make the frequencies in different sizes are identical to a 7x7 frequency space\n        # eg, (2,2) in 14x14 is identical to (1,1) in 7x7\n\n        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, channel)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        n,c,h,w = x.shape\n        x_pooled = x\n        if h != self.dct_h or w != self.dct_w:\n            x_pooled = torch.nn.functional.adaptive_avg_pool2d(x, (self.dct_h, self.dct_w))\n            # If you have concerns about one-line-change, don't worry.   :)\n            # In the ImageNet models, this line will never be triggered. \n            # This is for compatibility in instance segmentation and object detection.\n        y = self.dct_layer(x_pooled)\n\n        y = self.fc(y).view(n, c, 1, 1)\n        return x * y.expand_as(x)\n\n\nclass MultiSpectralDCTLayer(nn.Module):\n    \"\"\"\n    Generate dct filters\n    \"\"\"\n    def __init__(self, height, width, mapper_x, mapper_y, channel):\n        super(MultiSpectralDCTLayer, self).__init__()\n        \n        assert len(mapper_x) == len(mapper_y)\n        assert channel % len(mapper_x) == 0\n\n        self.num_freq = len(mapper_x)\n\n        # fixed DCT init\n        self.register_buffer('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))\n        \n        # fixed random init\n        # self.register_buffer('weight', torch.rand(channel, height, width))\n\n        # learnable DCT init\n        # self.register_parameter('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))\n        \n        # learnable random init\n        # self.register_parameter('weight', torch.rand(channel, height, width))\n\n        # num_freq, h, w\n\n    def forward(self, x):\n        assert len(x.shape) == 4, 'x must been 4 dimensions, but got ' + str(len(x.shape))\n        # n, c, h, w = x.shape\n\n        x = x * self.weight\n\n        result = torch.sum(x, dim=[2,3])\n        return result\n\n    def build_filter(self, pos, freq, POS): #即基础函数\n        result = math.cos(math.pi * freq * (pos + 0.5) / POS) / math.sqrt(POS) \n        if freq == 0:\n            return result\n        else:\n            return result * math.sqrt(2)\n    \n    def get_dct_filter(self, tile_size_x, tile_size_y, mapper_x, mapper_y, channel):\n        dct_filter = torch.zeros(channel, tile_size_x, tile_size_y)\n\n        c_part = channel // len(mapper_x)\n\n        for i, (u_x, v_y) in enumerate(zip(mapper_x, mapper_y)):\n            for t_x in range(tile_size_x):\n                for t_y in range(tile_size_y):\n                    dct_filter[i * c_part: (i+1)*c_part, t_x, t_y] = self.build_filter(t_x, u_x, tile_size_x) * self.build_filter(t_y, v_y, tile_size_y)\n                    #dct_filter即得到每个channel对应的基础函数\n                        \n        return dct_filter\n    \n'''\n    FCAM\n'''\nclass DFCAM(nn.Module):\n    def __init__(self, in_planes, dct_h, dct_w, reduction = 16,freq_size=8, freq_sel_method = 'top16', fc='conv', transform=None):\n        super(DFCAM, self).__init__()\n        \n        self.transform=transform\n        self.reduction = reduction\n        self.dct_h = dct_h\n        self.dct_w = dct_w\n\n        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n        self.num_split = len(mapper_x)\n        mapper_x = [temp_x * (dct_h // freq_size) for temp_x in mapper_x]\n        mapper_y = [temp_y * (dct_w // freq_size) for temp_y in mapper_y]\n\n        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, in_planes)\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc_method = fc\n        \n        if fc == 'linear':\n            self.fc = nn.Sequential(\n                nn.Linear(in_planes, in_planes // reduction, bias=False),#这里用的是Linear，所以输入形状必须是【bs,channels】\n                nn.ReLU(inplace=True),\n                nn.Linear(in_planes // reduction, in_planes, bias=False),\n            )\n        elif fc == 'conv':\n            self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // reduction, 1, bias=False),\n                               nn.ReLU(inplace=True),\n                               nn.Conv2d(in_planes // reduction, in_planes, 1, bias=False))\n        \n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        n,c,h,w = x.shape\n        x_pooled = x\n        if h != self.dct_h or w != self.dct_w:\n            x_pooled = torch.nn.functional.adaptive_avg_pool2d(x, (self.dct_h, self.dct_w))\n            # If you have concerns about one-line-change, don't worry.   :)\n            # In the ImageNet models, this line will never be triggered. \n            # This is for compatibility in instance segmentation and object detection.\n        y = self.dct_layer(x_pooled)\n\n        if self.fc_method == 'conv':\n            y=y.view(n,c,1,1)\n        y = self.sigmoid(self.fc(y)).view(n, c, 1, 1)\n        dct_att = x * y.expand_as(x)\n        \n        \n        if self.transform:\n            f_x=transformTo(x, self.transform)\n        else:\n            f_x=x\n        \n        if self.fc_method == 'linear':\n            f_avg_out = self.fc(self.avg_pool(f_x).squeeze())\n            f_max_out = self.fc(self.max_pool(f_x).squeeze())\n        else:\n            f_avg_out = self.fc(self.avg_pool(f_x))\n            f_max_out = self.fc(self.max_pool(f_x))\n        f_out=f_avg_out + f_max_out\n        f_out=f_out.view(n, c, 1, 1)\n        f_out=self.sigmoid(f_out)*f_x\n        \n        return dct_att + f_out\n    \n    \n'''\nGlobal Transformer block\n'''\nclass ChannelSelfAttention(nn.Module):\n    def __init__(self, in_channels):\n        super(ChannelSelfAttention, self).__init__()\n        \n        self.inter_channels=in_channels//8\n        self.conv1 = nn.Conv2d(in_channels, self.inter_channels, kernel_size=3, padding=1, bias=False)\n        self.conv2 = nn.Conv2d(in_channels, self.inter_channels, kernel_size=3, padding=1, bias=False)\n        self.conv3 = nn.Conv2d(in_channels, self.inter_channels, kernel_size=3, padding=1, bias=False)\n        self.lastconv = nn.Conv2d(self.inter_channels, in_channels, 1, bias=False)\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, x):\n        q=self.gap(self.conv1(x)).flatten(2) # [b,c,1]\n        \n        k_T=self.conv2(x).flatten(2).permute(0,2,1) # [b,h*w,c]\n        \n        v=self.conv3(x).flatten(2) # [b,c,h*w]\n        \n        att = torch.matmul(k_T, q) # [b,h*w,1]\n        att = F.softmax(att, dim=1)\n        out = torch.matmul(v, att)\n        out = out.view(out.shape[0],out.shape[1],1,1)\n        out = self.lastconv(out)\n        out = out + x\n        \n        return out\n    \nclass SpatialSelfAttention(nn.Module):\n    def __init__(self, in_channels, extra_channels=None):\n        super(SpatialSelfAttention, self).__init__()\n        \n        self.inter_channels=in_channels//2\n        self.conv1 = nn.Conv2d(in_channels, self.inter_channels, kernel_size=3, padding=1, bias=False)\n        self.conv2 = nn.Conv2d(in_channels, self.inter_channels, kernel_size=3, padding=1, bias=False)\n        self.conv3 = nn.Conv2d(in_channels, self.inter_channels, kernel_size=3, padding=1, bias=False)\n        self.lastconv = nn.Conv2d(self.inter_channels, in_channels, 1, bias=False)\n        \n        if extra_channels is not None:\n            self.extra_project=nn.Conv2d(extra_channels, in_channels, 1, bias=False)\n            \n\n    def forward(self, x, extra=None):\n        b, c, h, w = x.shape\n        \n        q=self.conv1(x).view(b, self.inter_channels, -1)\n        \n        if extra is not None:\n            extra=self.extra_project(extra)\n            feature=extra\n        else:\n            feature=x\n            \n        k_T=self.conv2(feature).view(b, self.inter_channels, -1).permute(0,2,1)\n        v=self.conv3(feature).view(b, self.inter_channels, -1)\n        att = torch.matmul(k_T, q)\n        att = F.softmax(att, dim=-1)\n        \n        out = torch.matmul(v, att)\n        out = out.view(b, self.inter_channels, h, w)\n        \n        out = self.lastconv(out)\n        out = out + x\n        \n        return out\n    \nclass RTBlock(nn.Module):\n    def __init__(self, in_channels, patch=None):\n        super().__init__()\n        self.patch=patch\n        if self.patch:\n            self.patch_avg = nn.ModuleList([nn.AdaptiveAvgPool2d(output_size=(patch, patch)) for _ in range(2)]) #b h w c->b p*p c\n        \n            self.avg_map = nn.ModuleList([\n                    nn.Sequential(nn.Conv2d(in_channels, in_channels, 1),\n                                  nn.BatchNorm2d(in_channels), \n                                  nn.ReLU(inplace=True))\n                    for _ in range(2)\n                ])\n\n            # self.patch_avg = nn.ModuleList([nn.Conv2d(in_channels,in_channels,kernel_size=patch,stride=patch) for _ in range(2)])\n            # self.avg_map = nn.ModuleList([nn.Identity() for _ in range(2)])\n            \n            self.upconv = nn.ModuleList([\n                    nn.Conv2d(in_channels, in_channels, 1) for _ in range(2)\n                ])\n            \n        # self.csa=ChannelSelfAttention(in_channels)\n        # self.csa_extra=ChannelSelfAttention(in_channels)\n        self.ssa=SpatialSelfAttention(in_channels)\n        self.ssa_cross=SpatialSelfAttention(in_channels, in_channels)\n        \n        self.conv = nn.Conv2d(in_channels*2, in_channels, kernel_size=1, bias=False)\n        \n    def forward(self, x, extra=None):\n        raw_size = x.shape[2]\n        if self.patch:\n            x = self.patch_avg[0](x)\n            x = self.avg_map[0](x)\n            if extra != None:\n                extra = self.patch_avg[1](extra)\n                extra = self.avg_map[1](extra)\n        \n        # if extra != None:\n        #     x = self.csa(x)\n        #     extra = self.csa_extra(extra)\n        # else:\n        #     raw_featrues=x\n        #     x = self.csa(x)\n        #     extra = self.csa_extra(raw_featrues)\n        \n        self_att = self.ssa(x)\n        cross_att = self.ssa_cross(x, extra)\n        \n        x = self.conv(torch.cat((self_att,cross_att), dim=1))\n        \n        if self.patch:\n            x = self.upsample(x, raw_size)\n            extra = self.upsample(extra, raw_size)\n            x = self.upconv[0](x)\n            extra = self.upconv[1](extra)\n            \n        return x, extra\n\n    def upsample(self, x, raw_size):\n        if raw_size == x.shape[2]:\n            return x\n        x = F.interpolate(x, scale_factor=raw_size//x.shape[2], mode=\"nearest\")\n        return x\n    \nclass DualCrossAttentionBlock(nn.Module):\n    def __init__(self, hidden_channels, patch=None, net_type='linear'):\n        super().__init__()\n        self.hidden_size = hidden_channels\n        self.attention_norm = nn.ModuleList([LayerNorm(hidden_channels, eps=1e-6) for _ in range(2)])\n        self.ffn_norm = LayerNorm(hidden_channels, eps=1e-6)\n        self.ffn = TranUnetMlp(hidden_channels, net_type=net_type)\n        self.attn = RTBlock(hidden_channels, patch=patch)\n\n    def forward(self, x, x_dual):\n        raw = x\n        b, p2, c= x.shape\n        p = int(math.sqrt(p2))\n        x = self.attention_norm[0](x)\n        x_dual = self.attention_norm[1](x_dual)\n        \n        x = einops.rearrange(x, 'B (H W) C-> B C H W', H=p) \n        x_dual = einops.rearrange(x_dual, 'B (H W) C-> B C H W', H=p) \n        x, x_dual = self.attn(x, x_dual)\n        x = einops.rearrange(x, 'B C H W-> B (H W) C') \n        x_dual = einops.rearrange(x_dual, 'B C H W-> B (H W) C')\n        x = x + raw\n\n        raw = x\n        x = self.ffn_norm(x)\n        x = self.ffn(x)\n        x = x + raw\n        return x, x_dual\n    \nclass DualCrossTransformer(nn.Module):\n    def __init__(self, in_channels, num_layer=1, patch=None, net_type='linear'):\n        super().__init__()\n        self.blocks=nn.ModuleList([\n                DualCrossAttentionBlock(in_channels, patch=patch) for _ in range(num_layer)\n            ])\n        \n    def forward(self, x, x_dual):\n        b, c, h, w= x.shape\n        x = einops.rearrange(x, 'B C H W-> B (H W) C') \n        x_dual = einops.rearrange(x_dual, 'B C H W-> B (H W) C')\n        \n        for i,b in enumerate(self.blocks):\n            x, x_dual = b(x, x_dual)\n            \n        x = einops.rearrange(x, 'B (H W) C-> B C H W', H=h) \n        x_dual = einops.rearrange(x_dual, 'B (H W) C-> B C H W', H=h)\n\n        return x, x_dual\n        \n\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(MultiScaleFusion, self).__init__()\n\n        self.myffs = nn.ModuleList([nn.Sequential(\n                        DFCAM(inc, channel2wh(out_channels), channel2wh(out_channels), freq_sel_method = freq_method),\n                        FSpatialAttention(transform='dct')\n                      ) for inc in in_channels])\n\n        self.upconv = nn.ModuleList([nn.Conv2d(inc, out_channels, 1) for inc in in_channels])\n        \n        self.conv = nn.Sequential(nn.Conv2d(len(in_channels)*out_channels, out_channels, kernel_size=1, bias=False),\n                                  nn.BatchNorm2d(out_channels),\n                                  nn.ReLU(inplace = True))\n\n\n    def forward(self, x):\n        # x = self.m_apply(raw, self.myffs)\n        max_size = x[-1].shape[2]\n\n        for i,_x in enumerate(x):\n            x[i] = self.upsample(_x, max_size)\n        x=self.m_apply(x, self.upconv)\n        x = self.m_apply(x, self.myffs)\n        x = self.conv(torch.cat((x), dim=1))\n        \n        return x\n    \n    def m_apply(self, x, module):\n        return [module[i](j) for i, j in enumerate(x)]\n\n    def upsample(self, x, size):\n        x = F.interpolate(x, scale_factor=size//x.shape[2], mode=\"nearest\")\n        return x\n\nclass MultiScaleFreqAttention(nn.Module):\n    def __init__(self, in_channels, out_channels, freq_method='dct8'):\n        super(MultiScaleFreqAttention, self).__init__()\n\n        self.myffs = nn.ModuleList([nn.Sequential(\n                        DFCAM(out_channels, channel2wh(out_channels), channel2wh(out_channels), freq_sel_method = freq_method),\n                        FSpatialAttention(transform='dct')\n                      ) for inc in in_channels])\n        \n        self.upconv = nn.ModuleList([nn.Conv2d(inc, out_channels, 1) for inc in in_channels])\n        \n        self.conv = nn.Conv2d(len(in_channels)*out_channels, out_channels, kernel_size=1, bias=False)\n\n    def forward(self, features):\n        max_size = features[-1].shape[2]\n        \n        for i,f in enumerate(features):\n            features[i] = self.upsample(f, max_size)\n        \n        features=self.m_apply(features, self.upconv)\n        features=self.m_apply(features, self.myffs)\n        \n        out = self.conv(torch.cat((features), dim=1))\n        \n        return out\n    \n    def m_apply(self, x, module):\n        return [module[i](j) for i, j in enumerate(x)]\n\n    def upsample(self, x, size):\n        x = F.interpolate(x, scale_factor=size//x.shape[2], mode=\"nearest\")\n        return x\n\n# class MultiScaleRelationFusion(nn.Module):\n#     def __init__(self, in_channels, out_channels):\n#         super(MultiScaleFusion, self).__init__()\n#\n#         self.dct = nn.ModuleList([DualCrossTransformer(inc) for inc in in_channels])\n#\n#         self.upconv = nn.ModuleList([nn.Conv2d(inc, out_channels, 1) for inc in in_channels])\n#\n#         self.conv = nn.Sequential(nn.Conv2d(len(in_channels)*out_channels, out_channels, kernel_size=1, bias=False),\n#                                   nn.BatchNorm2d(out_channels),\n#                                   nn.ReLU(inplace = True))\n#\n#\n#     def forward(self, x):\n#         # x = self.m_apply(raw, self.myffs)\n#         max_size = x[-1].shape[2]\n#\n#         for i,_x in enumerate(x):\n#             x[i] = self.upsample(_x, max_size)\n#         x=self.m_apply(x, self.upconv)\n#         x = self.m_apply(x, self.myffs)\n#         x = self.conv(torch.cat((x), dim=1))\n#\n#         return x\n#\n#     def m_apply(self, x, module):\n#         return [module[i](j) for i, j in enumerate(x)]\n#\n#     def upsample(self, x, size):\n#         x = F.interpolate(x, scale_factor=size//x.shape[2], mode=\"nearest\")\n#         return x\n\n\nprint('modules')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:45.955186Z","iopub.execute_input":"2024-12-23T02:53:45.955529Z","iopub.status.idle":"2024-12-23T02:53:46.221201Z","shell.execute_reply.started":"2024-12-23T02:53:45.955493Z","shell.execute_reply":"2024-12-23T02:53:46.220310Z"}},"outputs":[{"name":"stdout","text":"modules\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport math\nimport torch.nn.functional as F\n\n\nclass KANLinear(torch.nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        enable_standalone_scale_spline=True,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        h = (grid_range[1] - grid_range[0]) / grid_size\n        grid = (\n            (\n                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n                + grid_range[0]\n            )\n            .expand(in_features, -1)\n            .contiguous()\n        )\n        self.register_buffer(\"grid\", grid)\n\n        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.spline_weight = torch.nn.Parameter(\n            torch.Tensor(out_features, in_features, grid_size + spline_order)\n        )\n        if enable_standalone_scale_spline:\n            self.spline_scaler = torch.nn.Parameter(\n                torch.Tensor(out_features, in_features)\n            )\n\n        self.scale_noise = scale_noise\n        self.scale_base = scale_base\n        self.scale_spline = scale_spline\n        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n        self.base_activation = base_activation()\n        self.grid_eps = grid_eps\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n        with torch.no_grad():\n            noise = (\n                (\n                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n                    - 1 / 2\n                )\n                * self.scale_noise\n                / self.grid_size\n            )\n            self.spline_weight.data.copy_(\n                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n                * self.curve2coeff(\n                    self.grid.T[self.spline_order : -self.spline_order],\n                    noise,\n                )\n            )\n            if self.enable_standalone_scale_spline:\n                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n\n    def b_splines(self, x: torch.Tensor):\n        \"\"\"\n        Compute the B-spline bases for the given input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        grid: torch.Tensor = (\n            self.grid\n        )  # (in_features, grid_size + 2 * spline_order + 1)\n        x = x.unsqueeze(-1)\n        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n        for k in range(1, self.spline_order + 1):\n            bases = (\n                (x - grid[:, : -(k + 1)])\n                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n                * bases[:, :, :-1]\n            ) + (\n                (grid[:, k + 1 :] - x)\n                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n                * bases[:, :, 1:]\n            )\n\n        assert bases.size() == (\n            x.size(0),\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return bases.contiguous()\n\n    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        Compute the coefficients of the curve that interpolates the given points.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n\n        Returns:\n            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        assert y.size() == (x.size(0), self.in_features, self.out_features)\n\n        A = self.b_splines(x).transpose(\n            0, 1\n        )  # (in_features, batch_size, grid_size + spline_order)\n        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n        solution = torch.linalg.lstsq(\n            A, B\n        ).solution  # (in_features, grid_size + spline_order, out_features)\n        result = solution.permute(\n            2, 0, 1\n        )  # (out_features, in_features, grid_size + spline_order)\n\n        assert result.size() == (\n            self.out_features,\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return result.contiguous()\n\n    @property\n    def scaled_spline_weight(self):\n        return self.spline_weight * (\n            self.spline_scaler.unsqueeze(-1)\n            if self.enable_standalone_scale_spline\n            else 1.0\n        )\n\n    def forward(self, x: torch.Tensor):\n        assert x.size(-1) == self.in_features\n        original_shape = x.shape\n        x = x.reshape(-1, self.in_features)\n\n        base_output = F.linear(self.base_activation(x), self.base_weight)\n        spline_output = F.linear(\n            self.b_splines(x).view(x.size(0), -1),\n            self.scaled_spline_weight.view(self.out_features, -1),\n        )\n        output = base_output + spline_output\n        \n        output = output.reshape(*original_shape[:-1], self.out_features)\n        return output\n\n    @torch.no_grad()\n    def update_grid(self, x: torch.Tensor, margin=0.01):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        batch = x.size(0)\n\n        splines = self.b_splines(x)  # (batch, in, coeff)\n        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n        unreduced_spline_output = unreduced_spline_output.permute(\n            1, 0, 2\n        )  # (batch, in, out)\n\n        # sort each channel individually to collect data distribution\n        x_sorted = torch.sort(x, dim=0)[0]\n        grid_adaptive = x_sorted[\n            torch.linspace(\n                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n            )\n        ]\n\n        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n        grid_uniform = (\n            torch.arange(\n                self.grid_size + 1, dtype=torch.float32, device=x.device\n            ).unsqueeze(1)\n            * uniform_step\n            + x_sorted[0]\n            - margin\n        )\n\n        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n        grid = torch.concatenate(\n            [\n                grid[:1]\n                - uniform_step\n                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n                grid,\n                grid[-1:]\n                + uniform_step\n                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n            ],\n            dim=0,\n        )\n\n        self.grid.copy_(grid.T)\n        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        \"\"\"\n        Compute the regularization loss.\n\n        This is a dumb simulation of the original L1 regularization as stated in the\n        paper, since the original one requires computing absolutes and entropy from the\n        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n        behind the F.linear function if we want an memory efficient implementation.\n\n        The L1 regularization is now computed as mean absolute value of the spline\n        weights. The authors implementation also includes this term in addition to the\n        sample-based regularization.\n        \"\"\"\n        l1_fake = self.spline_weight.abs().mean(-1)\n        regularization_loss_activation = l1_fake.sum()\n        p = l1_fake / regularization_loss_activation\n        regularization_loss_entropy = -torch.sum(p * p.log())\n        return (\n            regularize_activation * regularization_loss_activation\n            + regularize_entropy * regularization_loss_entropy\n        )\n        \nprint('kan.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:46.222605Z","iopub.execute_input":"2024-12-23T02:53:46.222981Z","iopub.status.idle":"2024-12-23T02:53:46.245924Z","shell.execute_reply.started":"2024-12-23T02:53:46.222924Z","shell.execute_reply":"2024-12-23T02:53:46.245048Z"}},"outputs":[{"name":"stdout","text":"kan.py\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport math\n\nfrom os.path import join as pjoin\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\nfrom torch.nn.modules.utils import _pair\nfrom scipy import ndimage\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\nACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu}\n\n\nclass TranUnetAttention(nn.Module):\n    def __init__(self, hidden_channels, num_heads=8, net_type='linear'):\n        super(TranUnetAttention, self).__init__()\n        self.num_attention_heads = num_heads\n        self.attention_head_size = int(hidden_channels / self.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        if net_type == 'linear':\n            net=Linear\n        elif net_type == 'kan':\n            net=KANLinear\n            \n        self.query = net(hidden_channels, self.all_head_size)\n        self.key = net(hidden_channels, self.all_head_size)\n        self.value = net(hidden_channels, self.all_head_size)\n\n        self.out = net(hidden_channels, hidden_channels)\n\n        self.softmax = Softmax(dim=-1)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_probs = self.softmax(attention_scores)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        return attention_output\n\n\nclass TranUnetMlp(nn.Module):\n    def __init__(self, hidden_channels, expand_ratio=4, net_type='linear'):\n        super(TranUnetMlp, self).__init__()\n        self.net_type=net_type\n        if net_type == 'linear':\n            net=Linear\n        elif net_type == 'kan':\n            net=KANLinear\n            \n        self.fc1 = net(hidden_channels, hidden_channels*expand_ratio)\n        self.fc2 = net(hidden_channels*expand_ratio, hidden_channels)\n        self.act_fn = ACT2FN[\"gelu\"]\n        self.dropout = Dropout(0.1)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        if self.net_type == 'linear':\n            nn.init.xavier_uniform_(self.fc1.weight)\n            nn.init.xavier_uniform_(self.fc2.weight)\n            nn.init.normal_(self.fc1.bias, std=1e-6)\n            nn.init.normal_(self.fc2.bias, std=1e-6)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Embeddings(nn.Module):\n    \"\"\"Construct the embeddings from patch, position embeddings.\n    \"\"\"\n    def __init__(self, patch_size, img_size, downsample_factor, pretrained=True):\n        super(Embeddings, self).__init__()\n        self.downsample_factor = downsample_factor\n        img_size = img_size//downsample_factor\n        patch_size = patch_size\n        patch = img_size // patch_size       \n        n_patches = patch**2\n        # print(f'img_size:{img_size}')\n        self.img_size=img_size\n        self.patch=patch\n        self.hidden_channels=patch_size**2 * 3\n        backbone='rs50'\n        rsd=[downsample_factor<(2**i) for i in range(3,6)]\n        if backbone == 'rs50':\n            encoder = resnet50(pretrained=pretrained, replace_stride_with_dilation=rsd)\n        elif backbone == 'rs101':\n            encoder = resnet101(pretrained=pretrained, replace_stride_with_dilation=rsd)\n        elif backbone == 'rs152':\n            encoder = resnet152(pretrained=pretrained, replace_stride_with_dilation=rsd)\n        else:\n            assert f'{backbone} does not exists!'\n        \n        conv1 = nn.Sequential(encoder.conv1,\n                                   encoder.bn1,\n                                   encoder.relu)#x/2\n        \n        conv2=nn.Sequential(encoder.maxpool,\n                                 encoder.layer1)#x/4\n        \n        conv3=nn.Sequential(encoder.layer2)#x/8\n        \n        conv4=nn.Sequential(encoder.layer3)#x/16\n        \n        conv5=nn.Sequential(encoder.layer4)#x/32\n    \n        stages = [conv1, conv2, conv3, conv4, conv5]\n        self.stages = nn.ModuleList(stages)\n        self.in_channels = [0, 64, 256, 512, 1024]\n        self.skip_channels = self.in_channels[::-1]\n        \n        \n        self.patch_embeddings = Conv2d(in_channels=2048,\n                                       out_channels=self.hidden_channels,\n                                       kernel_size=patch_size,\n                                       stride=patch_size)\n        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, self.hidden_channels))\n\n        self.dropout = Dropout(0.1)\n\n\n    def forward(self, x):\n        \n        features=[]\n        for m in self.stages:\n            x=m(x)\n            features.append(x)\n        #feature不需要conv5，conv5不属于skip\n        last_conv=features[-1]\n        features=features[:-1]\n        features=features[::-1]\n        x = self.patch_embeddings(last_conv)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n        # print(x.shape)\n        x = x.flatten(2)\n        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n        # print(x.shape)\n\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        return embeddings, features\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module): # W-MSA in the paper\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, net_type='linear', qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n        self.net_type=net_type\n        if net_type == 'linear':\n            net=Linear\n        elif net_type == 'kan':\n            net=KANLinear\n        \n        self.qkv = net(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = net(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C) >>> (B * 32*32, 4*4, 192)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  #AMBIGUOUS X)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads=12, window_size=7, shift_size=3,\n                 qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm, net_type='linear'):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = (input_resolution,input_resolution)\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, net_type=net_type)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.mlp = TranUnetMlp(dim, net_type=net_type)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass TranUnetBlock(nn.Module):\n    def __init__(self, hidden_channels, net_type='linear'):\n        super(TranUnetBlock, self).__init__()\n        self.hidden_size = hidden_channels\n        self.attention_norm = LayerNorm(hidden_channels, eps=1e-6)\n        self.ffn_norm = LayerNorm(hidden_channels, eps=1e-6)\n        self.ffn = TranUnetMlp(hidden_channels, net_type=net_type)\n        self.attn = TranUnetAttention(hidden_channels, net_type=net_type)\n\n    def forward(self, x):\n        h = x\n        x = self.attention_norm(x)\n        x = self.attn(x)\n        x = x + h\n\n        h = x\n        x = self.ffn_norm(x)\n        x = self.ffn(x)\n        x = x + h\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, hidden_channels, num_layers=12, net_type='linear', transformer_block='vit', **kwargs):\n        super(Encoder, self).__init__()\n        self.layer = nn.ModuleList()\n        self.encoder_norm = LayerNorm(hidden_channels, eps=1e-6)\n        \n        if transformer_block == 'vit':\n            for _ in range(num_layers):\n                layer = TranUnetBlock(hidden_channels, net_type=net_type)\n                self.layer.append(copy.deepcopy(layer))\n        elif transformer_block == 'swin':\n            for i in range(num_layers):\n                layer = SwinTransformerBlock(dim=hidden_channels, input_resolution=kwargs['input_resolution'],\n                                     shift_size=0 if (i % 2 == 0) else 3)\n                self.layer.append(copy.deepcopy(layer))\n\n    def forward(self, hidden_states):\n        for layer_block in self.layer:\n            # print(hidden_states.shape)\n            hidden_states = layer_block(hidden_states)\n        encoded = self.encoder_norm(hidden_states)\n        return encoded\n\nclass Transformer(nn.Module):\n    def __init__(self, patch_size, img_size, downsample_factor, num_layers, transformer_block='vit', net_type='linear'):\n        super(Transformer, self).__init__()\n        self.embeddings = Embeddings(patch_size=patch_size, img_size=img_size, downsample_factor=downsample_factor)\n        self.encoder = Encoder(self.embeddings.hidden_channels, num_layers, net_type=net_type, transformer_block=transformer_block, input_resolution=self.embeddings.patch)\n        self.skip_channels=self.embeddings.skip_channels\n        self.hidden_channels=self.embeddings.hidden_channels\n        self.patch=self.embeddings.patch\n\n    def forward(self, input_ids):\n        embedding_output, features = self.embeddings(input_ids)\n        encoded = self.encoder(embedding_output)  # (B, n_patch, hidden)\n        return encoded, features\n\n\nclass Conv2dReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=0,\n            stride=1,\n            use_batchnorm=True,\n    ):\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        bn = nn.BatchNorm2d(out_channels)\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\n\nclass TranUnetDecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            skip_channels=0,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n        self.conv1 = Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.conv2 = Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, x, skip=None):\n        x = self.up(x)\n        # print(f'x:{x.shape}')\n        if skip is not None:\n            # print(f'skip:{skip.shape}')\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass SegmentationHead(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n        super().__init__(conv2d, upsampling)\n\n\nclass DecoderCup(nn.Module):\n    def __init__(self, hidden_channels, skip_channels, upsample_mode):\n        super().__init__()\n        head_channels = skip_channels[0]\n        self.conv_more = Conv2dReLU(\n            hidden_channels,\n            head_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=True,\n        )\n        self.decoder_channels = [1024, 512, 256, 128, 64, 16]\n\n        self.decoder_channels = self.decoder_channels[6-len(skip_channels):]\n        in_channels = [head_channels] + list(self.decoder_channels[:-1]) #[1024, 512,256,128,64]\n        out_channels = self.decoder_channels\n        self.skip_channels =skip_channels #[1024,512,256,64,0]\n\n        blocks = [\n            DecoderBlock(in_ch, out_ch, sk_ch, mode=upsample_mode) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n        ]\n\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, hidden_states, features=None):\n        x = self.conv_more(hidden_states)\n        for i, decoder_block in enumerate(self.blocks):\n            if self.skip_channels[i] != 0:\n                x = decoder_block(x, skip=features[i])\n            else:\n                x = decoder_block(x)\n        return x\n    \nclass UnetPPDecoderCup(nn.Module):\n    def __init__(\n        self, hidden_channels, skip_channels, upsample_mode\n    ):\n        super().__init__()\n        head_channels = skip_channels[0]\n        self.conv_more = Conv2dReLU(\n            hidden_channels,\n            head_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=True,\n        )\n        self.decoder_channels = [1024, 512, 256, 128, 64, 16]\n\n        self.decoder_channels = self.decoder_channels[6-len(skip_channels):]\n        self.in_channels = [head_channels] + list(self.decoder_channels[:-1])#[512,256,128,64]\n        self.out_channels = self.decoder_channels\n        self.skip_channels = skip_channels#[512,256,64,0]\n        self.depth = len(self.in_channels) - 1\n\n        blocks = {}\n        for layer_idx in range(len(self.in_channels) - 1):\n            for depth_idx in range(layer_idx + 1):\n                if depth_idx == 0:\n                    in_ch = self.in_channels[layer_idx]\n                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1)\n                    out_ch = self.out_channels[layer_idx]\n                    # print(f'x_{depth_idx}_{layer_idx}--in_ch:{in_ch}')\n                    # print(f'x_{depth_idx}_{layer_idx}--skip_ch:{skip_ch}')\n                    # print(f'x_{depth_idx}_{layer_idx}--out_ch:{out_ch}')\n                else:\n                    out_ch = self.skip_channels[layer_idx]\n                    skip_ch = self.skip_channels[layer_idx] * (\n                        layer_idx + 1 - depth_idx\n                    )\n                    in_ch = self.skip_channels[layer_idx - 1]\n                blocks[f\"x_{depth_idx}_{layer_idx}\"] = DecoderBlock(\n                    in_ch, out_ch, skip_ch, mode=upsample_mode\n                )\n        blocks[f\"x_{0}_{len(self.in_channels)-1}\"] = DecoderBlock(\n            self.in_channels[-1], self.out_channels[-1], 0, mode='interp'\n        )\n        self.blocks = nn.ModuleDict(blocks)\n        # print(blocks)\n\n    def forward(self, hidden_states, features=None):\n        x = self.conv_more(hidden_states)\n        features = [x] + features\n        # for i,f in enumerate(features):\n        #     print(f'f[i]:{f.shape}')\n        # start building dense connections\n        dense_x = {}\n        for layer_idx in range(len(self.in_channels) - 1):\n            for depth_idx in range(self.depth - layer_idx):\n                if layer_idx == 0:\n                    # print(f'features[{depth_idx}].shape:{features[depth_idx].shape}')\n                    # print(f'features[{depth_idx+1}].shape:{features[depth_idx+1].shape}')\n                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](\n                        features[depth_idx], features[depth_idx + 1]\n                    )\n                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n                else:\n                    dense_l_i = depth_idx + layer_idx\n                    cat_features = [\n                        dense_x[f\"x_{idx}_{dense_l_i}\"]\n                        for idx in range(depth_idx + 1, dense_l_i + 1)\n                    ]\n                    cat_features = torch.cat(\n                        cat_features + [features[dense_l_i + 1]], dim=1\n                    )\n                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[\n                        f\"x_{depth_idx}_{dense_l_i}\"\n                    ](dense_x[f\"x_{depth_idx}_{dense_l_i-1}\"], cat_features)\n        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](\n            dense_x[f\"x_{0}_{self.depth-1}\"]\n        )\n\n        last_out = dense_x[f\"x_{0}_{self.depth}\"]\n        return last_out\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size=224, num_classes=4, downsample_factor=8, upsample_mode='interp', transformer_block='vit', net_type='linear', decoder='unet'):\n        super(VisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.patch_size= 8 if img_size<=512 else 16\n        self.transformer = Transformer(patch_size=self.patch_size, img_size=img_size, downsample_factor=downsample_factor, num_layers=12, transformer_block=transformer_block, net_type=net_type)\n        self.downsample_factor=downsample_factor\n        if decoder == 'unet':\n            self.decoder = DecoderCup(self.transformer.hidden_channels, self.transformer.skip_channels, upsample_mode)\n        elif decoder == 'unetpp':\n            self.decoder = UnetPPDecoderCup(self.transformer.hidden_channels, self.transformer.skip_channels, upsample_mode)\n        self.segmentation_head = SegmentationHead(\n            in_channels=self.decoder.decoder_channels[-1],\n            out_channels=num_classes,\n            kernel_size=3,\n        )\n\n    def forward(self, x):\n        x, features = self.transformer(x)  # (B, n_patch, hidden)\n        # print(f'transformer x:{x.shape}')\n        B, n_patch, hidden = x.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n        x = x.permute(0, 2, 1)\n        x = x.contiguous().view(B, hidden, h, w)\n        # print(f'before up:{x.shape}')\n        x = F.interpolate(x, scale_factor=self.patch_size, mode=\"nearest\")\n        x = self.decoder(x, features)\n        logits = self.segmentation_head(x)\n        return logits\n    \nprint('transunet.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:46.247484Z","iopub.execute_input":"2024-12-23T02:53:46.247860Z","iopub.status.idle":"2024-12-23T02:53:47.479763Z","shell.execute_reply.started":"2024-12-23T02:53:46.247823Z","shell.execute_reply":"2024-12-23T02:53:47.478893Z"}},"outputs":[{"name":"stdout","text":"transunet.py\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\nimport torch\nimport einops\nimport math\n\n\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_c)\n\n        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_c)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, skip_channels=0, is_up=True, scale_factor=2, mode='conv', fusion='cat'):\n        super(DecoderBlock, self).__init__()\n        self.in_channels = in_channels\n        self.is_up=is_up\n        self.mode = mode\n        self.fusion = fusion\n\n        if mode == 'conv':\n            self.up = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2*scale_factor, stride=scale_factor, padding=scale_factor//2)\n        elif mode == 'interp':\n            self.up = nn.Upsample(scale_factor=scale_factor)\n        \n        self.conv = ConvRelu(in_channels + skip_channels, out_channels)\n        \n    def forward(self, x, skip=None):\n        if skip is not None:\n            if skip.shape[2] == x.shape[2]:\n                x = x\n            else:\n                x = self.up(x)\n        else:\n            x = self.up(x)\n            \n            \n        if skip != None:\n            # print(f'x:{x.shape}')\n            # print(f'skip:{skip.shape}')\n\n            x = torch.cat([x, skip], axis=1)\n            # print(f'cat_x:{x.shape}')\n            \n        x = self.conv(x)\n            \n        return x\n    \nclass UNetResNet(nn.Module):\n    def __init__(self, num_classes=1, pretrained=False, module_list = None, downsample_factor=8, upsample_mode='conv', dual=False, **kwargs):\n        super(UNetResNet, self).__init__()\n\n        self.args = kwargs['args']\n        self.downsample_factor=downsample_factor\n        self.module_list = module_list\n        self.backbone=kwargs['backbone']\n        self.resolution=kwargs['resolution']\n        self.num_classes=num_classes\n        self.dual=dual\n        if self.backbone[:2] == 'rs':\n            self.stages, encoder_channels=get_resnet_stages(self.downsample_factor, self.backbone, self.module_list, self.args)\n        elif 'wtconvnext' in self.backbone:\n            self.stages, encoder_channels=get_wtconvnext_stages(self.downsample_factor, self.backbone, self.args)\n        self.stages=nn.ModuleList(self.stages)\n        \n        decoder_channels = [1024,512,256,128,64,32]\n        decoder_channels = decoder_channels[6-len(encoder_channels):]\n        \n        encoder_channels = encoder_channels[::-1]\n        \n        head_channels = encoder_channels[0]\n        self.in_channels = [head_channels] + list(decoder_channels[:-1]) # 2048, 256, 128, 64, 32, 16\n        self.skip_channels = list(encoder_channels[1:]) + [0] # 1024,512,128,64,0\n        self.out_channels = decoder_channels \n        \n        blocks = [\n            DecoderBlock(in_ch, out_ch, sk_ch, mode=upsample_mode) for in_ch, out_ch, sk_ch in zip(self.in_channels, self.out_channels, self.skip_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n        if self.dual:\n            if self.args.seg_vessel:\n                self.blocks_dual = nn.ModuleList([\n                    DecoderBlock(in_ch, out_ch, sk_ch, mode=upsample_mode) for in_ch, out_ch, sk_ch in zip(self.in_channels, self.out_channels, self.skip_channels)\n                ])\n                self.final_dual = nn.Conv2d(decoder_channels[-1], 1, 1)                            \n            else:\n                self.stages_dual, _=get_resnet_stages(self.downsample_factor, self.backbone, self.module_list, self.args)\n                self.stages_dual=nn.ModuleList(self.stages)\n                if self.module_list:\n                    if self.module_list.get('serial_eam', False):\n                        if self.module_list['serial_eam'] == 'dct':\n                            dual_cross_att=DualCrossTransformer\n                        elif self.module_list['serial_eam'] == 'rtb':\n                            dual_cross_att=RTBlock\n                    else:\n                        dual_cross_att=RTBlock\n                else:\n                    dual_cross_att=RTBlock\n                self.dual_cross_att=nn.ModuleList([dual_cross_att(in_c) for in_c in encoder_channels[1:3][::-1]])\n            \n\n        self.final = nn.Conv2d(decoder_channels[-1], num_classes, kernel_size=1)\n            \n        if self.module_list:\n            self.att_modules = nn.Sequential(*get_modules(self.module_list, head_channels))\n            if self.module_list.get('msfm', False):\n                if self.module_list['msfm'] == 'dca':\n                    patch=int(self.resolution/min(16,self.downsample_factor))#应该变为crop_size=512，resolution是1024\n                    self.dca=DCA(n=1,\n                                 features = encoder_channels[::-1][:-1],\n                                 patch=patch)\n                elif self.module_list['msfm'] == 'msf':\n                    patch=int(self.resolution/self.downsample_factor)\n                    self.msf=MultiScaleFusion(in_channels=encoder_channels[::-1])\n                if self.dual and self.args.seg_vessel:\n                    if self.module_list['msfm'] == 'rtb':\n                        self.rtb=RTBlock(encoder_channels[0])\n            if self.module_list.get('dam', False):\n                if self.module_list['dam'] == 'msfa':\n                    self.msfa=MultiScaleFreqAttention(decoder_channels[:-1], decoder_channels[-2], freq_method=self.module_list.get('freq_method','dct8'))\n                if self.dual and self.args.seg_vessel:\n                    if self.module_list['dam'] == 'rtb':\n                        self.rtb=RTBlock(decoder_channels[-1], 16)\n                if self.module_list['dam'] == 'msf':\n                    self.msf=MultiScaleFusion(decoder_channels[:-1], decoder_channels[-2])\n            if self.module_list.get('serial_dam', False):\n                if self.module_list['serial_dam'] == 'dct':\n                    dual_cross_att=DualCrossTransformer\n                elif self.module_list['serial_dam'] == 'rtb':\n                    dual_cross_att=RTBlock\n                self.dual_cross_att=nn.ModuleList([dual_cross_att(in_c, patch=None if i<2 else 32) for i,in_c in enumerate(decoder_channels)])\n            \n        \n    \n    def forward(self, x, x_dual=None):\n        encoder_features=[]\n        if self.dual and not self.args.seg_vessel:\n            x_dual = x_dual.unsqueeze(1).repeat(1,3,1,1)\n            for i,(m, m_dual) in enumerate(zip(self.stages, self.stages_dual)):\n                x=m(x)\n                x_dual=m_dual(x_dual)\n                encoder_features.append(x)\n                if i>=2 and i<4:\n                    x, x_dual = self.dual_cross_att[i-2](x, x_dual)\n        else:\n            for m in self.stages:\n                x=m(x)\n                encoder_features.append(x)\n\n        if self.module_list:\n            encoder_features[-1] = self.att_modules(encoder_features[-1])\n            if self.module_list.get('msfm', False):\n                if self.module_list['msfm'] == 'dca':\n                    encoder_features[:-1] = self.dca(encoder_features[:-1])\n                elif self.module_list['msfm'] == 'msf':\n                    encoder_features = self.msf(encoder_features)\n                if self.dual and self.args.seg_vessel:\n                    if self.module_list['msfm'] == 'rtb':\n                        x, x_dual = self.rtb(x)\n                        \n        encoder_features = encoder_features[::-1]\n        decoder_features=[]\n        encoder_features=encoder_features[1:]\n        head_feature = x\n        \n        if self.dual and self.args.seg_vessel:\n            decoder_features_dual=[]\n\n        for i, decoder_block in enumerate(self.blocks):\n            if self.skip_channels[i] != 0:\n                x = decoder_block(x, skip=encoder_features[i])\n                if self.dual and self.args.seg_vessel:\n                    if x_dual == None:\n                        x_dual = head_feature\n                    x_dual = self.blocks_dual[i](x_dual, skip=encoder_features[i])\n                    if self.module_list:\n                        if self.module_list.get('serial_dam', False):\n                            x, x_dual = self.dual_cross_att[i](x, x_dual)\n                    decoder_features_dual.append(x_dual)\n                decoder_features.append(x)\n            \n        if self.module_list:\n            if self.module_list.get('dam', False):\n                if self.module_list['dam'] == 'msfa':\n                    x = self.msfa(decoder_features)\n                if self.dual and self.args.seg_vessel:\n                    if self.module_list['dam'] == 'rtb':\n                        x, x_dual = self.rtb(x,x_dual)\n                        \n        x = self.blocks[-1](x)\n        \n        if self.dual and self.args.seg_vessel:\n            x_dual = self.blocks_dual[-1](x_dual)\n            return self.final(x), self.final_dual(x_dual)\n\n        return self.final(x)\n\n\nprint('unet.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:47.481190Z","iopub.execute_input":"2024-12-23T02:53:47.481452Z","iopub.status.idle":"2024-12-23T02:53:47.509687Z","shell.execute_reply.started":"2024-12-23T02:53:47.481427Z","shell.execute_reply":"2024-12-23T02:53:47.508850Z"}},"outputs":[{"name":"stdout","text":"unet.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n    \nclass UnetPlusDecoderBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels\n    ):\n        super().__init__()\n        self.conv = ConvRelu(in_channels+skip_channels, out_channels)\n\n    def forward(self, x, skip=None):\n        if skip is not None:\n            # print(f'skip:{skip.shape}')\n            # print(f'x:{x.shape}')\n            if skip.shape[2] == x.shape[2]:\n                x = x\n            else:\n                x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        else:\n            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.conv(x)\n        return x\n\n\nclass UnetPP(nn.Module):\n    def __init__(\n        self,\n        **kwargs\n    ):\n        super().__init__()\n        self.args=kwargs['args']\n        self.resolution=kwargs['resolution']\n        self.downsample_factor= self.args.downsample_factor\n        self.module_list = self.args.module_list\n        self.backbone= self.args.backbone\n        self.num_classes= self.args.num_classes\n        self.dual=self.args.dual\n        upsample_mode=self.args.upsample_mode\n        \n        if self.backbone[:2] == 'rs':\n            self.stages, encoder_channels=get_resnet_stages(self.downsample_factor, self.backbone, self.module_list, self.args)\n        elif 'wtconvnext' in self.backbone:\n            self.stages, encoder_channels=get_wtconvnext_stages(self.downsample_factor, self.backbone, self.args)\n        \n        self.stages=nn.ModuleList(self.stages)\n        # encoder_channels = [self.conv1_out_c, self.conv2_out_c, self.conv3_out_c, self.conv4_out_c, self.conv5_out_c]\n        decoder_channels = [1024,512,256,128,64,32]\n        decoder_channels = decoder_channels[6-len(encoder_channels):]\n        \n        encoder_channels = encoder_channels[::-1]\n\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        self.in_channels = [head_channels] + list(decoder_channels[:-1]) # 2048, 256, 128, 64, 32, 16\n        self.skip_channels = list(encoder_channels[1:]) + [0] # 1024,512,128,64,0\n        self.out_channels = decoder_channels \n\n        blocks = {}\n        for layer_idx in range(len(self.in_channels) - 1):\n            for depth_idx in range(layer_idx + 1):\n                if depth_idx == 0:\n                    in_ch = self.in_channels[layer_idx]\n                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1)\n                    out_ch = self.out_channels[layer_idx]\n                else:\n                    out_ch = self.skip_channels[layer_idx]\n                    skip_ch = self.skip_channels[layer_idx] * (\n                        layer_idx + 1 - depth_idx\n                    )\n                    in_ch = self.skip_channels[layer_idx - 1]\n                blocks[f\"x_{depth_idx}_{layer_idx}\"] = DecoderBlock(\n                    in_ch, out_ch, skip_ch, mode=self.args.upsample_mode\n                )\n        blocks[f\"x_{0}_{len(self.in_channels)-1}\"] = DecoderBlock(\n            self.in_channels[-1], self.out_channels[-1], 0, mode=self.args.upsample_mode, scale_factor=2 if len(encoder_channels) == 5 else 4\n        )\n\n        self.final = nn.Conv2d(decoder_channels[-1], self.num_classes, kernel_size=1)\n        \n        self.blocks = nn.ModuleDict(blocks)\n        self.depth = len(self.in_channels) - 1\n        \n        if self.dual:\n            if self.args.seg_vessel:\n                self.blocks_dual = nn.ModuleList([\n                    DecoderBlock(in_ch, out_ch, sk_ch, mode=upsample_mode) for in_ch, out_ch, sk_ch in zip(self.in_channels, self.out_channels, self.skip_channels)\n                ])\n                self.final_dual = nn.Conv2d(decoder_channels[-1], 1, 1)\n            else:\n                self.stages_dual, _=get_resnet_stages(self.downsample_factor, self.backbone, self.module_list, self.args)\n                self.stages_dual=nn.ModuleList(self.stages)\n                if self.module_list:\n                    if self.module_list.get('cross_att', False):\n                        if self.module_list['cross_att'] == 'dct':\n                            dual_cross_att=DualCrossTransformer\n                        elif self.module_list['cross_att'] == 'rtb':\n                            dual_cross_att=RTBlock\n                    else:\n                        dual_cross_att=RTBlock\n                else:\n                    dual_cross_att=RTBlock\n                self.dual_cross_att=nn.ModuleList([dual_cross_att(in_c) for in_c in encoder_channels[1:3][::-1]])\n            \n        \n        \n        \n        if self.module_list:\n            self.att_modules = nn.Sequential(*get_modules(self.module_list, head_channels))\n            if self.module_list.get('msfm', False):\n                if self.module_list['msfm'] == 'dca':\n                    patch=int(self.resolution/min(16,self.downsample_factor))#应该变为crop_size=512，resolution是1024\n                    self.dca=DCA(n=1,\n                                 features = encoder_channels[::-1][:-1],\n                                 patch=patch)\n                elif self.module_list['msfm'] == 'msf':\n                    patch=int(self.resolution/self.downsample_factor)\n                    self.msf=MultiScaleFusion(in_channels=encoder_channels[::-1])\n                if self.dual and self.args.seg_vessel:\n                    if self.module_list['msfm'] == 'rtb':\n                        self.rtb=RTBlock(encoder_channels[0])\n            if self.module_list.get('dam', False):\n                if self.module_list['dam'] == 'msfa':\n                    self.msfa=MultiScaleFreqAttention(decoder_channels[:-1], decoder_channels[-2], freq_method=self.module_list.get('freq_method','dct8'))\n                if self.dual and self.args.seg_vessel:\n                    if self.module_list['dam'] == 'rtb':\n                        self.rtb=RTBlock(decoder_channels[-1], 16)\n                if self.module_list['dam'] == 'msf':\n                    self.msf=MultiScaleFusion(decoder_channels[:-1], decoder_channels[-2])\n        \n\n    def forward(self, x, x_dual=None):\n        encoder_features=[]\n        if self.dual and not self.args.seg_vessel:\n            x_dual = x_dual.unsqueeze(1).repeat(1,3,1,1)\n            for i,(m, m_dual) in enumerate(zip(self.stages, self.stages_dual)):\n                x=m(x)\n                x_dual=m_dual(x_dual)\n                encoder_features.append(x)\n                if i>=2 and i<4:\n                    x, x_dual = self.dual_cross_att[i-2](x, x_dual)\n        else:\n            for m in self.stages:\n                x=m(x)\n                encoder_features.append(x)\n\n\n        if self.module_list:\n            encoder_features[-1] = self.att_modules(encoder_features[-1])\n            if self.module_list.get('msfm', False):\n                if self.module_list['msfm'] == 'dca':\n                    encoder_features[:-1] = self.dca(encoder_features[:-1])\n                elif self.module_list['msfm'] == 'msf':\n                    encoder_features = self.msf(encoder_features)\n                if self.dual and self.args.seg_vessel:\n                    if self.module_list['msfm'] == 'rtb':\n                        x, x_dual = self.rtb(x)\n                    \n        features = encoder_features[::-1]\n        encoder_features = encoder_features[::-1]\n        # print('features:\\n')\n        # for i,f in enumerate(features):\n        #     print('features[{}].shape={}'.format(i, f.shape))\n            \n        # start building dense connections\n        dense_x = {}\n        for layer_idx in range(len(self.in_channels) - 1):\n            for depth_idx in range(self.depth - layer_idx):\n                if layer_idx == 0:\n                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](\n                        features[depth_idx], features[depth_idx + 1]\n                    )\n                    # print('\\ndense_x[x_{}_{}].shape={},\\n input:features[{}].shape={},\\n skip:features[{}].shape={}'.format(\n                    #         depth_idx, depth_idx, output.shape, \n                    #         depth_idx, features[depth_idx].shape,\n                    #         depth_idx+1, features[depth_idx + 1].shape))\n                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n                else:\n                    dense_l_i = depth_idx + layer_idx\n                    cat_names=['dense[x_{}_{}].shape={}'.format(idx, dense_l_i, dense_x[f\"x_{idx}_{dense_l_i}\"].shape) for idx in range(depth_idx + 1, dense_l_i + 1)]\n                    # print(f'cat_features:\\n--------\\n{cat_names}\\n--------\\n')\n                    cat_features = [\n                        dense_x[f\"x_{idx}_{dense_l_i}\"]\n                        for idx in range(depth_idx + 1, dense_l_i + 1)\n                    ]\n                    # print('cat_features+=features[{}].shape={}\\n'.format(dense_l_i + 1, features[dense_l_i + 1].shape))\n                    cat_features = torch.cat(\n                        cat_features + [features[dense_l_i + 1]], dim=1\n                    )\n                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[\n                        f\"x_{depth_idx}_{dense_l_i}\"\n                    ](dense_x[f\"x_{depth_idx}_{dense_l_i-1}\"], cat_features)\n                    # print('dense_x[x_{}_{}].shape={},\\n input:dense_x[f\"x_{}_{}\"].shape={},\\n skip:cat_features.shape={}'.format(\n                    #         depth_idx, dense_l_i, dense_x[f\"x_{depth_idx}_{dense_l_i}\"].shape, \n                    #         depth_idx, dense_l_i-1, dense_x[f\"x_{depth_idx}_{dense_l_i-1}\"].shape,\n                    #         cat_features.shape))\n        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](\n            dense_x[f\"x_{0}_{self.depth-1}\"]\n        )\n        # print('dense_x[x_{}_{}].shape={}, input:dense_x[f\"x_{}_{}\"].shape={}'.format(\n        #                     0, self.depth, dense_x[f\"x_{0}_{self.depth}\"].shape, \n        #                     0, self.depth-1, dense_x[f\"x_{0}_{self.depth-1}\"].shape))\n\n        if self.dual and self.args.seg_vessel:\n            decoder_features_dual=[]\n            if x_dual == None:\n                x_dual = encoder_features[0]\n                encoder_features=encoder_features[1:]\n            for i, decoder_block in enumerate(self.blocks_dual):\n                if self.skip_channels[i] != 0:\n                    # print('encoder_features[{}].shape={}'.format(i, encoder_features[i].shape))\n                    x_dual = decoder_block(x_dual, skip=encoder_features[i])\n                    decoder_features_dual.append(x_dual)\n\n        decoder_features=[dense_x[f'x_{0}_{d}'] for d in range(self.depth)]\n        # for df in decoder_features:\n        #     print(df.shape)\n\n        x = dense_x[f'x_{0}_{self.depth}']\n        \n        if self.module_list:\n            if self.module_list.get('dam', False):\n                if self.module_list['dam'] == 'msfa':\n                    x = self.msfa(decoder_features)\n                    x = self.blocks[f\"x_{0}_{self.depth}\"](x)\n                if self.module_list['dam'] == 'msf':\n                    x = self.msf(decoder_features)\n                    x = self.blocks[f\"x_{0}_{self.depth}\"](x)\n                    \n        \n        \n        if self.dual and self.args.seg_vessel:\n            x_dual = self.blocks_dual[-1](x_dual)\n            return self.final(x), self.final_dual(x_dual)\n        \n        out = self.final(x)\n        return out\n\nprint('unetpp.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:47.511160Z","iopub.execute_input":"2024-12-23T02:53:47.511429Z","iopub.status.idle":"2024-12-23T02:53:47.541784Z","shell.execute_reply.started":"2024-12-23T02:53:47.511389Z","shell.execute_reply":"2024-12-23T02:53:47.541052Z"}},"outputs":[{"name":"stdout","text":"unetpp.py\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\nimport torch\n\n\nclass ResNetLseg(nn.Module):\n    def __init__(self, num_classes=1, pretrained=False, module_list = None, downsample_factor=8, task_num=4, **kwargs):\n        super(ResNetLseg, self).__init__()\n        \n        self.task_num=task_num\n        self.downsample_factor=downsample_factor\n        self.module_list = module_list\n        self.backbone=kwargs['backbone']\n        self.rsd=[self.downsample_factor<(2**i) for i in range(3,6)]\n        if self.backbone == 'rs50':\n            self.encoder = resnet50(pretrained=pretrained, replace_stride_with_dilation=self.rsd)\n        elif self.backbone == 'rs101':\n            self.encoder = resnet101(pretrained=pretrained, replace_stride_with_dilation=self.rsd)\n        elif self.backbone == 'rs152':\n            self.encoder = resnet152(pretrained=pretrained, replace_stride_with_dilation=self.rsd)\n        else:\n            assert f'{self.backbone} does not exists!'\n        \n        self.conv1 = nn.Sequential(self.encoder.conv1,\n                                   self.encoder.bn1,\n                                   self.encoder.relu)#x/2\n        self.conv1_out_c=64\n        self.supervision1=nn.Sequential(nn.Conv2d(self.conv1_out_c,self.task_num,1),\n                                        nn.Upsample(scale_factor=2))\n        \n        self.conv2=nn.Sequential(self.encoder.maxpool,\n                                 self.encoder.layer1)#x/4\n        self.conv2_out_c=256\n        self.supervision2=nn.Sequential(nn.Conv2d(self.conv2_out_c,self.task_num,1),\n                                        nn.Upsample(scale_factor=4))\n        \n        self.conv3=nn.Sequential(self.encoder.layer2)#x/8\n        self.conv3_out_c=512\n        self.supervision3=nn.Sequential(nn.Conv2d(self.conv3_out_c,self.task_num,1),\n                                        nn.Upsample(scale_factor=8))\n        \n        self.conv4=nn.Sequential(self.encoder.layer3)#x/16\n        self.conv4_out_c=1024\n        self.supervision4=nn.Sequential(nn.Conv2d(self.conv4_out_c,self.task_num,1),\n                                        nn.Upsample(scale_factor=min(16,self.downsample_factor)))\n        \n        self.conv5=nn.Sequential(self.encoder.layer4)\n        self.conv5_out_c=2048\n        self.supervision5=nn.Sequential(nn.Conv2d(self.conv5_out_c,self.task_num,1),\n                                        nn.Upsample(scale_factor=min(32,self.downsample_factor)))\n        \n        self.conv1x1=nn.ModuleList([nn.Conv2d(5,1,1) for _ in range(self.task_num)])\n        \n        # self.sigmoid=nn.Sigmoid()\n        # self.final = nn.Conv2d(4, num_classes, kernel_size=1)\n\n    \n    def forward(self, x):\n        conv1 = self.conv1(x)#x/2\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        conv4 = self.conv4(conv3)\n        conv5 = self.conv5(conv4)#x/32\n        \n        feature1 = self.supervision1(conv1)\n        feature2 = self.supervision2(conv2)\n        feature3 = self.supervision3(conv3)\n        feature4 = self.supervision4(conv4)\n        feature5 = self.supervision5(conv5)\n#         print(f'feature1:{feature1.shape}')\n        \n        cw = []\n        for i in range(self.task_num):\n            cw.append(torch.cat((feature1[:,i:i+1],feature2[:,i:i+1],feature3[:,i:i+1],feature4[:,i:i+1],feature5[:,i:i+1]),dim=1))\n        \n        for i,(c,m) in enumerate(zip(cw,self.conv1x1)):\n            cw[i] = m(c)\n            \n        out = torch.cat((cw), dim=1)\n        \n        return feature1, feature2, feature3, feature4, feature5, out\n\nprint('lseg.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:47.542989Z","iopub.execute_input":"2024-12-23T02:53:47.543263Z","iopub.status.idle":"2024-12-23T02:53:47.559593Z","shell.execute_reply.started":"2024-12-23T02:53:47.543239Z","shell.execute_reply":"2024-12-23T02:53:47.558843Z"}},"outputs":[{"name":"stdout","text":"lseg.py\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\n\n\nclass IDRiDDataset(Dataset):\n    def __init__(self, path, dataset_type='train', resolution=[512,700], leisions=['MA','HE','EX','SE'], transform=None, args=None):\n        super(IDRiDDataset, self).__init__()\n        self.task_dicts = {'MA':'1. Microaneurysms','HE':'2. Haemorrhages','EX':'3. Hard Exudates','SE':'4. Soft Exudates','VES':'5. Vessel/640'}\n        self.tasks=self.get_tasks(leisions)\n        self.label2color = {'MA':[0,0,255],'HE':[0,255,0],'EX':[255,0,0],'SE':[255,0,255]}\n\n        self.preprocess=args.preprocess\n        self.resolution=resolution\n        self.dual=args.dual\n        self.args=args\n        \n        \n        if self.preprocess:\n            preprocess_path = os.path.join(path, 'clahe', '2_8')\n            if dataset_type == 'train':\n                self.image_root=os.path.join(preprocess_path, 'trn', 'image')\n                self.mask_root=os.path.join(preprocess_path, 'trn', 'GT')\n            elif dataset_type == 'val':\n                self.image_root=os.path.join(preprocess_path, 'val', 'image')\n                self.mask_root=os.path.join(preprocess_path, 'val', 'GT')\n            else:\n                raise EnvironmentError('You should put a valid mode to generate the dataset')\n        else:\n            if dataset_type == 'train':\n                self.image_root=os.path.join(path,'1. Original Images','a. Training Set')\n                self.mask_root=os.path.join(path,'2. All Segmentation Groundtruths','a. Training Set')\n            elif dataset_type == 'val':\n                self.image_root=os.path.join(path,'1. Original Images', 'b. Testing Set')\n                self.mask_root=os.path.join(path,'2. All Segmentation Groundtruths','b. Testing Set')\n            else:\n                raise EnvironmentError('You should put a valid mode to generate the dataset')\n        self.image_names=os.listdir(self.image_root)\n        self.dataset_type=dataset_type\n        self.transform = transform\n        print('{} dataset contains {} images at {}'.format(self.dataset_type, len(self.image_names), self.image_root))\n        print(self.image_names)\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        'Generate one batch of data'\n        sample = self.load(idx)\n        \n        if self.args.inference:\n            image_name=self.image_names[idx].split('.')[0]\n            return image_name, sample['image'], sample['masks']\n        if self.args.dual:\n            return sample['image'], sample['masks']\n        return sample['image'], sample['masks'][0]\n    \n    def get_tasks(self, leisions):\n        tasks={}\n        for k,v in self.task_dicts.items():\n            if k in leisions:\n                tasks.update({k:v})\n        return tasks\n\n    def eliminate_overlap(self,ms,m):\n        # print('masks : {}  m : {}'.format(ms.shape, m.shape))\n        _m=m>=1\n        _ms=ms>=1\n        inter=np.logical_and(_ms,_m)\n        # print('inter:{}'.format(inter.reshape(-1).tolist().count(True)))\n        m=np.where(inter,0,m)\n        return m\n    \n    def read_mask(self,path):\n        mask=Image.open(path)\n        mask=np.array(mask)\n        #这里不用归一化，因为掩码值已经在0-1内\n        mask = cv2.resize(mask, self.resolution)\n        mask=mask.astype(np.int32)#这里必须是long，否则传入损失函数时会报错\n#         print('read mask shape:{}'.format(mask.shape))\n        return mask\n    \n    def read_image(self,path):\n        image = Image.open(path)\n        image = np.array(image)\n        image = cv2.resize(image, self.resolution)\n        image = np.transpose(image, (2,0,1))#(512, 512, 3)->(3, 512, 512)\n        image = image/255.0 #归一化 (3, 512, 512)\n        image = image.astype(np.float32)\n#         print('read image np shape:{}'.format(image.shape))\n        image = torch.from_numpy(image)\n        return image\n        \n    def load(self, idx):\n        # Get masks from a particular idx\n        image_name=self.image_names[idx]\n        image_path = os.path.join(self.image_root, image_name)\n        image = self.read_image(image_path)\n        \n        idx=image_name.split('_')[1][:2]\n\n        mask=np.zeros(image.shape[1:3],dtype=np.int32)\n#         out_str='\\n'\n        for i,task in enumerate(self.tasks.items()):\n            suffix = '.tif'\n            mask_name = 'IDRiD_'+ idx + '_' + task[0] + suffix  # if idx = 0. we look for the image 1\n            mask_path = os.path.join(self.mask_root, task[1], mask_name)\n            if os.path.exists(mask_path):\n                m = self.read_mask(mask_path)*(i+1)\n                if len(m.shape)==3:\n                    m=np.transpose(m,(2,0,1))\n                    m=m[0]/765\n                    m=m.astype(np.int32)\n                m = self.eliminate_overlap(mask,m)\n                mask+=m\n#                 out_str+='{} - {} - {} mask : {} | masks : {}\\n'.format(image_name, mask_name, task[0], np.unique(m), np.unique(mask))\n        \n        if self.dual:\n            suffix='.tif'\n            ves_mask_name = 'IDRiD_'+ idx + '_VES' + suffix\n            ves_mask_path = os.path.join(self.mask_root, self.task_dicts['VES'], ves_mask_name)\n            ves_mask = self.read_mask(ves_mask_path)\n            sample = {'image':image, 'masks':[torch.from_numpy(mask),torch.from_numpy(ves_mask)]}\n        else:   \n            sample = {'image':image , 'masks': [torch.from_numpy(mask)]}\n        # If transform apply transformation\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n    \nclass DDRDataset(Dataset):\n    def __init__(self, path, dataset_type='train', resolution=[512,700], leisions=['MA','HE','EX','SE'], transform=None, args=None):\n        super(DDRDataset, self).__init__()\n        self.task_dicts = {'MA':'MA','HE':'HE','EX':'EX','SE':'SE'}\n        self.tasks=self.get_tasks(leisions)\n        self.label2color = {'MA':[0,0,255],'HE':[0,255,0],'EX':[255,0,0],'SE':[255,0,255]}\n\n        self.preprocess=args.preprocess\n        self.resolution=resolution\n        self.dual=args.dual\n        self.args=args\n\n        if self.preprocess:\n            if dataset_type == 'train':\n                self.image_root=os.path.join(path, 'trn', 'image')\n                self.mask_root=os.path.join(path, 'trn', 'GT')\n            elif dataset_type == 'val':\n                self.image_root=os.path.join(path, 'val', 'image')\n                self.mask_root=os.path.join(path, 'val', 'GT')\n            elif dataset_type == 'test':\n                self.image_root=os.path.join(path, 'tst', 'image')\n                self.mask_root=os.path.join(path, 'tst', 'GT')\n                \n            else:\n                raise EnvironmentError('You should put a valid mode to generate the dataset')\n        else:\n            if dataset_type == 'train':\n                self.image_root=os.path.join(path, 'train', 'image')\n                self.mask_root=os.path.join(path, 'train', 'label')\n            elif dataset_type == 'val':\n                self.image_root=os.path.join(path, 'valid', 'image')\n                self.mask_root=os.path.join(path, 'valid', 'segmentation label')\n            elif dataset_type == 'test':\n                self.image_root=os.path.join(path, 'test', 'image')\n                self.mask_root=os.path.join(path, 'test', 'label')\n            else:\n                raise EnvironmentError('You should put a valid mode to generate the dataset')\n        \n            \n        \n        self.image_names=os.listdir(self.image_root)\n        self.dataset_type=dataset_type\n        self.transform = transform\n        print('{} dataset contains {} images at {}'.format(self.dataset_type, len(self.image_names), self.image_root))\n        \n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        'Generate one batch of data'\n        sample = self.load(idx)\n        \n        if self.args.inference:\n            image_name=self.image_names[idx].split('.')[0]\n            return image_name, sample['image'], sample['masks']\n        if self.args.dual:\n            return sample['image'], sample['masks']\n        return sample['image'], sample['masks'][0]\n    \n    def get_tasks(self, leisions):\n        tasks={}\n        for k,v in self.task_dicts.items():\n            if k in leisions:\n                tasks.update({k:v})\n        return tasks\n\n    def eliminate_overlap(self,ms,m):\n        # print('masks : {}  m : {}'.format(ms.shape, m.shape))\n        _m=m>=1\n        _ms=ms>=1\n        inter=np.logical_and(_ms,_m)\n        # print('inter:{}'.format(inter.reshape(-1).tolist().count(True)))\n        m=np.where(inter,0,m)\n        return m\n    \n    def read_mask(self,path):\n        mask=Image.open(path)\n        mask=np.array(mask)\n        #这里不用归一化，因为掩码值已经在0-1内\n        mask = cv2.resize(mask, self.resolution)\n        if not self.preprocess:\n            mask = mask/255.0\n        mask=mask.astype(np.int32)#这里必须是long，否则传入损失函数时会报错\n#         print('read mask shape:{}'.format(mask.shape))\n        return mask\n    \n    def read_image(self,path):\n        image = Image.open(path)\n        image = np.array(image)\n        image = cv2.resize(image, self.resolution)\n        image = np.transpose(image, (2,0,1))#(512, 512, 3)->(3, 512, 512)\n        image = image/255.0 #归一化 (3, 512, 512)\n        image = image.astype(np.float32)\n#         print('read image np shape:{}'.format(image.shape))\n        image = torch.from_numpy(image)\n        return image\n        \n    def load(self, idx):\n        # Get masks from a particular idx\n        image_name=self.image_names[idx]\n        image_path = os.path.join(self.image_root, image_name)\n        image = self.read_image(image_path)\n        \n        idx=image_name.split('.')[0]\n        \n        mask=np.zeros(image.shape[1:3],dtype=np.int32)\n#         out_str='\\n'\n        for i,task in enumerate(self.tasks.items()):\n            suffix = '.tif'\n            mask_name = idx + suffix  # if idx = 0. we look for the image 1\n            mask_path = os.path.join(self.mask_root, task[1], mask_name)\n            if os.path.exists(mask_path):\n                m = self.read_mask(mask_path)*(i+1)\n                if len(m.shape)==3:\n                    m=np.transpose(m,(2,0,1))\n                    m=m[0]/765\n                    m=m.astype(np.int32)\n                m = self.eliminate_overlap(mask,m)\n                mask+=m\n#                 out_str+='{} - {} - {} mask : {} | masks : {}\\n'.format(image_name, mask_name, task[0], np.unique(m), np.unique(mask))\n        \n        sample = {'image':image , 'masks': [torch.from_numpy(mask)]}\n        # If transform apply transformation\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n    \nclass VesselDataset(Dataset):\n    def __init__(self, path, dataset_type='train', resolution=[512,700], transform=None, args=None):\n        super(VesselDataset, self).__init__()\n        self.preprocess=args.preprocess\n        self.resolution=resolution\n        self.args=args\n        self.tasks={'VES':'Vessel'}\n        \n        if self.preprocess:\n            preprocess_path = os.path.join(path, 'clahe', '2_8')\n            if dataset_type == 'train':\n                self.image_root=os.path.join(preprocess_path, 'trn', 'image')\n                self.mask_root=os.path.join(preprocess_path, 'trn', 'GT')\n            elif dataset_type == 'val':\n                self.image_root=os.path.join(preprocess_path, 'val', 'image')\n                self.mask_root=os.path.join(preprocess_path, 'val', 'GT')\n            else:\n                raise EnvironmentError('You should put a valid mode to generate the dataset')\n        else:\n            if dataset_type == 'train':\n                self.image_root=os.path.join(path,'Training','Images')\n                self.mask_root=os.path.join(path,'Training','Masks')\n            elif dataset_type == 'val':\n                self.image_root=os.path.join(path,'Test','Images')\n                self.mask_root=os.path.join(path,'Test','Masks')\n            else:\n                raise EnvironmentError('You should put a valid mode to generate the dataset')\n        self.image_names=os.listdir(self.image_root)\n        self.dataset_type=dataset_type\n        self.transform = transform\n        print('{} dataset contains {} images'.format(self.dataset_type, len(self.image_names)))\n        print(self.image_names)\n        \n\n    def __len__(self):\n        return len(self.image_names)\n    \n    def __getitem__(self, idx):\n        'Generate one batch of data'\n        sample = self.load(idx)\n        return sample['image'], sample['mask']\n\n    def read_mask(self,path):\n        mask=Image.open(path)\n        mask=np.array(mask)\n        mask=mask/np.max(mask)\n        mask = cv2.resize(mask, self.resolution)\n        mask=mask.astype(np.int32)#这里必须是long，否则传入损失函数时会报错\n#         print('read mask shape:{}'.format(mask.shape))\n        return mask\n    \n    def read_image(self,path):\n        image = Image.open(path)\n        image = np.array(image)\n        image = cv2.resize(image, self.resolution)\n        image = np.transpose(image, (2,0,1))#(512, 512, 3)->(3, 512, 512)\n        image = image/255.0 #归一化 (3, 512, 512)\n        image = image.astype(np.float32)\n#         print('read image np shape:{}'.format(image.shape))\n        image = torch.from_numpy(image)\n        return image\n        \n    def load(self, idx):\n        # Get masks from a particular idx\n        image_name=self.image_names[idx]\n        image_path = os.path.join(self.image_root, image_name)\n        image = self.read_image(image_path)\n        \n        name=image_name.split('.')[0]\n\n        if name.split('_')[-1] == 'HRF':\n            suffix = '.tif'\n        elif name.split('_')[-1] == 'DRIVE':\n            suffix = '.gif'\n        elif name.split('_')[-1] == 'CHASE':\n            suffix = '.png'\n        mask_name = name + suffix  # if idx = 0. we look for the image 1\n        mask_path = os.path.join(self.mask_root, mask_name)\n        if os.path.exists(mask_path):\n            mask = self.read_mask(mask_path)\n#                 out_str+='{} - {} - {} mask : {} | masks : {}\\n'.format(image_name, mask_name, task[0], np.unique(m), np.unique(mask))\n        \n#         print(out_str)\n        sample = {'image':image , 'mask': torch.from_numpy(mask)}\n        # If transform apply transformation\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n        \nprint('dataset.py')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:47.560949Z","iopub.execute_input":"2024-12-23T02:53:47.561612Z","iopub.status.idle":"2024-12-23T02:53:47.602117Z","shell.execute_reply.started":"2024-12-23T02:53:47.561576Z","shell.execute_reply":"2024-12-23T02:53:47.601262Z"}},"outputs":[{"name":"stdout","text":"dataset.py\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nimport cv2\n\nimport torch\nfrom torch.utils.data import Dataset\nimport csv\nimport itertools\nfrom torch.utils.data.sampler import Sampler\nfrom torch.utils.data.dataloader import DataLoader\nimport torchvision.transforms as transforms\n\n# from utils import *\n# from contrast import ImageContraster\n\nNO_LABEL=-1\n# img_cter=ImageContraster()\n\nclass ImageDataset(Dataset):\n    def __init__(self):\n        super(ImageDataset, self).__init__()\n    \n    def read_label(self, path):\n        with open(path,'r') as f:\n            if isinstance(self.num_classes, list):\n                return np.array(list(csv.reader(f,delimiter=',')))[1:,:len(self.num_classes)+1]\n            else:\n                return np.array(list(csv.reader(f,delimiter=',')))[1:,:2]\n    \n    def read_image(self,path):\n        size=self.resolution#(W,H)\n        image = cv2.imread(path, cv2.IMREAD_COLOR)\n        # image = image[:,:,::-1] #将BGR变为RGB\n        image = cv2.resize(image,size)\n        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n        #这里是为了兼容不使用transform的情况\n        image = image/255.0 #归一化 (3, 512, 512)\n        image = image.astype(np.float32)\n        image = torch.from_numpy(image)\n        return image\n    \n    def toImage(self,img):\n        img=img.numpy()\n        img*=255\n        img=np.transpose(img,(1,2,0))#传入Image.fromarray的图片形状必须为（H,W,C），否则会报错\n        img=np.uint8(img)#传入Image.fromarray的图片必须是uint8格式，否则报错\n        img = Image.fromarray(img)\n        return img\n    \n    def get_class_weight(self, num_classes):\n        class_weight=[]\n        if isinstance(num_classes, list):\n            for i,n in enumerate(num_classes):\n                cw=[]\n                l=self.label[:,i]\n                total = len(l)\n                for i in range(n):\n                    t=total/len(l[l==i])\n                    cw.append(t/n)\n                class_weight.append(torch.tensor(cw))\n            return class_weight\n        total=len(self.label)\n        for i in range(num_classes):\n            t=total/len(self.label[self.label==i])\n            class_weight.append(t/num_classes)\n        return torch.tensor(class_weight)\n    \n    def load_data_from_dir(self):\n        label=[]\n        image_names=[]\n        for i in range(self.num_classes):\n            names=os.listdir(os.path.join(self.image_root,str(i)))\n            image_names+=names\n            label+=[i for _ in range(len(names))]\n        l=np.array([image_names,label])\n        l=l.transpose()\n        np.random.shuffle(l)\n        print(l.shape)\n        return l\n    \n    def balance_dataset(self, label,num_classes,factor=1.5):\n        l=label[:,1].astype(np.int32)\n        l_list=l.tolist()\n        cnt=[l_list.count(i) for i in range(num_classes)]\n        min_num=min(cnt)\n        max_num=int(min_num*factor)\n        cnt=np.array(cnt).clip(min_num,max_num)\n    \n        balanced_l=[]\n        for i in range(num_classes):\n            t=label[l==i]\n            if i == 0:\n                balanced_l=t[:cnt[i]]\n            else:\n                balanced_l=np.vstack((balanced_l,t[:cnt[i]]))\n    \n        print(balanced_l.shape)\n        return balanced_l\n\nclass IDataset(ImageDataset):\n    def __init__(self, path, dataset_type='train', resolution=512, args=None):\n        super(IDataset, self).__init__()\n        self.args=args\n        self.resolution=resolution\n        self.dataset_type=dataset_type\n        self.num_classes=args.gan_grading_classes\n        self.transform=None\n        if dataset_type == 'train':\n            self.image_root=os.path.join(path,'1. Original Images','a. Training Set')\n            self.label=self.read_label(os.path.join(path,'2. Groundtruths','a. IDRiD_Disease Grading_Training Labels.csv'))\n        elif dataset_type == 'val':\n            self.image_root=os.path.join(path,'1. Original Images', 'b. Testing Set')\n            self.label=self.read_label(os.path.join(path,'2. Groundtruths','b. IDRiD_Disease Grading_Testing Labels.csv'))\n        else:\n            self.trn_image_root=os.path.join(path,'1. Original Images','a. Training Set')\n            trn_label=self.read_label(os.path.join(path,'2. Groundtruths','a. IDRiD_Disease Grading_Training Labels.csv'))\n            self.tst_image_root=os.path.join(path,'1. Original Images', 'b. Testing Set')\n            tst_label=self.read_label(os.path.join(path,'2. Groundtruths','b. IDRiD_Disease Grading_Testing Labels.csv'))\n            self.trn_length=len(trn_label)\n            self.label = np.vstack((trn_label,tst_label))\n\n        self.image_names=self.label[:,0]\n        self.label=self.label[:,1:].astype(np.int32).squeeze()\n        self.count_result=count_class(self.label, self.num_classes)\n        self.class_weight=self.get_class_weight(self.num_classes)\n        self.dataset_len=len(self.image_names)\n        if dataset_type != 'all': print(f'{self.image_root} contains {self.dataset_len} samples.')\n        print(self.count_result)\n\n    def __getitem__(self, idx):\n        # print(f'idx:{idx}')\n        if self.dataset_type == 'all':\n            if idx < self.trn_length:\n                self.image_root = self.trn_image_root\n            else:\n                self.image_root = self.tst_image_root\n                \n        image_path=os.path.join(self.image_root,self.image_names[idx]+'.jpg')\n        img=self.read_image(image_path)\n#         print(img.shape)\n        if self.transform is not None:\n            img=self.toImage(img)\n            img = self.transform(img)\n        \n        if isinstance(self.num_classes, list):\n            temp=[img]\n            for l in self.label[idx]:\n                temp.append(int(l))\n            return temp\n        \n        return img,int(self.label[idx])\n\n    def __len__(self):\n        return self.dataset_len\n\nclass GradingDDRDataset(ImageDataset):\n    def __init__(self, path, label, dataset_type='train', resolution=512, args=None, binary=False):\n        super(GradingDDRDataset, self).__init__()\n        self.args=args\n        self.binary=binary\n        self.resolution=resolution\n        self.dataset_type=dataset_type\n        self.image_root=path\n        self.num_classes=args.gan_grading_classes\n        self.label=label\n        self.image_names=self.label[:,0]\n        self.label=self.label[:,1].astype(np.int32).squeeze()\n        self.count_result=count_class(self.label, self.num_classes)\n        self.class_weight=self.get_class_weight(self.num_classes)\n        \n        self.dataset_len=len(self.label)\n        self.transform=None\n        print(f'{self.image_root}/{self.dataset_type} contains {self.dataset_len} samples.')\n        print(self.count_result)\n        self.need_suffix=False\n        if len(self.image_names[0].split('.')) == 1:\n            self.need_suffix=True\n\n    def __getitem__(self, idx):\n#         print(f'idx:{idx}, image_names:{self.image_names[idx]}, label:{self.label[idx]}\\n')\n        image_path=os.path.join(self.image_root,self.image_names[idx])\n        if self.need_suffix:\n            image_path=image_path+'.jpeg'\n#         print(image_path)\n        img=self.read_image(image_path)\n        if self.transform is not None:\n            img=self.toImage(img)\n            img = self.transform(img)\n            \n        if self.binary:\n            if self.label[idx] <= 1:\n                temp_l=0\n            else:\n                temp_l=1\n            return img,int(temp_l)\n        return img,int(self.label[idx])\n\n    def __len__(self):\n        return self.dataset_len\n    \nclass CamDataset(ImageDataset):\n    def __init__(self, path, label, dataset_type='train', resolution=512, args=None, binary=False):\n        super(CamDataset, self).__init__()\n        self.args=args\n        self.binary=binary\n        self.resolution=resolution\n        self.dataset_type=dataset_type\n        self.image_root=path\n        self.num_classes=args.gan_grading_classes\n        self.label=label\n        self.image_names=self.label[:,0]\n        self.label=self.label[:,1].astype(np.int32).squeeze()\n        self.count_result=count_class(self.label, self.num_classes)\n        self.class_weight=self.get_class_weight(self.num_classes)\n        \n        self.dataset_len=len(self.label)\n        self.transform=None\n        print(f'{self.image_root}/{self.dataset_type} contains {self.dataset_len} samples.')\n        print(self.count_result)\n        self.need_suffix=False\n        if len(self.image_names[0].split('.')) == 1:\n            self.need_suffix=True\n    \n    def read_image(self,path):\n        size=np.array([1,1])*self.resolution#(W,H)\n        image = cv2.imread(path, cv2.IMREAD_COLOR)\n        # image = image[:,:,::-1]\n#         image = cv2.resize(image,size)\n        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n        image = image/255.0 ## (512, 512, 3)\n        image = image.astype(np.float32)\n        image = torch.from_numpy(image)\n        return image\n\n    def __getitem__(self, idx):\n#         print(f'idx:{idx}, image_names:{self.image_names[idx]}, label:{self.label[idx]}\\n')\n        image_path=os.path.join(self.image_root,self.image_names[idx])\n        if self.need_suffix:\n            image_path=image_path+'.jpeg'\n#         print(image_path)\n        img=self.read_image(image_path)\n        if self.transform is not None:\n            img=self.toImage(img)\n            img = self.transform(img)\n            \n        if self.binary:\n            if self.label[idx] <= 1:\n                temp_l=0\n            else:\n                temp_l=1\n            return img,int(temp_l)\n        return self.image_names[idx],img,int(self.label[idx])\n\n    def __len__(self):\n        return self.dataset_len\n    \nclass EyepacDataset(ImageDataset):\n    def __init__(self, path, dataset_type='train', resolution=512, args=None):\n        super(EyepacDataset, self).__init__()\n        self.args=args\n        self.dataset_type=dataset_type\n        self.num_classes=args.gan_grading_classes\n        self.resolution = resolution\n        \n        self.image_root = os.path.join(path, dataset_type)\n        self.label = self.load_data_from_dir()\n        self.image_names=self.label[:,0]\n        self.label=self.label[:,1].astype(np.int32).squeeze()\n        self.count_result=count_class(self.label, self.num_classes)\n        self.class_weight=self.get_class_weight(self.num_classes)\n        \n        self.dataset_len=len(self.label)\n        self.transform=None\n        print(f'{self.image_root}/{self.dataset_type} contains {self.dataset_len} samples.')\n        print(self.count_result)\n        \n        \n    def __getitem__(self, idx):\n#         print(f'idx:{idx}, image_names:{self.image_names[idx]}, label:{self.label[idx]}\\n')\n        image_path=os.path.join(self.image_root,str(self.label[idx]),self.image_names[idx])\n#         print(image_path)\n        img=self.read_image(image_path)\n        if self.transform is not None:\n            img=self.toImage(img)\n            img = self.transform(img)\n            \n        return img,int(self.label[idx])\n\n    def __len__(self):\n        return self.dataset_len\n\nprint('grading_dataset.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:47.603321Z","iopub.execute_input":"2024-12-23T02:53:47.603581Z","iopub.status.idle":"2024-12-23T02:53:47.637242Z","shell.execute_reply.started":"2024-12-23T02:53:47.603557Z","shell.execute_reply":"2024-12-23T02:53:47.636439Z"}},"outputs":[{"name":"stdout","text":"grading_dataset.py\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import *\n\nimport wandb\nfrom torch.autograd import Variable\nimport os\nfrom tqdm import tqdm\nimport copy\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\n\nimport logger\nfrom torch.utils.data import dataloader\nimport time\n\n\n# from utils import utils\n# from utils import metrics\n# from loss import *\n\nclass SLTrainer:\n    def __init__(self, model, optimizer, criterion, device, config):\n        print(\"Supervised trainer\")\n        self.ssl_name='SL'\n        self.config=config\n        self.model=model\n        self.optimizer=optimizer\n        self.device=device\n        self.criterion=criterion\n        self.estimator=Estimator(config.metrics, config.num_classes)\n        self.id=config.id\n        \n        if self.config.dual and self.config.seg_vessel:\n            self.criterion_extra=nn.BCEWithLogitsLoss(pos_weight=torch.tensor(10).to(self.device))\n        \n        self.data_dir=os.path.join(config.checkpoint_dir,'Supervised', self.id)\n        self.log_dir=os.path.join(self.data_dir,'log')\n        self.cpt_dir=os.path.join(self.data_dir,'checkpoint')\n        self.best_checkpoint=''\n        self.epoch       = 0\n        self.start_epoch = 0\n        self.last_epoch = -1\n        mkdir(self.data_dir)\n        mkdir(self.log_dir)\n        mkdir(self.cpt_dir)\n        \n    def trn_iteration(self, data_loader, scheduler):\n        self.model.train()\n        self.estimator.reset()\n        trn_loss = AverageMeter()\n        \n        lr=self.optimizer.state_dict()['param_groups'][0]['lr']\n        with tqdm(total=len(data_loader), desc=f\"Epoch {self.epoch}/{self.config.num_epochs} --- training lr:{lr}\", unit='batch') as pbar:\n            for step, (x, y) in enumerate(data_loader):\n                if self.config.dual:\n                    x = x.to(self.device)\n                    x = Variable(x)\n                    y = [_y.to(self.device) for _y in y]\n                    y_extra = Variable(y[1]).float()\n                    y = Variable(y[0]).float()\n                    if self.config.seg_vessel:\n                        y_pred = self.model(x.float())\n                        y_extra_pred = y_pred[1].squeeze()\n                        y_pred = y_pred[0]\n                    else:\n                        y_pred = self.model(x.float(), y_extra)\n                else:\n                    x, y = [t.to(self.device) for t in (x,y)]\n                    x, y=Variable(x),Variable(y)\n                    y_pred = self.model(x.float())\n                    \n                loss = self.criterion(y_pred, y.long())\n                if self.config.dual and self.config.seg_vessel:\n                    loss_extra = self.criterion_extra(y_extra_pred, y_extra)\n                    loss += self.config.dual_loss_factor*loss_extra\n                \n                if self.config.accumulation_step:\n                    loss /= self.config.accumulation_step\n                    loss.backward()\n                    \n                    if ((step + 1) % self.config.accumulation_step == 0) or (step + 1 == len(data_loader)):\n                        trn_loss.update(loss.item()*self.config.accumulation_step)\n                        self.optimizer.step()\n                        self.optimizer.zero_grad()\n                else: \n                    trn_loss.update(loss.item())\n                    self.optimizer.zero_grad()\n                    loss.backward()\n                    self.optimizer.step()\n                if not self.config.upd_by_ep and scheduler != None: scheduler.step()\n                self.estimator.update(y_pred, y)\n                pbar.update(1)\n        \n        score=self.estimator.get_scores(4)\n\n        for m in set(self.estimator.need_named_metrics)&set(score.keys()):\n            val=score.pop(m)\n            score.update(self.estimator.name_val(data_loader.dataset.tasks, m, val))\n        \n        info_dict=dict([('epoch', self.epoch),\n                   ('lr', lr),\n                   ('trn_loss', round(trn_loss.avg,4))])\n        \n        info_dict.update({'trn_'+k:v for k,v in score.items()})\n        # if self.config.dual:\n        #     k=self.config.dual\n        #     score=self.estimator_extra.get_scores(4)\n        #     if self.config.show_log:\n        #         print(f'trn_gt_{k}:{self.estimator_extra.count(\"gt\")}')\n        #         print(f'trn_pred_{k}:{self.estimator_extra.count(\"pred\")}')\n        #     info_dict.update({'trn_'+_k+'_'+k:_v for _k,_v in score.items()})\n        return info_dict\n        \n    def val_iteration(self, data_loader, val_loss):\n        self.model.eval()\n        \n        self.estimator.reset()\n        \n        with tqdm(total=len(data_loader), desc=f\"Epoch {self.epoch}/{self.config.num_epochs} --- validating\", unit='batch') as pbar:\n            with torch.no_grad():\n                for step, (x, y) in enumerate(data_loader):\n                    if self.config.dual:\n                        x = x.to(self.device)\n                        x = Variable(x)\n                        y = [_y.to(self.device) for _y in y]\n                        y_extra = Variable(y[1]).float()\n                        y = Variable(y[0]).float()\n                        if self.config.seg_vessel:\n                            y_pred = self.model(x.float())\n                            y_extra_pred = y_pred[1].squeeze()\n                            y_pred = y_pred[0]\n                        else:\n                            y_pred = self.model(x.float(), y_extra)\n                    \n                    elif self.config.grid_size:\n                        x, y = [t.to(self.device) for t in (x,y)]\n                        x, y=Variable(x),Variable(y)\n                        x = x.float()\n                        x = x.transpose(0,1) # b, p1, p2, c, h, w\n                        y = y.transpose(0,1)\n                        \n                        y_pred_pieces = []\n                        for x_piece in x:\n                            y_pred_pieces.append(self.model(x_piece.float()))\n                    else:\n                        x, y = [t.to(self.device) for t in (x,y)]\n                        x, y=Variable(x),Variable(y)\n                        y_pred = self.model(x.float())\n\n                    \n                    if self.config.grid_size:\n                        loss = self.criterion(y_pred_pieces[0], y[0].long())\n                        for y_pred,y_true in zip(y_pred_pieces, y):\n                            self.estimator.update(y_pred, y_true)\n                    else:\n                        loss=self.criterion(y_pred, y.long())\n                        self.estimator.update(y_pred, y)\n                    if self.config.dual and self.config.seg_vessel:\n                        loss_extra = self.criterion_extra(y_extra_pred, y_extra)\n                        loss += self.config.dual_loss_factor*loss_extra\n                        \n                    if self.config.accumulation_step:\n                        loss /= self.config.accumulation_step\n                        if ((step + 1) % self.config.accumulation_step == 0) or (step + 1 == len(data_loader)):\n                            val_loss.update(loss.item()*self.config.accumulation_step)\n                    else: \n                        val_loss.update(loss.item())\n\n                    pbar.update(1) \n                \n        score=self.estimator.get_scores(4)\n        \n        for m in set(self.estimator.need_named_metrics)&set(score.keys()):\n            val=score.pop(m)\n            score.update(self.estimator.name_val(data_loader.dataset.tasks, m, val))\n        \n        info_dict=dict([('val_loss', round(val_loss.avg,4))])\n        info_dict.update({'val_'+k:v for k,v in score.items()})\n        # info_dict.update({f'roc_e{self.epoch}':self.plot_roc(self.estimator.y, self.estimator.y_pred)})\n        \n        return info_dict\n    \n    def loop(self, num_epochs, trn_loader, val_loader):\n        logger.configure(dir=self.log_dir, log_suffix=self.id)\n        \n        if self.config.use_wandb:\n            wandb.init(\n              project=self.config.experiment_name,\n              config=vars(self.config)\n            )\n            wandb.run.name = self.id\n            wandb.run.save()\n            \n        self.load()\n            \n        scheduler=get_scheduler(self.config, self.optimizer, last_epoch = self.last_epoch)\n        \n        best_perf = [sys.float_info.min]*2\n        val_loss = AverageMeter()\n        last_info=None\n\n        if self.config.use_wandb:\n            for k,v in self.model.named_children():\n                print(f'{k}:{v}')\n\n        timer = Timer()\n        \n        for epoch in range(self.start_epoch+1, num_epochs+1):\n            self.epoch=epoch\n            timer.start_timer(time.time())\n            info_dict=self.trn_iteration(trn_loader, scheduler)\n            timer.compute(time.time())\n            if self.config.upd_by_ep and scheduler != None: scheduler.step()\n                    \n            if epoch % self.config.save_interval == 0:\n                self.save(epoch, self.model.state_dict(), self.optimizer.state_dict())\n            \n            if epoch % self.config.validation_interval == 0 or epoch == 1:\n                timer.start_timer(time.time())\n                info_dict.update(self.val_iteration(val_loader, val_loss))\n                timer.compute(time.time())\n                \n                if info_dict['val_'+self.config.best_metric] > best_perf[1]:\n                    best_perf[0]=epoch\n                    best_perf[1]=info_dict['val_'+self.config.best_metric]\n                    self.best_checkpoint = [os.path.join(self.cpt_dir, self.id+f'_e{epoch}.pt')]\n                    if epoch != num_epochs:\n                        self.best_checkpoint.append(os.path.join(self.cpt_dir, self.id+f'_e{num_epochs}.pt'))\n                    if epoch % self.config.save_interval != 0 and info_dict['val_'+self.config.best_metric] > self.config.good_value:\n                        self.save(epoch, self.model.state_dict(), self.optimizer.state_dict())\n                    \n                if epoch == num_epochs:\n                    last_info= {self.config.best_metric:info_dict['val_'+self.config.best_metric],'best_cpt':self.best_checkpoint}\n                    \n            if self.config.use_wandb:\n                    wandb.log(info_dict)\n\n            if self.config.show_log:\n                logger.logkvs(info_dict)\n                logger.dumpkvs()\n                str_output = 'epoch & {} \\n'.format(self.config.best_metric)\n                str_output += f'{best_perf[0]} & {best_perf[1]:.4f} \\n'\n                logger.log(str_output)\n            \n            if val_loss.is_overfitting() and self.config.early_stop:\n                print('Detected overfitting signs, stop training.')\n                break\n            \n            val_loss.reset()\n                \n        if self.config.use_wandb:\n            wandb.finish()\n            \n        return last_info\n    \n    def evaluate(self, val_loader):\n        print('-------------------EVALUATING-------------------')\n        \n        logger.configure(dir=self.log_dir, log_suffix=self.id)\n\n        cpt_name=os.path.split(self.config.resume_checkpoint)[-1].split('.')[0]\n        account=cpt_name.split('_')[-3]\n        version=cpt_name.split('_')[-2]\n        \n        if self.config.use_wandb:\n            wandb.init(\n              project=self.config.experiment_name,\n              config=vars(self.config)\n            )\n            wandb.run.name = f'test_{self.config.dataset_name}_by_{account}_{version}'\n            wandb.run.save()\n        \n        self.load()\n        \n        val_loss = AverageMeter()\n        self.epoch=1\n        info_dict=self.val_iteration(val_loader, val_loss)\n        \n        if self.config.use_wandb:\n            wandb.log(info_dict)\n        for k,v in info_dict.items():\n            print(f'{k}:{v}')\n        # logger.logkvs(info_dict)\n        # logger.dumpkvs()\n        \n    def inference(self, data_loader, type):\n        print('-------------------INFERENCE-------------------')\n        logger.configure(dir=self.log_dir, log_suffix=self.id)\n        \n        self.load()\n        \n        self.model.eval()\n        \n        result_dir=os.path.join(self.config.inference_root_dir, self.config.dataset_name, type)\n        mkdir(result_dir)\n        \n        with tqdm(total=len(data_loader), desc=f\"Inference\", unit='batch') as pbar:\n            with torch.no_grad():\n                for step, (n, x, y) in enumerate(data_loader):\n                    x, y = x.to(self.device), y.to(self.device)\n                    x, y = Variable(x), Variable(y)\n    \n                    y_pred = self.model(x.float())\n                    logits = y_pred.detach().cpu()\n                    x_np = x.detach().cpu().numpy()\n                    \n                    logits = F.softmax(logits, dim=1)\n                    predict = np.argmax(logits.numpy(), axis=1).astype(np.uint8)\n                    \n                    for i,p in enumerate(predict):\n                        img = Image.fromarray(p)\n                        result_path=os.path.join(result_dir, n[i]+'_'+self.config.inference_suffix+'.tif')\n                        print(result_path)\n                        img.save(result_path, format='TIFF')\n                        \n                        o_img=np.transpose(x_np[i],(1,2,0))\n                        imgs=[o_img, img]\n                        titles=[n[i], 'vessel_mask']\n                        fig,axs = plt.subplots(1,len(imgs),figsize=(15,30),sharey=True)\n                        for i,ax in enumerate(axs):\n                            ax.imshow(imgs[i])\n                            ax.set_title(titles[i])\n                            ax.axis('off')\n                        plt.show()\n                    \n                    pbar.update(1) \n        \n    def plot_roc(self, y, y_pred):\n        fpr, tpr, thresholds = roc_curve(y, y_pred)\n        roc_auc = auc(fpr, tpr)\n        fig=plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'Receiver Operating Characteristic e{self.epoch}')\n        plt.legend(loc=\"lower right\")\n        import matplotlib_inline\n        matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n        plt.show()\n        plt.save(f'/kaggle/working/roc_{self.epoch}.svg')\n        plt.close()\n        return fig\n    \n    def plot_cm(self,cm,name):\n        import seaborn as sns\n        plt.figure()\n        sns.heatmap(cm, annot=True, cmap=\"Blues\", cbar=False, fmt=\"2d\")\n\n        # 设置标签和标题\n        plt.xlabel('Predict')\n        plt.ylabel(\"Ground Truth\")\n        plt.gca().xaxis.tick_top()\n        plt.gca().xaxis.set_label_position('top')\n        # plt.title()\n#         import matplotlib_inline\n#         matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n        plt.savefig(os.path.join(self.data_dir,f'{name}_cm_e{self.epoch}.svg'), format='svg')\n#         # 显示图像\n#         plt.show()\n        plt.close()\n        \n    def save(self, epoch, model_state, optim_state, name=None):\n        if epoch < self.config.skip_epoch:\n            return\n        mkdir(self.cpt_dir)\n        if name != None:\n            cpt_name=name\n        else: cpt_name=self.id+f'_e{epoch}.pt'\n        save_dict={'epoch': epoch, 'args':self.config, 'model_state_dict': model_state}\n        if self.config.save_optim:\n            save_dict.update({'optim_state_dict': optim_state})\n        torch.save(save_dict, os.path.join(self.cpt_dir,cpt_name))\n        print(f'model saved in {self.cpt_dir}/{cpt_name}')\n        \n    def load(self):\n        if self.config.resume_checkpoint:\n            if os.path.exists(self.config.resume_checkpoint):\n                _, cpt_name=os.path.split(self.config.resume_checkpoint)\n                if '.pt' in cpt_name:\n                    print(f'loading file at {self.config.resume_checkpoint}')\n                    cpt_dict = torch.load(self.config.resume_checkpoint, map_location=self.device)\n                else:\n                    print('please provide a file path with suffix of \".pt\"')\n                    return\n            else:\n                print(f'{self.config.resume_checkpoint} does not exists!')\n                return\n            \n            if cpt_dict:\n                self.start_epoch=cpt_dict['epoch']\n                self.model.load_state_dict(cpt_dict['model_state_dict'])\n                if self.config.load_optimizer:\n                    op_state=cpt_dict.get('optim_state_dict', -1)\n                    if op_state !=-1:\n                        self.last_epoch=self.start_epoch-1\n                        self.optimizer.load_state_dict(op_state)\n                    else:\n                        self.last_epoch=-1\n                print('Checkpoint has been loaded.')\n            else:\n                print('Checkpoint has no contents.')\n                return\n\nprint('sl.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:47.638467Z","iopub.execute_input":"2024-12-23T02:53:47.638724Z","iopub.status.idle":"2024-12-23T02:53:47.693445Z","shell.execute_reply.started":"2024-12-23T02:53:47.638700Z","shell.execute_reply":"2024-12-23T02:53:47.692406Z"}},"outputs":[{"name":"stdout","text":"sl.py\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import *\n\nimport wandb\nfrom torch.autograd import Variable\nimport os\nfrom tqdm import tqdm\nimport copy\nimport matplotlib.pyplot as plt\n\nimport logger\nfrom torch.utils.data import dataloader\n\n# from utils import utils\n# from utils import metrics\n# from loss import *\n\nclass MTSLTrainer(SLTrainer):\n    def __init__(self, model, optimizer, criterion, device, config):\n        print(\"Multi-Task Supervised trainer\")\n        self.ssl_name='MTSL'\n        self.config=config\n        self.model=model\n        self.optimizer=optimizer\n        self.device=device\n        self.criterion=criterion\n        self.estimator=[Estimator(config.metrics, 2, binary=True, input_logits=config.input_logits) for _ in range(config.num_classes)]\n        self.id=config.id\n\n        if self.config.dual and self.config.seg_vessel:\n            self.criterion_extra=nn.BCEWithLogitsLoss(pos_weight=torch.tensor(10).to(self.device))\n        \n        self.data_dir=os.path.join(config.checkpoint_dir,'Multi-Task Supervised trainer', self.id)\n        self.log_dir=os.path.join(self.data_dir,'log')\n        self.cpt_dir=os.path.join(self.data_dir,'checkpoint')\n        self.best_checkpoint=''\n        self.epoch       = 0\n        self.start_epoch = 0\n        self.last_epoch = -1\n        mkdir(self.data_dir)\n        mkdir(self.log_dir)\n        mkdir(self.cpt_dir)\n        \n    def trn_iteration(self, data_loader, scheduler):\n        self.model.train()\n        for e in self.estimator:\n            e.reset()\n        trn_loss = AverageMeter()\n        \n        lr=self.optimizer.state_dict()['param_groups'][0]['lr']\n        with tqdm(total=len(data_loader), desc=f\"Epoch {self.epoch}/{self.config.num_epochs} --- training lr:{lr}\", unit='batch') as pbar:\n            for step, (x, y) in enumerate(data_loader):\n                if self.config.dual:\n                    x = x.to(self.device)\n                    x = Variable(x)\n                    y = [_y.to(self.device) for _y in y]\n                    y_extra = Variable(y[1]).float()\n                    y = Variable(y[0]).float()\n                    if self.config.seg_vessel:\n                        y_pred = self.model(x.float())\n                        y_extra_pred = y_pred[1].squeeze()\n                        y_pred = y_pred[0]\n                    else:\n                        y_pred = self.model(x.float(), y_extra)\n                elif self.config.grid_size:\n                    x, y = [t.to(self.device) for t in (x,y)]\n                    x, y=Variable(x),Variable(y)\n                    x = x.float()\n                    x = x.transpose(0,1) # b, p1, p2, c, h, w\n                    y = y.transpose(0,1)\n                    \n                    y_pred_pieces = []\n                    for x_piece in x:\n                        y_pred_pieces.append(self.model(x_piece.float()))\n                    x_coord = self.config.resolution[0]//self.config.grid_size\n                    y_coord = self.config.resolution[1]//self.config.grid_size\n                    y_pred = torch.cat([\n                                        torch.cat(y_pred_pieces[i*x_coord:(i+1)*x_coord],dim=3) for i in range(y_coord)\n                                        ], dim=2)\n                    y_pieces=[]\n                    for y_piece in y:\n                        y_pieces.append(y_piece)\n                    # print(len(y_pieces))\n                    y = torch.cat([\n                                    torch.cat(y_pieces[i*x_coord:(i+1)*x_coord],dim=2) for i in range(y_coord)\n                                        ], dim=1)\n                else:\n                    x, y = [t.to(self.device) for t in (x,y)]\n                    x, y=Variable(x),Variable(y)\n\n                    # print(y.shape)\n                    # print(y[0].shape)\n                    # print(np.max(y.cpu().detach().numpy()))\n                    # plt.figure()\n                    # plt.imshow(y[0].cpu().detach().numpy())\n                    # plt.show()\n                    # plt.close()\n\n                    \n                    y_pred = self.model(x.float())\n\n                #--------lseg------------\n                if self.config.netframe=='lseg':\n                    features=y_pred[:-1]\n                    y_pred=y_pred[-1]\n                    targets=[]\n                    for i in range(1,self.config.num_classes+1):\n                        t=(y==i).float()\n                        targets.append(t)\n                        o=y_pred[:,i-1]\n                        self.estimator[i-1].update(o, t)\n                    targets=torch.stack(targets,dim=1)\n                    loss = self.criterion(y_pred, targets)\n                    for f in features:\n                        loss += self.criterion(f, targets)\n                else:\n                    #--------normal----------\n                    targets=[]\n                    for i in range(1,self.config.num_classes+1):\n                        t=(y==i).float()\n                        targets.append(t)\n                        o=y_pred[:,i-1]\n                        # print(f't:{t.shape}')\n                        # print(f'o:{o.shape}')\n                        self.estimator[i-1].update(o, t)\n                    targets=torch.stack(targets,dim=1)\n                    loss = self.criterion(y_pred, targets)\n                    if self.config.dual and self.config.seg_vessel:\n                        loss_extra = self.criterion_extra(y_extra_pred, y_extra)\n                        loss += self.config.dual_loss_factor*loss_extra\n\n                \n                \n\n                if self.config.accumulation_step:\n                    loss /= self.config.accumulation_step\n                    loss.backward()\n                    \n                    if ((step + 1) % self.config.accumulation_step == 0) or (step + 1 == len(data_loader)):\n                        trn_loss.update(loss.item()*self.config.accumulation_step)\n                        self.optimizer.step()\n                        self.optimizer.zero_grad()\n                else: \n                    trn_loss.update(loss.item())\n                    self.optimizer.zero_grad()\n                    loss.backward()\n                    self.optimizer.step()\n                \n                if not self.config.upd_by_ep and scheduler != None: scheduler.step()\n                pbar.update(1)\n        \n        info_dict=dict([('epoch', self.epoch),\n                   ('lr', lr),\n                   ('trn_loss', round(trn_loss.avg,4))])\n\n        for i,k in enumerate(self.config.leisions):\n            score=self.estimator[i].get_scores(4)\n            info_dict.update({'trn_'+_k+'_'+k:_v for _k,_v in score.items()})\n        \n        return info_dict\n        \n    def val_iteration(self, data_loader, val_loss):\n        self.model.eval()\n        \n        for e in self.estimator:\n            e.reset()\n        \n        with tqdm(total=len(data_loader), desc=f\"Epoch {self.epoch}/{self.config.num_epochs} --- validating\", unit='batch') as pbar:\n            with torch.no_grad():\n                for step, (x, y) in enumerate(data_loader):\n                    if self.config.dual:\n                        x = x.to(self.device)\n                        x = Variable(x)\n                        y = [_y.to(self.device) for _y in y]\n                        y_extra = Variable(y[1]).float()\n                        y = Variable(y[0]).float()\n                        if self.config.seg_vessel:\n                            y_pred = self.model(x.float())\n                            y_extra_pred = y_pred[1].squeeze()\n                            y_pred = y_pred[0]\n                        else:\n                            y_pred = self.model(x.float(), y_extra)\n                    elif self.config.grid_size:\n                        x, y = [t.to(self.device) for t in (x,y)]\n                        x, y=Variable(x),Variable(y)\n                        x = x.float()\n                        x = x.transpose(0,1) # b, p1, p2, c, h, w\n                        y = y.transpose(0,1)\n                        \n                        y_pred_pieces = []\n                        for x_piece in x:\n                            y_pred_pieces.append(self.model(x_piece.float()))\n                        \n                    else:\n                        x, y = [t.to(self.device) for t in (x,y)]\n                        x, y=Variable(x),Variable(y)\n                        y_pred = self.model(x.float())\n\n                    #--------lseg------------\n                    if self.config.netframe=='lseg':\n                        features=y_pred[:-1]\n                        y_pred=y_pred[-1]\n                        targets=[]\n                        for i in range(1,self.config.num_classes+1):\n                            t=(y==i).float()\n                            targets.append(t)\n                            o=y_pred[:,i-1]\n                            self.estimator[i-1].update(o, t)\n                        targets=torch.stack(targets,dim=1)\n                        loss = self.criterion(y_pred, targets)\n                        for f in features:\n                            loss += self.criterion(f, targets)\n                    elif self.config.grid_size:\n                        for y_pred,y_true in zip(y_pred_pieces, y):\n                            targets=[]\n                            for i in range(1,self.config.num_classes+1):\n                                t=(y_true==i).float()\n                                targets.append(t)\n                                o=y_pred[:,i-1]\n                                self.estimator[i-1].update(o, t)\n                            targets=torch.stack(targets,dim=1)\n                            loss = self.criterion(y_pred, targets) #这里的loss有错，应该是所有piece的loss加起来\n                    else:\n                        #--------normal----------\n                        targets=[]\n                        for i in range(1,self.config.num_classes+1):\n                            t=(y==i).float()\n                            targets.append(t)\n                            o=y_pred[:,i-1]\n                            self.estimator[i-1].update(o, t)\n                        targets=torch.stack(targets,dim=1)\n                        loss = self.criterion(y_pred, targets)\n                        if self.config.dual and self.config.seg_vessel:\n                            loss_extra = self.criterion_extra(y_extra_pred, y_extra)\n                            loss += self.config.dual_loss_factor*loss_extra\n\n                    if self.config.accumulation_step:\n                        loss /= self.config.accumulation_step\n                        if ((step + 1) % self.config.accumulation_step == 0) or (step + 1 == len(data_loader)):\n                            val_loss.update(loss.item()*self.config.accumulation_step)\n                    else: \n                        val_loss.update(loss.item())\n                    pbar.update(1) \n                \n        info_dict=dict([('epoch', self.epoch),\n                   ('val_loss', round(val_loss.avg,4))])\n\n        mean_metrics={}\n        for n in set(self.estimator[0].need_named_metrics)&set(self.estimator[0].get_scores(4).keys()):\n            mean_metrics[n]=[]\n\n        print('start calculating scores')\n        for i,k in enumerate(self.config.leisions):\n            timer1=Timer()\n            timer1.start_timer(time.time())\n            score=self.estimator[i].get_scores(4)\n            timer1.compute(time.time(),name='calculate score')\n    \n            for n in set(self.estimator[i].need_named_metrics)&set(score.keys()):\n                mean_metrics[n].append(score[n])\n        \n            info_dict.update({'val_'+_k+'_'+k:_v for _k,_v in score.items()})\n            \n        for k,v in mean_metrics.items():\n            info_dict.update({'val_'+k:np.round(np.mean(v), 4)})\n            \n        return info_dict\n        \n\nprint('bsl.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:47.694863Z","iopub.execute_input":"2024-12-23T02:53:47.695474Z","iopub.status.idle":"2024-12-23T02:53:47.732500Z","shell.execute_reply.started":"2024-12-23T02:53:47.695435Z","shell.execute_reply":"2024-12-23T02:53:47.731498Z"}},"outputs":[{"name":"stdout","text":"bsl.py\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import *\n\nimport wandb\nfrom torch.autograd import Variable\nimport os\nfrom tqdm import tqdm\nimport copy\nimport matplotlib.pyplot as plt\n\nimport logger\nfrom torch.utils.data import dataloader\nfrom itertools import cycle\n\n# from utils import utils\n# from utils import metrics\n# from loss import *\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        channels = [32, 64, 128, 256]\n        channels = [in_channels] + channels\n        self.convs = nn.ModuleList([\n                self.basic_conv(channels[i], channels[i+1])\n                for i in range(len(channels)-1)\n            ])\n        self.fc = nn.Linear(channels[-1], 1)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def basic_conv(self, in_channels, out_channels):\n        return nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, 3, 1),\n                    nn.Conv2d(out_channels, out_channels, 3, 1),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True),\n                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n                )\n        \n    def forward(self, x):\n        for m in self.convs:\n            x = m(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return F.sigmoid(x)\n\nclass GANTrainer(SLTrainer):\n    def __init__(self, seg_model, grading_model, optimizers, criterions, device, config):\n        print(\"GAN trainer\")\n        self.ssl_name='GAN'\n        self.config=config\n        self.seg_model=seg_model  \n        self.grading_model=grading_model\n        self.optimizerS=optimizers[0]\n        self.optimizerG=optimizers[1]\n        self.device=device\n        self.criterionS=criterions[0]\n        self.criterionG=criterions[1]\n        self.estimatorS=[Estimator(config.metrics, 2, binary=True, input_logits=config.input_logits) for _ in range(config.num_classes)]\n        self.estimatorG=Estimator(config.gan_grading_metrics, config.gan_grading_classes, input_logits=config.input_logits)\n        self.id=config.id\n        \n        \n        self.discriminator=Discriminator(config.num_classes).to(self.device)\n        self.optimizerD=torch.optim.SGD(self.discriminator.parameters(),lr=1e-3, momentum=0.9, weight_decay=0.0005)\n        self.criterionD=torch.nn.BCELoss()\n        \n        self.data_dir=os.path.join(config.checkpoint_dir,'GAN trainer', self.id)\n        self.log_dir=os.path.join(self.data_dir,'log')\n        self.cpt_dir=os.path.join(self.data_dir,'checkpoint')\n        self.best_checkpoint=''\n        self.grading_best_checkpoint=''\n        self.epoch       = 0\n        self.start_epoch = 0\n        self.last_epoch = -1\n        mkdir(self.data_dir)\n        mkdir(self.log_dir)\n        mkdir(self.cpt_dir)\n        \n    def trn_iteration(self, seg_loader, grading_loader, scheduler):\n        self.seg_model.train()\n        self.grading_model.train()\n        self.discriminator.train()\n        \n        for e in self.estimatorS:\n            e.reset()\n        self.estimatorG.reset()\n        \n        trn_seg_loss = AverageMeter()\n        trn_grading_loss = AverageMeter()\n        trn_dis_loss = AverageMeter()\n        \n        seg_loader=iter(cycle(seg_loader))\n        \n        lr=self.optimizerS.state_dict()['param_groups'][0]['lr']\n        grading_lr=self.optimizerG.state_dict()['param_groups'][0]['lr']\n        with tqdm(total=len(grading_loader), desc=f\"Epoch {self.epoch}/{self.config.num_epochs} --- training seg_lr:{lr} grading_lr:{grading_lr}\", unit='batch') as pbar:\n            for step, (x, y) in enumerate(grading_loader):\n                \n                x, y = [t.to(self.device) for t in (x,y)]\n                x, y=Variable(x),Variable(y)\n                \n                s_x, s_y = next(seg_loader)\n                s_x, s_y = [t.to(self.device) for t in (s_x,s_y)]\n                s_x, s_y=Variable(s_x),Variable(s_y)\n                s_y = self.separate_mask(s_y)\n                \n                # train seg and grading model\n                pseudo_masks = self.seg_model(x.float())\n                pseudo_masks_cpu = pseudo_masks.detach().cpu().transpose(0,1).contiguous()\n                pseudo_maps = []\n                for m in pseudo_masks_cpu:\n                    pseudo_maps.append(m)\n                grading_pred, refined_pseudo_masks = self.grading_model(x.float(), pseudo_maps)\n                s_y_pred = self.seg_model(s_x.float())\n\n                grading_loss = self.criterionG(grading_pred, y)\n                trn_grading_loss.update(grading_loss.item())\n                self.estimatorG.update(grading_pred, y)\n\n                s_y = torch.cat((s_y, refined_pseudo_masks), dim=0)\n                s_y_pred = torch.cat((s_y_pred, pseudo_masks), dim=0)\n                \n                # update seg estimator\n                for i in range(0,self.config.num_classes):\n                    t=s_y[:,i].float()\n                    o=s_y_pred[:,i]\n                    # print(f't:{t.shape}')\n                    # print(f'o:{o.shape}')\n                    self.estimatorS[i-1].update(o, t)\n\n                valid = Variable(torch.FloatTensor(s_y_pred.size(0), 1).fill_(1.0), requires_grad=False).to(self.device)\n                fake = Variable(torch.FloatTensor(s_y_pred.size(0), 1).fill_(0.0), requires_grad=False).to(self.device)\n                \n                seg_loss = self.criterionS(s_y_pred, s_y) - torch.mean(self.discriminator(s_y_pred))\n                trn_seg_loss.update(seg_loss.item())\n                \n                #train discriminator\n                dis_loss = -torch.mean(self.discriminator(s_y)) + torch.mean(self.discriminator(s_y_pred))\n                trn_dis_loss.update(dis_loss.item())\n\n                loss = grading_loss + seg_loss + dis_loss\n                self.optimizerD.zero_grad()\n                self.optimizerG.zero_grad()\n                self.optimizerS.zero_grad()\n                loss.backward()\n                self.optimizerS.step()\n                self.optimizerG.step()\n                self.optimizerD.step()\n                \n                \n                # if self.config.accumulation_step:\n                #     loss /= self.config.accumulation_step\n                #     loss.backward()\n                #\n                #     if ((step + 1) % self.config.accumulation_step == 0) or (step + 1 == len(data_loader)):\n                #         trn_loss.update(loss.item()*self.config.accumulation_step)\n                #         self.optimizer.step()\n                #         self.optimizer.zero_grad()\n                # else: \n                #     trn_loss.update(loss.item())\n                #     self.optimizer.zero_grad()\n                #     loss.backward()\n                #     self.optimizer.step()\n                \n                if not self.config.upd_by_ep and scheduler != None: \n                    for s in scheduler:\n                        s.step()\n                pbar.update(1)\n        \n        info_dict=dict([('epoch', self.epoch),\n                   ('lr', lr),\n                   ('trn_seg_loss', round(trn_seg_loss.avg,4)),\n                   ('trn_grading_loss', round(trn_grading_loss.avg,4)),\n                   ('trn_dis_loss', round(trn_dis_loss.avg,4))])\n\n        for i,k in enumerate(self.config.leisions):\n            score=self.estimatorS[i].get_scores(4)\n            info_dict.update({'trn_seg_'+_k+'_'+k:_v for _k,_v in score.items()})\n        \n        grading_score = self.estimatorG.get_scores(4)\n        info_dict.update({'trn_grading_'+k:v for k,v in grading_score.items()})\n        \n        return info_dict\n        \n    def val_iteration(self, seg_loader, grading_loader, val_loss):\n        self.seg_model.eval()\n        self.grading_model.eval()\n        \n        for e in self.estimatorS:\n            e.reset()\n        self.estimatorG.reset()\n        \n        with tqdm(total=len(seg_loader), desc=f\"Epoch {self.epoch}/{self.config.num_epochs} --- Seg validating\", unit='batch') as pbar:\n            with torch.no_grad():\n                for step, (x, y) in enumerate(seg_loader):\n                    x, y = [t.to(self.device) for t in (x,y)]\n                    x, y=Variable(x),Variable(y)\n                    y_pred = self.seg_model(x.float())\n                        \n                    #--------normal----------\n                    targets=[]\n                    for i in range(1,self.config.num_classes+1):\n                        t=(y==i).float()\n                        targets.append(t)\n                        o=y_pred[:,i-1]\n                        self.estimatorS[i-1].update(o, t)\n                    targets=torch.stack(targets,dim=1)\n                    loss = self.criterionS(y_pred, targets)\n                    \n                    # if self.config.accumulation_step:\n                    #     loss /= self.config.accumulation_step\n                    #     if ((step + 1) % self.config.accumulation_step == 0) or (step + 1 == len(data_loader)):\n                    #         val_loss.update(loss.item()*self.config.accumulation_step)\n                    # else: \n                    val_loss.update(loss.item())\n                    pbar.update(1) \n                \n        info_dict=dict([('epoch', self.epoch),\n                   ('val_seg_loss', round(val_loss.avg,4))])\n\n        mean_metrics={}\n        for n in set(self.estimatorS[0].need_named_metrics)&set(self.estimatorS[0].get_scores(4).keys()):\n            mean_metrics[n]=[]\n        \n        for i,k in enumerate(self.config.leisions):\n            score=self.estimatorS[i].get_scores(4)\n    \n            for n in set(self.estimatorS[i].need_named_metrics)&set(score.keys()):\n                mean_metrics[n].append(score[n])\n        \n            info_dict.update({'val_seg_'+_k+'_'+k:_v for _k,_v in score.items()})\n            \n        for k,v in mean_metrics.items():\n            info_dict.update({'val_seg_'+k:np.round(np.mean(v), 4)})\n            \n        val_loss.reset()\n        with tqdm(total=len(grading_loader), desc=f\"Epoch {self.epoch}/{self.config.num_epochs} --- Grading validating\", unit='batch') as pbar:\n            with torch.no_grad():\n                for step, (x, y) in enumerate(grading_loader):\n                    x, y = x.to(self.device), y.to(self.device)\n                    x, y=Variable(x),Variable(y)\n    \n                    y_pred = self.grading_model(x.float())\n    \n                    loss=self.criterionG(y_pred, y)\n                    val_loss.update(loss.item())\n    \n                    self.estimatorG.update(y_pred, y)\n                    pbar.update(1)\n                \n        score=self.estimatorG.get_scores(4)\n        \n        info_dict.update(dict([('val_grading_loss', round(val_loss.avg,4))]))\n        info_dict.update({'val_grading_'+k:v for k,v in score.items()})\n            \n        return info_dict\n\n    def loop(self, num_epochs, trn_loader, val_loader):\n        logger.configure(dir=self.log_dir, log_suffix=self.id)\n        \n        if self.config.use_wandb:\n            wandb.init(\n              project=self.config.experiment_name,\n              config=vars(self.config)\n            )\n            wandb.run.name = self.id\n            wandb.run.save()\n            \n        self.load()\n            \n        schedulerS=get_scheduler(self.config, self.optimizerS, last_epoch = self.last_epoch)\n        schedulerG=lr_scheduler.CosineAnnealingLR(self.optimizerG,T_max=num_epochs,eta_min=self.config.min_lr)\n        schedulerD=lr_scheduler.CosineAnnealingLR(self.optimizerD,T_max=num_epochs,eta_min=self.config.min_lr)\n        scheduler=[schedulerS,schedulerG,schedulerD]\n        \n        best_perf = [sys.float_info.min]*2\n        grading_best_perf = [sys.float_info.min]*2\n        val_loss = AverageMeter()\n        last_info=None\n\n        timer = Timer()\n        \n        for epoch in range(self.start_epoch+1, num_epochs+1):\n            self.epoch=epoch\n            timer.start_timer(time.time())\n            info_dict=self.trn_iteration(trn_loader[0], trn_loader[1], scheduler)\n            timer.compute(time.time())\n            if self.config.upd_by_ep and scheduler != None: \n                for s in scheduler:\n                    s.step()\n                    \n            if epoch % self.config.save_interval == 0:\n                self.save(epoch, self.seg_model.state_dict(), self.optimizerS.state_dict())\n                self.save(epoch, self.grading_model.state_dict(), self.optimizerG.state_dict())\n            \n            if epoch % self.config.validation_interval == 0 or epoch == 1:\n                timer.start_timer(time.time())\n                info_dict.update(self.val_iteration(val_loader[0], val_loader[1], val_loss))\n                timer.compute(time.time())\n                \n                if info_dict['val_seg_'+self.config.best_metric] > best_perf[1]:\n                    best_perf[0]=epoch\n                    best_perf[1]=info_dict['val_seg_'+self.config.best_metric]\n                    self.best_checkpoint = [os.path.join(self.cpt_dir, self.id+f'_e{epoch}.pt')]\n                    if epoch != num_epochs:\n                        self.best_checkpoint.append(os.path.join(self.cpt_dir, self.id+f'_e{num_epochs}.pt'))\n                    if epoch % self.config.save_interval != 0 and info_dict['val_seg_'+self.config.best_metric] > self.config.good_value:\n                        self.save(epoch, self.seg_model.state_dict(), self.optimizerS.state_dict())\n                        \n                if info_dict['val_grading_'+self.config.gan_grading_best_metric] > grading_best_perf[1]:\n                    grading_best_perf[0]=epoch\n                    grading_best_perf[1]=info_dict['val_grading_'+self.config.gan_grading_best_metric]\n                    self.grading_best_checkpoint = [os.path.join(self.cpt_dir, 'gan_grading_rs50_myffs'+f'_e{epoch}.pt')]\n                    if epoch != num_epochs:\n                        self.grading_best_checkpoint.append(os.path.join(self.cpt_dir, self.id+f'_e{num_epochs}.pt'))\n                    if epoch % self.config.save_interval != 0 and info_dict['val_grading_'+self.config.gan_grading_best_metric] > self.config.gan_grading_good_value:\n                        self.save(epoch, self.grading_model.state_dict(), self.optimizerG.state_dict())\n                    \n            if self.config.use_wandb:\n                    wandb.log(info_dict)\n\n            if self.config.show_log:\n                logger.logkvs(info_dict)\n                logger.dumpkvs()\n                str_output = 'epoch & {} \\n'.format(self.config.best_metric)\n                str_output += f'{best_perf[0]} & {best_perf[1]:.4f} \\n'\n                grading_str_output = 'epoch & {} \\n'.format(self.config.gan_grading_best_metric)\n                grading_str_output += f'{grading_best_perf[0]} & {grading_best_perf[1]:.4f} \\n'\n                logger.log(str_output)\n                logger.log(grading_str_output)\n            \n            if val_loss.is_overfitting() and self.config.early_stop:\n                print('Detected overfitting signs, stop training.')\n                break\n            \n            val_loss.reset()\n                \n        if self.config.use_wandb:\n            wandb.finish()\n            \n        return last_info\n        \n    def separate_mask(self, masks):\n        targets=[]\n        for i in range(1,self.config.num_classes+1):\n            t=(masks==i).float()\n            targets.append(t)\n        return torch.stack(targets,dim=1)\n\n    def load(self):\n        if self.config.resume_checkpoint:\n            if os.path.exists(self.config.resume_checkpoint):\n                _, cpt_name=os.path.split(self.config.resume_checkpoint)\n                if '.pt' in cpt_name:\n                    print(f'loading file at {self.config.resume_checkpoint}')\n                    cpt_dict = torch.load(self.config.resume_checkpoint, map_location=self.device)\n                else:\n                    print('please provide a file path with suffix of \".pt\"')\n                    return\n            else:\n                print(f'{self.config.resume_checkpoint} does not exists!')\n                return\n            \n            if cpt_dict:\n                # self.start_epoch=cpt_dict['epoch']\n                self.seg_model.load_state_dict(cpt_dict['model_state_dict'])\n                if self.config.load_optimizer:\n                    op_state=cpt_dict.get('optim_state_dict', -1)\n                    if op_state !=-1:\n                        self.last_epoch=cpt_dict['epoch']-1\n                        self.optimizerS.load_state_dict(op_state)\n                    else:\n                        self.last_epoch=-1\n                print('Segmentation Model Checkpoint has been loaded.')\n            else:\n                print('Segmentation Model Checkpoint has no contents.')\n                return\n\nprint('gansl.py')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T02:53:47.733979Z","iopub.execute_input":"2024-12-23T02:53:47.734262Z","iopub.status.idle":"2024-12-23T02:53:47.779314Z","shell.execute_reply.started":"2024-12-23T02:53:47.734233Z","shell.execute_reply":"2024-12-23T02:53:47.778527Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"gansl.py\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import albumentations.augmentations.geometric as G\nimport albumentations.augmentations.transforms as A\nimport albumentations.augmentations.crops as C\nimport torchvision.transforms as T\nimport torch\nimport numpy as np\nimport random\n\n\nclass OneOf:\n    def __init__(self, transforms, choose_one_of=None):\n        self.transforms = transforms\n        self.choose_one_of = choose_one_of\n        transforms_ps = [t.p for t in self.transforms]\n        s = sum(transforms_ps)\n        self.transforms_ps = [t / s for t in transforms_ps]\n\n    def __call__(self, sample):\n        if self.choose_one_of:\n            t = self.transforms[self.choose_one_of]\n        else:\n            t = random.choices(self.transforms, weights=self.transforms_ps)[0]\n        return t(sample)\n\nclass ToNumpy(object):\n    \n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n        image=image.numpy()\n        masks=[m.numpy() for m in masks]\n        image=np.transpose(image,(1,2,0))\n        new_sample = {'image': image, 'masks': masks}\n        return new_sample\n    \nclass Resize(object):\n    def __init__(self, size, p=1):\n        self.size=size\n        self.p=p\n    \n    def __call__(self, sample):\n        if self.size:\n            image, masks = sample['image'], sample['masks']\n            augmented=G.Resize(height=self.size[0], width=self.size[1])(image=image,masks=masks)\n            new_sample = {'image': augmented['image'], 'masks': augmented['masks']}\n            return new_sample\n        return sample\n    \nclass RandomCrop(object):\n    def __init__(self, size, p=1):\n        self.size=size\n        self.p=p\n    \n    def __call__(self, sample):\n        if self.size:\n            image, masks = sample['image'], sample['masks']\n            augmented=C.RandomCrop(height=self.size[0], width=self.size[1])(image=image,masks=masks)\n            new_sample = {'image': augmented['image'], 'masks': augmented['masks']}\n            return new_sample\n        return sample\n\nclass RandomRotate90(object):\n    '''\n        Randomly rotates an image\n    '''\n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n        augmented=G.RandomRotate90()(image=image,masks=masks)\n        new_sample = {'image': augmented['image'], 'masks': augmented['masks']}\n        return new_sample\n\n\nclass HorizontalFlip(object):\n\n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n        augmented=G.HorizontalFlip()(image=image,masks=masks)\n        new_sample = {'image': augmented['image'], 'masks': augmented['masks']}\n        return new_sample\n\nclass VerticalFlip(object):\n\n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n        augmented=G.VerticalFlip()(image=image,masks=masks)\n        new_sample = {'image': augmented['image'], 'masks': augmented['masks']}\n        return new_sample\n    \nclass ColorJitter(object):\n    \n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n        augmented=A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0)(image=image)\n        new_sample = {'image': augmented['image'], 'masks': masks}\n        return new_sample\n\nclass CropTo4(object):\n    def __init__(self, size):\n        self.size=size\n\n    def __call__(self, sample):\n        if self.size:\n            image, masks = sample['image'], sample['masks']\n            augmented_image=self.crop_to_4(image)\n            augmented_masks=[]\n            for mask in masks:\n                augmented_masks.append(self.crop_to_4(mask))\n            new_sample = {'image': augmented_image, 'masks': augmented_masks}\n            return new_sample\n        else:\n            return sample\n        \n    def crop_to_4(self, img):\n        y_mid = img.shape[0]//2\n        y_end = img.shape[0]\n        x_mid = img.shape[1]//2\n        x_end = img.shape[1]\n        img_lt = img[0:y_mid, 0:x_mid]\n        img_lb = img[y_mid:y_end, 0:x_mid]\n        img_rt = img[0:y_mid, x_mid:x_end]\n        img_rb = img[y_mid:y_end, x_mid:x_end]\n        \n        img_cat = np.stack([img_lt, img_rt, img_lb, img_rb], axis=0)\n        return img_cat\n        \nclass Grid(object):\n    def __init__(self, grid_size, p=1):\n        self.grid_size=grid_size\n        self.p=p\n        \n    def __call__(self, sample):\n        if self.grid_size:\n            image, masks = sample['image'], sample['masks']\n            augmented_image=self.make_pieces(image)\n            augmented_masks=[]\n            for mask in masks:\n                augmented_masks.append(self.make_pieces(mask))\n            new_sample = {'image': augmented_image, 'masks': augmented_masks}\n            return new_sample\n        else:\n            return sample\n        \n    def make_pieces(self, img):\n        y_coord = np.arange(0, img.shape[0], self.grid_size)\n        x_coord = np.arange(0, img.shape[1], self.grid_size)\n    \n        img_pieces=[]\n        for y in y_coord:\n            for x in x_coord:\n                img_pieces.append(img[y:y+self.grid_size, x:x+self.grid_size])\n                \n        img_cat = np.stack(img_pieces, axis=0)\n        return img_cat\n        \n    \nclass ToTensor(object):\n    \n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n        if len(image.shape) == 4:\n            image=np.transpose(image, (0,3,1,2))\n        else:\n            image=np.transpose(image,(2,0,1))\n        image=torch.from_numpy(image)\n        masks=[torch.from_numpy(np.ascontiguousarray(m)) for m in masks]\n        new_sample = {'image': image, 'masks': masks}\n        return new_sample\n    \nclass Normalize(object):\n    def __init__(self):\n        self.channel_stats=dict(mean = [0.425753653049469, 0.29737451672554016, 0.21293757855892181],\n                         std = [0.27670302987098694, 0.20240527391433716, 0.1686241775751114])\n    \n    def __call__(self, sample):\n        image, masks = sample['image'], sample['masks']\n        augmented=T.Normalize(**self.channel_stats)(image)\n        new_sample = {'image': augmented, 'masks': masks}\n        return new_sample\n\nprint('transforms.py')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-12-23T02:53:47.782642Z","iopub.execute_input":"2024-12-23T02:53:47.782896Z","iopub.status.idle":"2024-12-23T02:53:48.287811Z","shell.execute_reply.started":"2024-12-23T02:53:47.782873Z","shell.execute_reply":"2024-12-23T02:53:48.286921Z"}},"outputs":[{"name":"stdout","text":"transforms.py\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport random\n\nimport wandb\nfrom torch.utils.data.dataloader import DataLoader\nimport traceback\n\nimport segmentation_models_pytorch as smp\n\nimport logger\n\n# from utils import *\n# from metrics import *\n# from loss import get_criterion, get_scheduler\n# from dataset import IDRiDDataset\n# from transform import *\n# from model.shufflenetv2_sa import *\n# from model.sa_resnet import *\n# from model.model import *\n# from sl import SLTrainer\n# from model.deeplabv3_plus import DeepLab\n\ndef train(args):\n    if args.load_args:\n        args=load_args(args)\n    seed_torch(args.seed)\n    \n    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    \n    #数据集\n    if args.dataset_name=='idrid':\n        args.dataset_path='/kaggle/input/idrid-dataset/A. Segmentation'\n        trn_set=IDRiDDataset(args.dataset_path, dataset_type='train', resolution=args.resolution, leisions=args.leisions, args=args)\n        val_set=IDRiDDataset(args.dataset_path, dataset_type='val', resolution=args.resolution, leisions=args.leisions, args=args)\n    elif args.dataset_name=='vessel':\n        args.dataset_path='/kaggle/input/retinal-vessel-segmentation-combined/Retina Vessel Segmentation'\n        trn_set=VesselDataset(args.dataset_path, dataset_type='train', resolution=args.resolution, args=args)\n        val_set=VesselDataset(args.dataset_path, dataset_type='val', resolution=args.resolution, args=args)\n    elif args.dataset_name=='ddr':\n        if args.preprocess:\n            args.dataset_path='/kaggle/input/cropped-ddr-seg'\n        else:\n            args.dataset_path='/kaggle/input/ddr-dataset/lesion_segmentation'\n        trn_set=DDRDataset(args.dataset_path, dataset_type='train', resolution=args.resolution, args=args)\n        val_set=DDRDataset(args.dataset_path, dataset_type='test', resolution=args.resolution, args=args)\n    \n    if args.use_T:\n        trn_set.transform=T.Compose([\n                    ToNumpy(),\n                    OneOf([\n                        Resize(args.crop_size),\n                        RandomCrop(args.crop_size),\n                        ],\n                          choose_one_of=args.choose_one_of),\n                    HorizontalFlip(),\n                    VerticalFlip(),\n                    RandomRotate90(),\n                    ColorJitter(),\n                    Grid(args.grid_size),\n                    ToTensor(),\n                    Normalize()\n                ])\n\n        val_set.transform=T.Compose([\n                    ToNumpy(),\n                    Grid(args.grid_size),\n                    ToTensor(),\n                    Normalize()\n        ])\n    \n    \n    print(f'transform:{trn_set.transform}')\n\n    trn_loader=DataLoader(trn_set, batch_size=args.batch_size, shuffle=True, num_workers=4)\n    val_loader=DataLoader(val_set, batch_size=args.batch_size, shuffle=True, num_workers=4)\n\n    #损失函数\n    criterion = get_criterion(args, device=device)\n\n    #模型\n    kwargs={'args':args}\n    kwargs['num_classes']=args.num_classes\n    kwargs['backbone']=args.backbone\n    kwargs['pretrained']=args.pretrained\n    kwargs['downsample_factor']=args.downsample_factor\n    kwargs['module_list']=args.module_list\n    kwargs['resolution']=args.crop_size[0] if args.crop_size else args.resolution[0]\n    kwargs['upsample_mode']=args.upsample_mode\n    kwargs['dual']=args.dual\n    if args.netframe=='deeplabv3':\n        model=DeepLab(**kwargs)\n    elif args.netframe=='unet':\n        if args.backbone=='ds':\n            model=UNetDenseNet(**kwargs)\n        elif args.backbone[:2]=='rs':\n            model=UNetResNet(**kwargs)\n        elif args.backbone=='new':\n            model=UNetWithResnet50Encoder(n_classes=args.num_classes)\n    elif args.netframe=='unetpp':\n        model=UnetPP(**kwargs)\n    elif args.netframe=='segformer':\n        model=Segformer(num_classes=args.num_classes)\n    elif args.netframe=='lseg':\n        model=ResNetLseg(**kwargs)\n    elif args.netframe[:3]=='smp':\n        decoder=getattr(smp, args.netframe[4:])\n        model = decoder(\n            encoder_name=args.backbone,\n            classes=args.num_classes,\n            encoder_weights='imagenet' if args.pretrained else None\n        )\n    elif args.netframe=='dwunet':\n        model=DWUnet(dwconv='wtconv',**kwargs)\n    elif args.netframe=='hiformer':\n        model=HiFormer(img_size=args.crop_size[0] if args.crop_size else args.resolution[0],\n                       n_classes=args.num_classes)\n    elif args.netframe=='transunet':\n        model=VisionTransformer(img_size=kwargs['resolution'], \n                                num_classes=args.num_classes, \n                                upsample_mode=args.upsample_mode, \n                                transformer_block=args.transformer_block, \n                                net_type=args.net_type, \n                                decoder=args.transunet_decoder)\n    # model=model(**kwargs)\n    model.to(device)\n    \n#     print({name:m for name,m in model.named_children()})\n    \n    \n    #优化器\n    if args.optim == 'Adam':\n        optimizer=torch.optim.Adam(model.parameters(),lr=args.lr, betas=args.momentum, weight_decay=args.weight_decay)\n    elif args.optim == 'SGD':\n        optimizer=torch.optim.SGD(model.parameters(),lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n    elif args.optim == 'AdamW':\n        optimizer=torch.optim.AdamW(model.parameters(),lr=args.lr, betas=args.momentum, weight_decay=args.weight_decay)\n    \n    \n    # for k,v in args.get_dict().items():\n    #     print(f'{k} : {v}')\n    print(args.get_dict())\n    \n    #半监督训练\n    if args.ssl=='SL':\n        trainer = SLTrainer(model, optimizer, criterion, device, args)\n    elif args.ssl=='MTSL':\n        trainer = MTSLTrainer(model, optimizer, criterion, device, args)\n    if args.ssl=='GANSL':\n        if args.gan_grading_dataset=='idrid':\n            dataset_path='/kaggle/input/idrid-dataset/B. Disease Grading'\n            grading_trn_set=IDataset(dataset_path, dataset_type='train', resolution=args.resolution, args=args)\n            grading_val_set=IDataset(dataset_path, dataset_type='val', resolution=args.resolution, args=args)\n        \n        if args.use_T:\n            #使用IMAGENET数据集的均值与标准差进行归一化，有助于网络训练\n            channel_stats = dict(mean = [0.425753653049469, 0.29737451672554016, 0.21293757855892181],\n                             std = [0.27670302987098694, 0.20240527391433716, 0.1686241775751114])\n            grading_trn_set.transform=T.Compose([\n    #                     transforms.RandomApply(\n    #                         [transforms.RandomCrop(512)],\n    #                         p=0.5\n    #                     ),\n                        T.RandomHorizontalFlip(p=0.5),\n                        T.RandomVerticalFlip(p=0.5),\n                        T.RandomApply(\n                            [T.ColorJitter(\n                                brightness=0.2, contrast=0.2,\n                                saturation=0, hue=0\n                            )],\n                            p=0.5\n                        ),\n                        T.RandomApply(\n                            [T.RandomRotation(\n                                degrees=[-180,180]\n                            )],\n                            p=0.5\n                        ),\n                        T.RandomApply(\n                            [T.RandomAffine(\n                                degrees=0,\n                                translate=[0.2,0.2]\n                            )],\n                            p=0.5\n                        ),\n                        T.ToTensor(),\n                        T.Normalize(**channel_stats)\n                    ])\n    \n            grading_val_set.transform=T.Compose([\n                        T.ToTensor(),\n                        T.Normalize(**channel_stats)\n            ])\n            \n            grading_trn_loader=DataLoader(grading_trn_set, batch_size=args.batch_size, shuffle=True, num_workers=4)\n            grading_val_loader=DataLoader(grading_val_set, batch_size=args.batch_size, shuffle=False, num_workers=4)\n            grading_model = resnet50(pretrained=False, num_classes=args.gan_grading_classes, module_list=args.module_list, weights=args.backbone_weights)\n            grading_model.to(device)\n            grading_criterion=torch.nn.CrossEntropyLoss()\n            optimizerG=torch.optim.SGD(grading_model.parameters(),lr=1e-3, momentum=0.9, weight_decay=0.0005)\n            \n            trainer = GANTrainer(model, grading_model, [optimizer, optimizerG], [criterion, grading_criterion], device, args)\n\n            trn_loader = [trn_loader, grading_trn_loader]\n            val_loader = [val_loader, grading_val_loader]\n            \n            \n    if args.evaluate:\n        return trainer.evaluate(val_loader)\n    if args.inference:\n        trainer.inference(trn_loader, type='trn')\n        trainer.inference(val_loader, type='val')\n        return\n        \n    return trainer.loop(args.num_epochs, trn_loader, val_loader)\n    \ndef main():\n    args = create_argparser()\n    try:\n        if args.ablation:\n            for selection in args.selection:\n                if isinstance(args.ablation, list):\n                    assert isinstance(selection, list), 'selection element not list!'\n                    args=update_args(args, {a:s for a,s in zip(args.ablation,selection)})\n                else:\n                    args=update_args(args, {args.ablation:selection})\n                train(args)\n        else:\n            train(args)\n    except:\n        traceback.print_exc()\n    finally:\n        if args.use_wandb: wandb.finish()\n            \ndef load_args(args):\n    if args.resume_checkpoint:\n        if os.path.exists(args.resume_checkpoint):\n            _, cpt_name=os.path.split(args.resume_checkpoint)\n            if '.pt' in cpt_name:\n                print(f'loading args at {args.resume_checkpoint}')\n                cpt_dict = torch.load(args.resume_checkpoint, map_location='cpu')\n            else:\n                return args\n        else:\n            return args\n\n        if cpt_dict:\n            _args = cpt_dict.get('args',-1)\n            if _args != -1:\n                _args.evaluate=args.evaluate\n                _args.inference=args.inference\n                _args.inference_root_dir=args.inference_root_dir\n                if args.inference:\n                    _args.inference_suffix=args.inference_suffix\n                    _args.dataset_name=args.dataset_name\n                    _args.resolution=args.resolution\n                    _args.preprocess=args.preprocess\n                    _args.use_T=args.use_T\n                _args.num_epochs=args.num_epochs\n                _args.skip_epoch=args.skip_epoch\n                _args.save_interval=args.save_interval\n                _args.validation_interval=args.validation_interval\n                _args.good_value=args.good_value\n                _args.resume_checkpoint=args.resume_checkpoint\n                _args.use_wandb=args.use_wandb\n                _args.load_optimizer=args.load_optimizer\n                _args.account=args.account\n                _args.a_version=args.a_version\n                # _args.crop_size=args.crop_size\n                _args.choose_one_of=args.choose_one_of\n                _args.ablation=args.ablation\n                _args.selection=args.selection\n                args=adjust_args(_args)\n                print('args has been loaded.')\n            return args\n    return args\n            \n            \ndef update_args(args, kwargs):\n    args.update(kwargs)\n    args=adjust_args(args)\n    return args\n    \ndef adjust_args(args):\n    if not isinstance(args.resolution, tuple):\n        args.resolution=(args.resolution, args.resolution)\n    if args.crop_size:\n        if not isinstance(args.crop_size, tuple):\n            args.crop_size=(args.crop_size, args.crop_size)\n    \n    if args.ssl == 'MTSL' or args.ssl == 'GANSL':\n        args.num_classes=len(args.leisions)\n\n    a=''\n    if args.module_list!=None:\n        a=list(args.module_list.values())[-1]+'_'\n\n    # if args.ablation:\n    #     val = [getattr(args, args.ablation) for a in args.ablation]\n    #     val_names = ''\n    #     if isinstance(val, dict):\n    #         for i,(k,v) in enumerate(val.items()):\n    #             val_names+=f'{k}_{v}_'\n    #     else:\n    #         val_names+=f'{val}_'\n    #     args.id=f'{args.ablation}_{val_names}BASE_{args.base_experiment}_{args.account}_{args.a_version}'\n    # else:\n    args.id=f'{args.dataset_name}_{args.netframe}_{args.backbone}_{a}{args.ssl}_bs{args.batch_size}_lr{str(args.lr).split(\".\")[-1]}_{args.loss_method}_{args.optim}_{args.account}_{args.a_version}'\n    \n    return args\n\ndef create_argparser():\n    # 运行前改\n    defaults = dict( \n        seed=random.randint(1,10000),\n        account=ACCOUNT,\n    #--------------version--------------\n        a_version=8,\n        a_note=NOTE,\n        a_goal='',\n        id='',\n    #消融实验\n        ablation=None,\n        selection=[\n            {'attention':'fcbam','freq_method':'dct8'},\n            {'attention':'cafs','freq_method':'dct8'},\n            {'attention':'fsa', 'freq_method':'dct8'}\n        ],\n    #推理\n        inference=False,\n        inference_root_dir='/kaggle/working/inference',\n        inference_suffix='VES',\n    #测试\n        evaluate=False,\n        enable_test=0,\n        test_single=0,\n    #数据位置\n        dataset_name='ddr',\n        dataset_path='',\n        preprocess=True,\n        one_mask=False,\n    #超参数\n        resolution=512,#(W,H)\n        crop_size=None,\n        grid_size=None,\n        batch_size=4,  #有bn层，所以bs应该大于1\n        lr=2e-4,\n        min_lr=1e-5,\n        scheduler='multi',\n        milestones=[60,120,180,210],\n        lr_gamma=0.5,\n        step_size=200,\n        upd_by_ep=1,\n        accumulation_step=4,\n    #模型\n        leisions=['MA','HE','EX','SE'], # ['MA','HE','EX','SE']\n        num_classes=5,\n        backbone='rs50',\n        netframe='unetpp',\n        pretrained=True,\n        downsample_factor=32,\n        module_list={'attention':'fca', 'freq_method':'dct8'},#不用时置None,使用时必须有attention\n        backbone_weights='',\n        upsample_mode='interp',\n        net_type='linear',\n        transunet_decoder='unet',\n        transformer_block='vit',\n        dual=None,\n        dual_loss_factor=0.5,\n        seg_vessel=False,\n    #损失函数\n        loss_method='bce',\n        ce_weight=[0.001,1, 0.1, 0.1, 0.1],\n        bce_weight=100,\n        focal_gamma=2,\n    #优化器设置\n        optim='Adam',\n        momentum=(0.9,0.999), #adam类优化器，也需要betas=(0.9,0.999),第一个类似动量\n        weight_decay=0.001,\n    #epoch\n        num_epochs=240,\n    #validation\n        metrics=['auprc'],\n        best_metric='auprc',\n        good_value=0.6,\n        validation_interval=1,\n        early_stop=0,\n    #-----------------------------------\n    #--------------use_wandb------------\n    #-----------------------------------\n        use_wandb=1,\n        experiment_name=\"DR_Segmentation\",\n    #log\n        log_dir='./log',\n        log_interval=10,\n        show_log=1,\n    #保存\n        save_interval=40,\n        skip_epoch=0,\n        checkpoint_dir='./checkpoint',\n        save_optim=1,\n    #恢复\n        resume_checkpoint='',\n        load_optimizer=True,\n        load_args=False,\n    #Transfrom\n        use_T=1,\n        choose_one_of=None,\n    #ssl\n        ssl='MTSL',\n        input_logits=True,\n        task_num=4,\n        gan_grading_metrics=['acc','kappa'],\n        gan_grading_best_metric='acc',\n        gan_grading_good_value=0.8,\n        gan_grading_classes=5,\n        gan_grading_dataset='idrid'\n    )\n    args=adjust_args(Aobj(defaults))\n    return args\n    \n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:01:16.637503Z","iopub.execute_input":"2024-12-23T03:01:16.637852Z","iopub.status.idle":"2024-12-23T03:01:26.244183Z","shell.execute_reply.started":"2024-12-23T03:01:16.637824Z","shell.execute_reply":"2024-12-23T03:01:26.243155Z"}},"outputs":[{"name":"stdout","text":"train dataset contains 383 images at /kaggle/input/cropped-ddr-seg/trn/image\ntest dataset contains 225 images at /kaggle/input/cropped-ddr-seg/tst/image\ntransform:Compose(\n    <__main__.ToNumpy object at 0x7eb58a816f80>\n    <__main__.OneOf object at 0x7eb568357fa0>\n    <__main__.HorizontalFlip object at 0x7eb568357d60>\n    <__main__.VerticalFlip object at 0x7eb568355b70>\n    <__main__.RandomRotate90 object at 0x7eb5683567a0>\n    <__main__.ColorJitter object at 0x7eb568357c70>\n    <__main__.Grid object at 0x7eb568357ee0>\n    <__main__.ToTensor object at 0x7eb568357e50>\n    <__main__.Normalize object at 0x7eb568356260>\n)\n{'module_list': {'attention': 'cafs', 'freq_method': 'dct8'}, 'weights': '', 'replace_stride_with_dilation': [False, False, False], 'url': 'https://download.pytorch.org/models/resnet50-0676ba61.pth'}\nbackbone loading Imagenet weights\n{'seed': 1222, 'account': 'yuyututa', 'a_version': 7, 'a_note': 'drseg', 'a_goal': '', 'id': 'ddr_unetpp_rs50_dct8_MTSL_bs4_lr0002_bce_Adam_yuyututa_7', 'ablation': None, 'selection': [{'attention': 'fcbam', 'freq_method': 'dct8'}, {'attention': 'cafs', 'freq_method': 'dct8'}, {'attention': 'fsa', 'freq_method': 'dct8'}], 'inference': False, 'inference_root_dir': '/kaggle/working/inference', 'inference_suffix': 'VES', 'evaluate': False, 'enable_test': 0, 'test_single': 0, 'dataset_name': 'ddr', 'dataset_path': '/kaggle/input/cropped-ddr-seg', 'preprocess': True, 'one_mask': False, 'resolution': (512, 512), 'crop_size': None, 'grid_size': None, 'batch_size': 4, 'lr': 0.0002, 'min_lr': 1e-05, 'scheduler': 'multi', 'milestones': [60, 120, 180, 210], 'lr_gamma': 0.5, 'step_size': 200, 'upd_by_ep': 1, 'accumulation_step': 4, 'leisions': ['MA', 'HE', 'EX', 'SE'], 'num_classes': 4, 'backbone': 'rs50', 'netframe': 'unetpp', 'pretrained': True, 'downsample_factor': 32, 'module_list': {'attention': 'cafs', 'freq_method': 'dct8'}, 'backbone_weights': '', 'upsample_mode': 'interp', 'net_type': 'linear', 'transunet_decoder': 'unet', 'transformer_block': 'vit', 'dual': None, 'dual_loss_factor': 0.5, 'seg_vessel': False, 'loss_method': 'bce', 'ce_weight': [0.001, 1, 0.1, 0.1, 0.1], 'bce_weight': 100, 'focal_gamma': 2, 'optim': 'Adam', 'momentum': (0.9, 0.999), 'weight_decay': 0.001, 'num_epochs': 240, 'metrics': ['auprc'], 'best_metric': 'auprc', 'good_value': 0.6, 'validation_interval': 1, 'early_stop': 0, 'use_wandb': 0, 'experiment_name': 'DR_Segmentation', 'log_dir': './log', 'log_interval': 10, 'show_log': 1, 'save_interval': 40, 'skip_epoch': 0, 'checkpoint_dir': './checkpoint', 'save_optim': 1, 'resume_checkpoint': '', 'load_optimizer': True, 'load_args': False, 'use_T': 1, 'choose_one_of': None, 'ssl': 'MTSL', 'input_logits': True, 'task_num': 4, 'gan_grading_metrics': ['acc', 'kappa'], 'gan_grading_best_metric': 'acc', 'gan_grading_good_value': 0.8, 'gan_grading_classes': 5, 'gan_grading_dataset': 'idrid'}\nMulti-Task Supervised trainer\nLogging to ./checkpoint/Multi-Task Supervised trainer/ddr_unetpp_rs50_dct8_MTSL_bs4_lr0002_bce_Adam_yuyututa_7/log\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/240 --- training lr:0.0002:   3%|▎         | 3/96 [00:08<04:32,  2.93s/batch]\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_23/1387064461.py\", line 232, in main\n    train(args)\n  File \"/tmp/ipykernel_23/1387064461.py\", line 218, in train\n    return trainer.loop(args.num_epochs, trn_loader, val_loader)\n  File \"/tmp/ipykernel_23/1221795158.py\", line 213, in loop\n    info_dict=self.trn_iteration(trn_loader, scheduler)\n  File \"/tmp/ipykernel_23/837679060.py\", line 127, in trn_iteration\n    self.estimator[i-1].update(o, t)\n  File \"/tmp/ipykernel_23/1536472188.py\", line 71, in update\n    targets = targets.detach().cpu().long()\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":33}]}